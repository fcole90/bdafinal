{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis problem\n",
    "The dataset that we want to analyze in this project is about number of deaths for viral hepatitis and sequelae of viral hepatitis in 25 European countries. Our dataset is divided into two files:\n",
    " - one contains the total amount of deaths for all causes of death from 2001 to 2010\n",
    " - the other contains the number of deaths for viral hepatitis and sequelae of viral hepatitis from 2001 to 2010\n",
    "\n",
    "The data in the first file will be used as normalization factor for the number of deaths for viral hepatitis. Therefore, the data that will be analyzed is the ratio of deaths for viral hepatitis over the total number of deaths in that country.\n",
    "Our objective for this analysis is to predict for each country the number of deaths for viral hepatitis in the following years. We decided not to make any prediction for a country outside the list because, even if there could be a hyperdistribution which the parameters of our models could follow, we considered that there could be too many differences among the countries which would make such a prediction unreliable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model description\n",
    "As first choice we will assess the use of a Separate model. We will consider each country as a separate group, and so we will have a distribution that describe each of them independently. \n",
    "\n",
    "![Separate Model](./separate.png)\n",
    "\n",
    "We will also evaluate a Hierarchial model, in order to verify that our initial assumption about the independece of the distribution of each country is correct. In other words, we expect that the hierarchial model will perform worse than the Separate model, which is our first choice.\n",
    "\n",
    "![Hierarchial Model](./hierarchial.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior choices\n",
    "Our prior hypothesis are the followings:\n",
    " - we assume that the data for each  distributed $y_{ij}\\mid\\theta_j \\sim \\mathcal{N}(\\mu_ {ij}, \\sigma_j)$\n",
    " - as prior distribution we will use an uninformative flat prior $\\theta_j \\sim \\mathcal{U}([0,1])=Beta(1,1)$\n",
    "\n",
    "In the Separate model we will fit a linear gaussian model for each group independently:\n",
    "$$\\mu_{ij} = \\alpha_j + \\beta_j x_{ij}$$\n",
    "and then for the Hierarchial model we will add the layer represented by the hyperdistributions for $\\alpha$, $\\beta$ and $\\sigma$:\n",
    "$$\\alpha \\sim \\mathcal{N}(\\mu_\\alpha, \\sigma_\\alpha)$$\n",
    "$$\\beta \\sim \\mathcal{N}(\\mu_\\beta, \\sigma_\\beta)$$\n",
    "$$\\sigma \\sim Inv-\\chi^2(\\sigma^2_0, \\nu_0)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pystan\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import stan_utility\n",
    "import matplotlib as mpl\n",
    "import psis\n",
    "\n",
    "d = pd.read_csv(\"../dataset/deads.txt\", sep=\" \", header=None, skiprows=1)\n",
    "h = pd.read_csv(\"../dataset/hepatitis.txt\", sep=\" \", header=None, skiprows=1)\n",
    "countries = d[0].as_matrix()\n",
    "d = d.iloc[:, 1:d.shape[1]].as_matrix()\n",
    "h = h.iloc[:, 1:h.shape[1]].as_matrix()\n",
    "\n",
    "data = h/d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f99929392b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(25):\n",
    "    plt.plot(range(2001, 2011), data[i]*100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached StanModel\n"
     ]
    }
   ],
   "source": [
    "separate_model_code = '''\n",
    "data {\n",
    "    int<lower=0> N;\n",
    "    vector[N] x; // group indicator\n",
    "    vector[N] y;\n",
    "    real xpred;\n",
    "}\n",
    "parameters {\n",
    "    real alpha;\n",
    "    real beta;\n",
    "    real<lower=0> sigma;\n",
    "}\n",
    "\n",
    "transformed parameters {\n",
    "  vector[N] mu;\n",
    "  mu = alpha + beta*x;\n",
    "}\n",
    "\n",
    "model {\n",
    "    y ~ normal(mu, sigma);\n",
    "}\n",
    "\n",
    "generated quantities {\n",
    "    real ypred;\n",
    "    vector[N] log_lik;\n",
    "    ypred = normal_rng(alpha + beta*xpred, sigma);\n",
    "    for (i in 1:N)\n",
    "        log_lik[i] = normal_lpdf(y[i] | mu[i], sigma);\n",
    "}\n",
    "'''\n",
    "\n",
    "sm = stan_utility.compile_model_plus(separate_model_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed: group  0\n"
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "x = range(2001, 2011)\n",
    "xpred=2011\n",
    "K = 25 \n",
    "\n",
    "samples = []\n",
    "for i in range(K):\n",
    "    y = np.log(data[i]).ravel()   # observations\n",
    "    separate_model_data = dict(\n",
    "        N = N,\n",
    "        #K = K,  # 25 contries\n",
    "        x = x,  # group indicators\n",
    "        y = y,  # observations\n",
    "        xpred=xpred\n",
    "    )\n",
    "\n",
    "    samples.append(sm.sampling(n_jobs=4, data=separate_model_data, control=dict(adapt_delta=0.95, max_treedepth=20)))\n",
    "\n",
    "    print('Completed: group ', i)\n",
    "print('Completed: separate model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_f33a8b74a230c2fdd82c5bbc279eeecb NOW.\n"
     ]
    }
   ],
   "source": [
    "hierarchical_model_code = '''\n",
    "data {\n",
    "  int<lower=0> N; // number of data points\n",
    "  int<lower=0> K; // number of groups\n",
    "  int<lower=1,upper=K> x[N]; // group indicator\n",
    "  vector[N] y; //\n",
    "  int xpred;\n",
    "}\n",
    "parameters {\n",
    "  vector[K] alpha;        // group means\n",
    "  real mu_alpha;\n",
    "  real sigma_alpha;\n",
    "  \n",
    "  vector[K] beta;        // group means\n",
    "  real mu_beta;\n",
    "  real sigma_beta;\n",
    "  \n",
    "  real<lower=0> sigma[K]; \n",
    "  real nu_0;\n",
    "  real sigma_0;\n",
    "}\n",
    "\n",
    "transformed parameters {\n",
    "    vector[N] mu;\n",
    "    for (i in 1:N)\n",
    "        mu[i] = alpha[x[i]] + beta[x[i]]*x[i];\n",
    "}\n",
    "\n",
    "model {\n",
    "    sigma ~ half-cauchy(mode, scale)\n",
    "\n",
    "    alpha ~ normal(mu_alpha, sigma_alpha);\n",
    "    beta ~ normal(mu_beta, sigma_beta);\n",
    "    sigma ~ scaled_inv_chi_square(sigma_0, nu_0);\n",
    "    \n",
    "    y ~ normal(mu, sigma[x]);\n",
    "}\n",
    "\n",
    "generated quantities{\n",
    "    real ypred[K];\n",
    "    vector[N] log_lik;\n",
    "    \n",
    "    for (i in 1:K)\n",
    "    {\n",
    "        ypred[i] = normal_rng(alpha[i] + beta[i]*xpred, sigma[i]);\n",
    "    }\n",
    "    \n",
    "    for (i in 1:N)\n",
    "    {\n",
    "        log_lik[i] = normal_lpdf(y[i] | mu[i], sigma[x[i]]);\n",
    "    }\n",
    "}\n",
    "'''\n",
    "\n",
    "sm_hierarchical = stan_utility.compile_model_plus(hierarchical_model_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed - hierarchical model\n"
     ]
    }
   ],
   "source": [
    "K=25\n",
    "nj = 10\n",
    "N = nj*K\n",
    "x = np.array([i for i in range(1,K+1) for j in range(10)])\n",
    "# print(x)\n",
    "y = np.log(data[0:K]).ravel()  # observations\n",
    "xpred=K+1\n",
    "\n",
    "hierarchical_model_data = dict(\n",
    "    N = N,\n",
    "    K = K,  # 25 contries\n",
    "    x = x,  # group indicators\n",
    "    y = y,  # observations\n",
    "    xpred=xpred\n",
    ")\n",
    "\n",
    "samples_hierarchical = sm_hierarchical.sampling(n_jobs=-1, data=hierarchical_model_data)\n",
    "\n",
    "print('Completed - hierarchical model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Stan model is run\n",
    "\n",
    "### Separate\n",
    "Given that we are using a separate model, we created a Stan model which is run once on the data of each country. This gives us K predictions, one for each analysed country.\n",
    "\n",
    "### Hierarchical\n",
    "In the hierarchical model we created a single Stan model to analyse the whole dataset at one, considering each parameter `alpha` (respectively `beta`) to be following a common hyperdistribution across all the countries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(np.exp(samples['ypred']), 50)\n",
    "plt.xlabel('y-prediction for x={}'.format(2011))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "color_scatter = 'C0'  # 'C0' for default color #0\n",
    "color_line = 'C1'     # 'C1' for default color #1\n",
    "\n",
    "color_shade = (\n",
    "    1 - 0.1*(1 - np.array(mpl.colors.to_rgb(color_line)))\n",
    ")\n",
    "\n",
    "plt.fill_between(\n",
    "    x[0],\n",
    "    np.percentile(samples['mu'], 5, axis=0),\n",
    "    np.percentile(samples['mu'], 95, axis=0),\n",
    "    color=color_shade\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    x[0],\n",
    "    np.percentile(samples['mu'], 50, axis=0),\n",
    "    color=color_line,\n",
    "    linewidth=1\n",
    ")\n",
    "\n",
    "plt.scatter(x[0], y, 5, color=color_scatter)\n",
    "plt.xlim((2001, 2010))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Convergence analysis\n",
    "### Separate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference for Stan model: anon_model_570544391b1e440e4aa07d2bfc218092.\n",
      "4 chains, each with iter=2000; warmup=1000; thin=1; \n",
      "post-warmup draws per chain=1000, total post-warmup draws=4000.\n",
      "\n",
      "             mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\n",
      "alpha       70.84    1.81  53.64 -35.16  38.79  70.16 103.43 179.22    875   1.01\n",
      "beta        -0.04  9.0e-4   0.03  -0.09  -0.06  -0.04  -0.02   0.01    875   1.01\n",
      "sigma        0.25  3.1e-3   0.09   0.14   0.19   0.23   0.28   0.44    751    1.0\n",
      "mu[0]        -7.7  4.4e-3   0.15  -7.98  -7.79   -7.7  -7.61   -7.4   1078    1.0\n",
      "mu[1]       -7.74  3.6e-3   0.12  -7.98  -7.82  -7.74  -7.66  -7.48   1200    1.0\n",
      "mu[2]       -7.78  2.8e-3   0.11  -7.99  -7.84  -7.78  -7.72  -7.56   1447    1.0\n",
      "mu[3]       -7.82  2.0e-3   0.09   -8.0  -7.87  -7.82  -7.76  -7.64   2001    1.0\n",
      "mu[4]       -7.86  1.4e-3   0.08  -8.03  -7.91  -7.86  -7.81  -7.69   3598    1.0\n",
      "mu[5]        -7.9  1.4e-3   0.08  -8.06  -7.95   -7.9  -7.85  -7.74   3626    1.0\n",
      "mu[6]       -7.94  1.8e-3   0.09  -8.12  -7.99  -7.94  -7.88  -7.76   2391    1.0\n",
      "mu[7]       -7.98  2.6e-3    0.1  -8.19  -8.04  -7.97  -7.91  -7.77   1653    1.0\n",
      "mu[8]       -8.02  3.4e-3   0.12  -8.28  -8.09  -8.01  -7.94  -7.77   1325    1.0\n",
      "mu[9]       -8.05  4.2e-3   0.14  -8.36  -8.14  -8.06  -7.97  -7.76   1163    1.0\n",
      "ypred       -8.09  6.2e-3   0.31  -8.69  -8.27  -8.09   -7.9  -7.49   2400    1.0\n",
      "log_lik[0]  -0.27    0.02    0.7  -2.09   -0.6  -0.12   0.22   0.65   1654    1.0\n",
      "log_lik[1]   0.08    0.01   0.45  -1.02  -0.16   0.15   0.41   0.74   1263    1.0\n",
      "log_lik[2]  -0.63    0.02    0.7  -2.39  -0.97  -0.49  -0.14   0.34   1513    1.0\n",
      "log_lik[3]   0.41  9.5e-3    0.3  -0.22   0.23   0.44   0.61   0.92    990    1.0\n",
      "log_lik[4]  -0.35  8.6e-3   0.46  -1.53  -0.58  -0.25  -0.02   0.32   2942    1.0\n",
      "log_lik[5]    0.1  6.9e-3   0.32  -0.63  -0.08   0.13   0.32   0.61   2087    1.0\n",
      "log_lik[6]   0.33  9.1e-3    0.3  -0.31   0.15   0.35   0.54   0.85   1106    1.0\n",
      "log_lik[7]   0.45    0.01   0.31  -0.25   0.26   0.48   0.66   0.96    907    1.0\n",
      "log_lik[8]   0.41    0.01   0.33  -0.36   0.22   0.45   0.64   0.95    906    1.0\n",
      "log_lik[9]   0.33    0.01   0.38  -0.53   0.13   0.38    0.6   0.93    960    1.0\n",
      "lp__          8.6    0.06   1.48   4.85   7.91    9.0   9.65  10.24    713    1.0\n",
      "\n",
      "Samples were drawn using NUTS at Thu Dec  7 13:21:17 2017.\n",
      "For each parameter, n_eff is a crude measure of effective sample size,\n",
      "and Rhat is the potential scale reduction factor on split chains (at \n",
      "convergence, Rhat=1).\n",
      "Inference for Stan model: anon_model_570544391b1e440e4aa07d2bfc218092.\n",
      "4 chains, each with iter=2000; warmup=1000; thin=1; \n",
      "post-warmup draws per chain=1000, total post-warmup draws=4000.\n",
      "\n",
      "             mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\n",
      "alpha      -213.3    2.08  56.91 -327.4 -247.1 -213.8 -179.8 -101.7    751   1.01\n",
      "beta          0.1  1.0e-3   0.03   0.05   0.09    0.1   0.12   0.16    751   1.01\n",
      "sigma        0.25  2.7e-3   0.08   0.15    0.2   0.23   0.29   0.44    865    1.0\n",
      "mu[0]       -8.93  5.1e-3   0.15  -9.23  -9.02  -8.93  -8.84  -8.61    912    1.0\n",
      "mu[1]       -8.83  4.1e-3   0.13  -9.09  -8.91  -8.83  -8.75  -8.56   1017    1.0\n",
      "mu[2]       -8.72  3.1e-3   0.11  -8.94  -8.79  -8.73  -8.66   -8.5   1242    1.0\n",
      "mu[3]       -8.62  2.2e-3   0.09   -8.8  -8.68  -8.62  -8.56  -8.43   1775    1.0\n",
      "mu[4]       -8.52  1.4e-3   0.08  -8.68  -8.57  -8.52  -8.47  -8.35   3340    1.0\n",
      "mu[5]       -8.42  1.3e-3   0.08  -8.58  -8.47  -8.42  -8.37  -8.26   3848    1.0\n",
      "mu[6]       -8.32  1.8e-3   0.09   -8.5  -8.37  -8.32  -8.26  -8.13   2463    1.0\n",
      "mu[7]       -8.21  2.6e-3   0.11  -8.43  -8.28  -8.21  -8.15  -8.01   1664    1.0\n",
      "mu[8]       -8.11  3.5e-3   0.13  -8.37  -8.19  -8.11  -8.03  -7.86   1318   1.01\n",
      "mu[9]       -8.01  4.4e-3   0.15  -8.31   -8.1  -8.01  -7.92  -7.71   1144   1.01\n",
      "ypred       -7.91  6.8e-3   0.31  -8.53   -8.1  -7.91  -7.72  -7.27   2129    1.0\n",
      "log_lik[0]   0.01    0.02   0.55  -1.41  -0.23   0.12   0.39   0.76   1317    1.0\n",
      "log_lik[1]   0.23    0.01    0.4  -0.72 8.7e-3   0.29   0.51   0.85   1128    1.0\n",
      "log_lik[2]   0.39    0.01   0.32  -0.31   0.19   0.42   0.61   0.92    965    1.0\n",
      "log_lik[3]    0.4  9.5e-3    0.3  -0.25   0.21   0.43   0.62   0.91   1008    1.0\n",
      "log_lik[4]   0.34  8.3e-3   0.28  -0.27   0.16   0.36   0.54   0.83   1172    1.0\n",
      "log_lik[5]  -0.49  9.6e-3   0.52   -1.8  -0.73  -0.39  -0.13   0.24   2941    1.0\n",
      "log_lik[6]   0.45    0.01    0.3   -0.2   0.26   0.48   0.66   0.95    877    1.0\n",
      "log_lik[7]  -0.92    0.02   0.82  -3.01  -1.32  -0.75  -0.33   0.19   1690    1.0\n",
      "log_lik[8]   0.33    0.01   0.36  -0.52   0.13   0.37   0.58    0.9   1018    1.0\n",
      "log_lik[9]  -0.09    0.02   0.61  -1.62  -0.36   0.04   0.33   0.71   1339    1.0\n",
      "lp__         8.41    0.06   1.44   4.69   7.75   8.78   9.48  10.07    681    1.0\n",
      "\n",
      "Samples were drawn using NUTS at Thu Dec  7 13:21:43 2017.\n",
      "For each parameter, n_eff is a crude measure of effective sample size,\n",
      "and Rhat is the potential scale reduction factor on split chains (at \n",
      "convergence, Rhat=1).\n",
      "Inference for Stan model: anon_model_570544391b1e440e4aa07d2bfc218092.\n",
      "4 chains, each with iter=2000; warmup=1000; thin=1; \n",
      "post-warmup draws per chain=1000, total post-warmup draws=4000.\n",
      "\n",
      "             mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\n",
      "alpha       36.54    0.75  19.79  -4.91  24.86  36.91   48.3  74.96    696   1.01\n",
      "beta        -0.02  3.7e-4 9.9e-3  -0.04  -0.03  -0.02  -0.02-8.5e-4    696   1.01\n",
      "sigma        0.08  9.4e-4   0.03   0.05   0.07   0.08    0.1   0.15    766    1.0\n",
      "mu[0]       -6.54  1.7e-3   0.05  -6.64  -6.57  -6.54  -6.51  -6.43    922    1.0\n",
      "mu[1]       -6.56  1.4e-3   0.04  -6.65  -6.59  -6.56  -6.53  -6.47   1041    1.0\n",
      "mu[2]       -6.58  1.0e-3   0.04  -6.65  -6.61  -6.58  -6.56  -6.51   1288    1.0\n",
      "mu[3]        -6.6  7.3e-4   0.03  -6.67  -6.62   -6.6  -6.58  -6.54   1870    1.0\n",
      "mu[4]       -6.63  5.0e-4   0.03  -6.68  -6.64  -6.63  -6.61  -6.57   3233    1.0\n",
      "mu[5]       -6.65  5.0e-4   0.03   -6.7  -6.66  -6.65  -6.63  -6.59   3163    1.0\n",
      "mu[6]       -6.67  7.3e-4   0.03  -6.73  -6.69  -6.67  -6.65  -6.61   1834    1.0\n",
      "mu[7]       -6.69  1.0e-3   0.04  -6.77  -6.71  -6.69  -6.67  -6.62   1265    1.0\n",
      "mu[8]       -6.71  1.4e-3   0.04  -6.81  -6.74  -6.71  -6.68  -6.62   1025   1.01\n",
      "mu[9]       -6.73  1.7e-3   0.05  -6.84  -6.76  -6.73   -6.7  -6.63    910   1.01\n",
      "ypred       -6.75  2.5e-3   0.11  -6.97  -6.82  -6.75  -6.69  -6.53   1970    1.0\n",
      "log_lik[0]   1.38    0.01    0.4   0.41   1.18   1.44   1.66   1.98    710   1.01\n",
      "log_lik[1]   1.39    0.01   0.36   0.55    1.2   1.43   1.64   1.94    888    1.0\n",
      "log_lik[2]   1.28    0.01   0.36   0.48   1.09   1.33   1.53   1.84   1169    1.0\n",
      "log_lik[3]   1.38  9.2e-3    0.3   0.71    1.2    1.4   1.58   1.88   1055    1.0\n",
      "log_lik[4]   1.44  8.7e-3   0.28   0.81   1.27   1.46   1.64   1.93   1041    1.0\n",
      "log_lik[5]   1.54    0.01   0.29   0.92   1.36   1.55   1.74   2.03    785    1.0\n",
      "log_lik[6]   0.63    0.01   0.55  -0.72   0.36   0.73   1.01    1.4   2097    1.0\n",
      "log_lik[7]   0.65    0.01   0.61  -0.93   0.35   0.77   1.08   1.49   1761    1.0\n",
      "log_lik[8]   1.06    0.01   0.49  -0.16    0.8   1.14   1.41   1.78   1276    1.0\n",
      "log_lik[9]   0.89    0.02   0.65  -0.73   0.58   1.03   1.35   1.76   1223    1.0\n",
      "lp__         18.3    0.06   1.49  14.45  17.63  18.67   19.4   20.0    661   1.01\n",
      "\n",
      "Samples were drawn using NUTS at Thu Dec  7 13:22:09 2017.\n",
      "For each parameter, n_eff is a crude measure of effective sample size,\n",
      "and Rhat is the potential scale reduction factor on split chains (at \n",
      "convergence, Rhat=1).\n",
      "Inference for Stan model: anon_model_570544391b1e440e4aa07d2bfc218092.\n",
      "4 chains, each with iter=2000; warmup=1000; thin=1; \n",
      "post-warmup draws per chain=1000, total post-warmup draws=4000.\n",
      "\n",
      "             mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\n",
      "alpha      -268.7    8.21 224.25 -733.6 -399.1 -266.0 -128.3 152.55    746    1.0\n",
      "beta         0.13  4.1e-3   0.11  -0.08   0.06   0.13   0.19   0.36    746    1.0\n",
      "sigma        0.95    0.01    0.3   0.57   0.74   0.88   1.07   1.75    793    1.0\n",
      "mu[0]       -8.91    0.02   0.59 -10.14  -9.27   -8.9  -8.56  -7.74    947    1.0\n",
      "mu[1]       -8.79    0.02   0.49  -9.82  -9.08  -8.78  -8.49   -7.8   1064    1.0\n",
      "mu[2]       -8.66    0.01   0.41  -9.48   -8.9  -8.65  -8.41  -7.84   1312    1.0\n",
      "mu[3]       -8.53  7.6e-3   0.34  -9.21  -8.74  -8.52  -8.31  -7.85   2034    1.0\n",
      "mu[4]        -8.4  5.0e-3   0.31  -9.01  -8.58  -8.39   -8.2  -7.81   3779    1.0\n",
      "mu[5]       -8.27  4.9e-3    0.3  -8.88  -8.45  -8.26  -8.08  -7.68   3817    1.0\n",
      "mu[6]       -8.14  7.6e-3   0.34  -8.84  -8.34  -8.14  -7.93  -7.47   2012    1.0\n",
      "mu[7]       -8.01    0.01   0.41  -8.82  -8.25   -8.0  -7.76  -7.18   1309    1.0\n",
      "mu[8]       -7.88    0.02   0.49  -8.85  -8.17  -7.87  -7.59  -6.87   1060    1.0\n",
      "mu[9]       -7.75    0.02   0.58  -8.89   -8.1  -7.74   -7.4  -6.54    943    1.0\n",
      "ypred        -7.6    0.03   1.23 -10.05  -8.35   -7.6  -6.87  -5.05   2069    1.0\n",
      "log_lik[0]  -1.02    0.01    0.4  -1.96  -1.23  -0.96  -0.74  -0.43    760    1.0\n",
      "log_lik[1]  -1.71    0.02   0.66  -3.31  -2.03  -1.58  -1.25   -0.8   1639    1.0\n",
      "log_lik[2]  -1.19    0.01   0.38   -2.1  -1.41  -1.14  -0.92  -0.62   1244    1.0\n",
      "log_lik[3]   -0.9    0.01    0.3  -1.57  -1.07  -0.87  -0.68   -0.4    823    1.0\n",
      "log_lik[4]  -1.19  6.6e-3    0.3  -1.82  -1.37  -1.16  -0.98  -0.67   2027    1.0\n",
      "log_lik[5]  -1.22  6.9e-3   0.31  -1.91   -1.4  -1.19   -1.0   -0.7   1956    1.0\n",
      "log_lik[6]  -0.95  9.8e-3    0.3  -1.61  -1.14  -0.92  -0.75  -0.45    919    1.0\n",
      "log_lik[7]  -0.91    0.01   0.31  -1.62   -1.1  -0.88  -0.69   -0.4    815    1.0\n",
      "log_lik[8]  -1.52    0.01   0.57  -2.95  -1.79  -1.41  -1.12  -0.73   1738    1.0\n",
      "log_lik[9]  -2.04    0.02   0.92  -4.42   -2.5  -1.83  -1.37  -0.85   1528    1.0\n",
      "lp__        -3.56    0.05   1.46  -7.43  -4.19  -3.19  -2.47  -1.89    715    1.0\n",
      "\n",
      "Samples were drawn using NUTS at Thu Dec  7 13:22:35 2017.\n",
      "For each parameter, n_eff is a crude measure of effective sample size,\n",
      "and Rhat is the potential scale reduction factor on split chains (at \n",
      "convergence, Rhat=1).\n",
      "Inference for Stan model: anon_model_570544391b1e440e4aa07d2bfc218092.\n",
      "4 chains, each with iter=2000; warmup=1000; thin=1; \n",
      "post-warmup draws per chain=1000, total post-warmup draws=4000.\n",
      "\n",
      "             mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\n",
      "alpha      -175.9    2.32  70.66 -320.7 -218.0 -177.8 -134.0 -25.37    924    1.0\n",
      "beta         0.08  1.2e-3   0.04 8.9e-3   0.06   0.08    0.1   0.16    924    1.0\n",
      "sigma         0.3  3.2e-3    0.1   0.18   0.23   0.28   0.34   0.54    878    1.0\n",
      "mu[0]       -7.92  5.5e-3   0.18  -8.28  -8.03  -7.92   -7.8  -7.55   1121    1.0\n",
      "mu[1]       -7.83  4.4e-3   0.16  -8.14  -7.93  -7.83  -7.74  -7.53   1246    1.0\n",
      "mu[2]       -7.75  3.3e-3   0.13   -8.0  -7.83  -7.75  -7.67  -7.49   1507    1.0\n",
      "mu[3]       -7.67  2.2e-3   0.11  -7.88  -7.73  -7.67   -7.6  -7.44   2385    1.0\n",
      "mu[4]       -7.58  1.7e-3    0.1  -7.78  -7.64  -7.58  -7.52  -7.38   3463    1.0\n",
      "mu[5]        -7.5  1.7e-3    0.1  -7.69  -7.56   -7.5  -7.44   -7.3   3395    1.0\n",
      "mu[6]       -7.41  2.3e-3   0.11  -7.64  -7.48  -7.42  -7.35  -7.19   2291    1.0\n",
      "mu[7]       -7.33  3.3e-3   0.13   -7.6  -7.41  -7.33  -7.25  -7.07   1657    1.0\n",
      "mu[8]       -7.25  4.3e-3   0.16  -7.57  -7.34  -7.24  -7.15  -6.93   1375    1.0\n",
      "mu[9]       -7.16  5.4e-3   0.19  -7.54  -7.28  -7.16  -7.05  -6.78   1222    1.0\n",
      "ypred       -7.09  8.1e-3   0.39  -7.85  -7.32  -7.09  -6.85   -6.3   2278    1.0\n",
      "log_lik[0]  -0.26    0.02   0.62  -1.92  -0.52  -0.13   0.16   0.55   1145    1.0\n",
      "log_lik[1]  -0.01    0.01   0.43  -1.04  -0.23   0.06   0.27   0.63   1080    1.0\n",
      "log_lik[2]  -0.52    0.01   0.56  -1.88  -0.81  -0.43  -0.14    0.3   1588    1.0\n",
      "log_lik[3]   0.27  9.7e-3    0.3  -0.42   0.09   0.29   0.48   0.77    962    1.0\n",
      "log_lik[4]   0.21  8.6e-3   0.29  -0.43   0.04   0.24   0.42    0.7   1119    1.0\n",
      "log_lik[5]   0.16  8.7e-3   0.28  -0.46  -0.01   0.19   0.36   0.65   1082    1.0\n",
      "log_lik[6]  -1.06    0.02   0.74  -2.89  -1.45  -0.89  -0.52  -0.09   1907    1.0\n",
      "log_lik[7]    0.2    0.01   0.32  -0.53   0.02   0.24   0.43   0.73    959    1.0\n",
      "log_lik[8]    0.2    0.01   0.35  -0.59 7.3e-3   0.24   0.44   0.75    916    1.0\n",
      "log_lik[9]  -0.23    0.02   0.61  -1.85  -0.51   -0.1   0.19   0.58   1372    1.0\n",
      "lp__         6.88    0.06   1.51   3.01   6.21   7.27   7.98    8.6    737    1.0\n",
      "\n",
      "Samples were drawn using NUTS at Thu Dec  7 13:23:01 2017.\n",
      "For each parameter, n_eff is a crude measure of effective sample size,\n",
      "and Rhat is the potential scale reduction factor on split chains (at \n",
      "convergence, Rhat=1).\n",
      "Inference for Stan model: anon_model_570544391b1e440e4aa07d2bfc218092.\n",
      "4 chains, each with iter=2000; warmup=1000; thin=1; \n",
      "post-warmup draws per chain=1000, total post-warmup draws=4000.\n",
      "\n",
      "             mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\n",
      "alpha      -14.77    1.61  40.59 -94.99 -39.57 -15.01    9.0  69.83    639   1.01\n",
      "beta       3.8e-3  8.0e-4   0.02  -0.04-8.1e-3 3.9e-3   0.02   0.04    639   1.01\n",
      "sigma        0.18  1.8e-3   0.05   0.11   0.14   0.17    0.2   0.32    912    1.0\n",
      "mu[0]       -7.18  3.6e-3   0.11  -7.39  -7.24  -7.18  -7.11  -6.95    885   1.01\n",
      "mu[1]       -7.17  2.9e-3   0.09  -7.35  -7.23  -7.17  -7.12  -6.99   1002   1.01\n",
      "mu[2]       -7.17  2.2e-3   0.08  -7.32  -7.22  -7.17  -7.12  -7.02   1249    1.0\n",
      "mu[3]       -7.17  1.4e-3   0.06  -7.29  -7.21  -7.17  -7.13  -7.03   2168    1.0\n",
      "mu[4]       -7.16 10.0e-4   0.06  -7.28   -7.2  -7.16  -7.13  -7.05   3307    1.0\n",
      "mu[5]       -7.16  9.6e-4   0.06  -7.27  -7.19  -7.16  -7.12  -7.05   3544    1.0\n",
      "mu[6]       -7.15  1.3e-3   0.06  -7.28  -7.19  -7.16  -7.12  -7.03   2487    1.0\n",
      "mu[7]       -7.15  2.1e-3   0.07  -7.31   -7.2  -7.15  -7.11   -7.0   1255    1.0\n",
      "mu[8]       -7.15  2.9e-3   0.09  -7.33   -7.2  -7.15  -7.09  -6.97    946   1.01\n",
      "mu[9]       -7.14  3.7e-3   0.11  -7.37  -7.21  -7.14  -7.08  -6.93    830   1.01\n",
      "ypred       -7.14  5.0e-3   0.22  -7.59  -7.28  -7.14  -7.01  -6.69   1980    1.0\n",
      "log_lik[0]  -0.55    0.03   1.03  -3.23   -1.0  -0.32   0.19   0.73   1270   1.01\n",
      "log_lik[1]   0.72    0.01   0.33  -0.04   0.53   0.76   0.95   1.25    922    1.0\n",
      "log_lik[2]   0.63  9.6e-3   0.32   -0.1   0.44   0.67   0.86   1.17   1114    1.0\n",
      "log_lik[3]   0.61  8.2e-3   0.29  -0.04   0.43   0.64   0.82   1.12   1279    1.0\n",
      "log_lik[4]   0.09  7.1e-3    0.4  -0.87  -0.12   0.15   0.37   0.71   3099    1.0\n",
      "log_lik[5]   0.47  6.2e-3   0.29  -0.17   0.29   0.49   0.68   0.97   2176    1.0\n",
      "log_lik[6]   0.45  7.3e-3   0.33   -0.3   0.27   0.49   0.68   0.98   1992    1.0\n",
      "log_lik[7]   0.63  9.2e-3   0.32   -0.1   0.44   0.66   0.85   1.16   1220    1.0\n",
      "log_lik[8]   0.67    0.01   0.33  -0.07   0.48   0.71   0.91   1.21   1098    1.0\n",
      "log_lik[9]    0.3    0.01   0.57  -1.17   0.05   0.42   0.69   1.06   1477    1.0\n",
      "lp__        11.46    0.05   1.39    7.8  10.77  11.84   12.5  13.08    767    1.0\n",
      "\n",
      "Samples were drawn using NUTS at Thu Dec  7 13:23:28 2017.\n",
      "For each parameter, n_eff is a crude measure of effective sample size,\n",
      "and Rhat is the potential scale reduction factor on split chains (at \n",
      "convergence, Rhat=1).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference for Stan model: anon_model_570544391b1e440e4aa07d2bfc218092.\n",
      "4 chains, each with iter=2000; warmup=1000; thin=1; \n",
      "post-warmup draws per chain=1000, total post-warmup draws=4000.\n",
      "\n",
      "             mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\n",
      "alpha        9.78    0.76  20.21 -31.27  -2.25    9.7   21.7  52.05    708    1.0\n",
      "beta      -7.8e-3  3.8e-4   0.01  -0.03  -0.01-7.8e-3-1.8e-3   0.01    708    1.0\n",
      "sigma        0.08  9.0e-4   0.03   0.05   0.07   0.08    0.1   0.15    846    1.0\n",
      "mu[0]        -5.9  1.7e-3   0.05  -6.01  -5.93   -5.9  -5.87  -5.79    953    1.0\n",
      "mu[1]       -5.91  1.3e-3   0.04   -6.0  -5.94  -5.91  -5.88  -5.82   1091    1.0\n",
      "mu[2]       -5.92  1.0e-3   0.04  -5.99  -5.94  -5.92  -5.89  -5.84   1380    1.0\n",
      "mu[3]       -5.92  6.8e-4   0.03  -5.99  -5.94  -5.92   -5.9  -5.86   2142    1.0\n",
      "mu[4]       -5.93  4.9e-4   0.03  -5.99  -5.95  -5.93  -5.91  -5.88   3336    1.0\n",
      "mu[5]       -5.94  5.0e-4   0.03   -6.0  -5.96  -5.94  -5.92  -5.88   3185    1.0\n",
      "mu[6]       -5.95  7.6e-4   0.03  -6.01  -5.97  -5.95  -5.93  -5.88   1763    1.0\n",
      "mu[7]       -5.96  1.1e-3   0.04  -6.03  -5.98  -5.95  -5.93  -5.88   1201    1.0\n",
      "mu[8]       -5.96  1.4e-3   0.05  -6.06  -5.99  -5.96  -5.94  -5.88    979    1.0\n",
      "mu[9]       -5.97  1.8e-3   0.05  -6.08   -6.0  -5.97  -5.94  -5.87    874    1.0\n",
      "ypred       -5.98  2.7e-3   0.11   -6.2  -6.04  -5.98  -5.91  -5.77   1550    1.0\n",
      "log_lik[0]   1.04    0.02   0.61  -0.47   0.76   1.17   1.46   1.85   1162    1.0\n",
      "log_lik[1]   1.33    0.01    0.4   0.39   1.11   1.38   1.61   1.94   1016    1.0\n",
      "log_lik[2]   1.51    0.01   0.31   0.81   1.32   1.54   1.74   2.02    811    1.0\n",
      "log_lik[3]   0.38    0.02   0.66  -1.25   0.07   0.51   0.85   1.28   1881    1.0\n",
      "log_lik[4]   1.43  8.6e-3   0.29    0.8   1.26   1.45   1.64   1.93   1101    1.0\n",
      "log_lik[5]   1.47  9.0e-3   0.29   0.84   1.29   1.49   1.68   1.96   1048    1.0\n",
      "log_lik[6]   1.53    0.01    0.3   0.88   1.34   1.55   1.74   2.03    837    1.0\n",
      "log_lik[7]   0.62    0.02   0.65  -0.95   0.32   0.75   1.08   1.48   1543    1.0\n",
      "log_lik[8]   0.98    0.02   0.55  -0.39   0.71   1.08   1.37   1.77   1306    1.0\n",
      "log_lik[9]   1.24    0.02   0.51  -0.04   1.01   1.32   1.58   1.92    892    1.0\n",
      "lp__        18.19    0.06   1.49  14.29   17.5  18.57  19.28  19.94    667    1.0\n",
      "\n",
      "Samples were drawn using NUTS at Thu Dec  7 13:24:00 2017.\n",
      "For each parameter, n_eff is a crude measure of effective sample size,\n",
      "and Rhat is the potential scale reduction factor on split chains (at \n",
      "convergence, Rhat=1).\n",
      "Inference for Stan model: anon_model_570544391b1e440e4aa07d2bfc218092.\n",
      "4 chains, each with iter=2000; warmup=1000; thin=1; \n",
      "post-warmup draws per chain=1000, total post-warmup draws=4000.\n",
      "\n",
      "             mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\n",
      "alpha       44.24    0.67  19.42   4.79  32.15  43.97  56.33  82.04    840    1.0\n",
      "beta        -0.03  3.3e-4 9.7e-3  -0.04  -0.03  -0.03  -0.02-5.7e-3    840    1.0\n",
      "sigma        0.09  9.3e-4   0.03   0.06   0.07   0.09    0.1   0.16    857    1.0\n",
      "mu[0]       -6.45  1.5e-3   0.05  -6.55  -6.48  -6.45  -6.41  -6.34   1188    1.0\n",
      "mu[1]       -6.47  1.2e-3   0.04  -6.56   -6.5  -6.47  -6.44  -6.38   1382    1.0\n",
      "mu[2]        -6.5  9.1e-4   0.04  -6.57  -6.52   -6.5  -6.47  -6.42   1732    1.0\n",
      "mu[3]       -6.52  6.5e-4   0.03  -6.59  -6.54  -6.52   -6.5  -6.46   2494    1.0\n",
      "mu[4]       -6.55  5.0e-4   0.03  -6.61  -6.57  -6.55  -6.53  -6.49   3558    1.0\n",
      "mu[5]       -6.57  5.1e-4   0.03  -6.63  -6.59  -6.57  -6.56  -6.51   3509    1.0\n",
      "mu[6]        -6.6  7.5e-4   0.03  -6.66  -6.62   -6.6  -6.58  -6.53   1969    1.0\n",
      "mu[7]       -6.62  1.0e-3   0.04   -6.7  -6.65  -6.62   -6.6  -6.55   1397    1.0\n",
      "mu[8]       -6.65  1.3e-3   0.05  -6.74  -6.68  -6.65  -6.62  -6.56   1195    1.0\n",
      "mu[9]       -6.67  1.6e-3   0.05  -6.78  -6.71  -6.67  -6.64  -6.57   1061    1.0\n",
      "ypred        -6.7  2.3e-3   0.11  -6.92  -6.77   -6.7  -6.63  -6.47   2301    1.0\n",
      "log_lik[0]   1.11    0.01   0.48  -0.08   0.89   1.19   1.45    1.8   1229    1.0\n",
      "log_lik[1]   1.29    0.01   0.35   0.46   1.09   1.34   1.54   1.86   1058    1.0\n",
      "log_lik[2]  -0.11    0.02    0.9  -2.38  -0.56   0.07   0.56   1.09   1634    1.0\n",
      "log_lik[3]   1.39  8.8e-3   0.28   0.77   1.21   1.42   1.59   1.89   1042   1.01\n",
      "log_lik[4]   1.45  9.2e-3   0.28   0.83   1.28   1.47   1.65   1.92    924   1.01\n",
      "log_lik[5]   1.46  9.3e-3   0.28   0.84   1.29   1.49   1.66   1.94    908   1.01\n",
      "log_lik[6]   1.46  9.5e-3   0.29   0.82   1.28   1.48   1.66   1.95    910   1.01\n",
      "log_lik[7]   0.18    0.02   0.78  -1.75  -0.22   0.33   0.74   1.23   1641    1.0\n",
      "log_lik[8]   1.32    0.01   0.35   0.53   1.12   1.36   1.57   1.87   1013    1.0\n",
      "log_lik[9]   1.18    0.01   0.45   0.09   0.96   1.26   1.49   1.83   1089    1.0\n",
      "lp__        17.47    0.05   1.32  14.02  16.82  17.81  18.47  19.03    805    1.0\n",
      "\n",
      "Samples were drawn using NUTS at Thu Dec  7 13:24:28 2017.\n",
      "For each parameter, n_eff is a crude measure of effective sample size,\n",
      "and Rhat is the potential scale reduction factor on split chains (at \n",
      "convergence, Rhat=1).\n",
      "Inference for Stan model: anon_model_570544391b1e440e4aa07d2bfc218092.\n",
      "4 chains, each with iter=2000; warmup=1000; thin=1; \n",
      "post-warmup draws per chain=1000, total post-warmup draws=4000.\n",
      "\n",
      "             mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\n",
      "alpha      -204.3    4.68 111.12 -433.5 -263.2 -203.4 -140.8   4.21    563   1.01\n",
      "beta          0.1  2.3e-3   0.06-5.8e-3   0.07    0.1   0.13   0.21    563   1.01\n",
      "sigma        0.42  6.4e-3   0.15   0.24   0.32   0.38   0.47   0.79    551    1.0\n",
      "mu[0]       -7.86    0.01   0.28  -8.43  -8.02  -7.86   -7.7  -7.32    757    1.0\n",
      "mu[1]       -7.76  7.9e-3   0.23  -8.23   -7.9  -7.76  -7.63   -7.3    871    1.0\n",
      "mu[2]       -7.66  5.8e-3   0.19  -8.05  -7.78  -7.67  -7.55  -7.28   1128    1.0\n",
      "mu[3]       -7.57  3.6e-3   0.16  -7.88  -7.66  -7.57  -7.47  -7.24   2032    1.0\n",
      "mu[4]       -7.47  2.5e-3   0.14  -7.75  -7.55  -7.47  -7.38  -7.17   3359    1.0\n",
      "mu[5]       -7.37  2.7e-3   0.15  -7.64  -7.45  -7.37  -7.29  -7.07   2832    1.0\n",
      "mu[6]       -7.27  4.7e-3   0.17  -7.58  -7.37  -7.28  -7.18  -6.92   1262    1.0\n",
      "mu[7]       -7.17  6.8e-3    0.2  -7.54  -7.29  -7.18  -7.07  -6.75    881    1.0\n",
      "mu[8]       -7.07  9.1e-3   0.25  -7.51  -7.22  -7.08  -6.95  -6.58    736    1.0\n",
      "mu[9]       -6.98    0.01   0.29   -7.5  -7.15  -6.98  -6.83  -6.37    668    1.0\n",
      "ypred       -6.88    0.01   0.56  -7.95   -7.2  -6.87  -6.55  -5.82   1409    1.0\n",
      "log_lik[0]  -0.41    0.02   0.53  -1.72  -0.66   -0.3  -0.04   0.33   1080    1.0\n",
      "log_lik[1]  -0.25    0.01    0.4   -1.2  -0.47  -0.19   0.03   0.38    870    1.0\n",
      "log_lik[2]  -0.35    0.01   0.38  -1.25  -0.56  -0.29  -0.07   0.27   1281    1.0\n",
      "log_lik[3]  -0.79    0.01   0.47  -1.93  -1.04  -0.71  -0.45  -0.09   2207    1.0\n",
      "log_lik[4]  -0.51  7.5e-3   0.34   -1.3   -0.7  -0.47  -0.28   0.04   2079    1.0\n",
      "log_lik[5]   -0.3  8.4e-3    0.3  -0.97  -0.47  -0.27  -0.09   0.23   1280    1.0\n",
      "log_lik[6]   -0.6  9.2e-3    0.4  -1.59  -0.81  -0.55  -0.32   0.03   1921    1.0\n",
      "log_lik[7]  -0.39    0.01   0.41  -1.38   -0.6  -0.33  -0.11   0.25    911    1.0\n",
      "log_lik[8]  -0.59    0.02   0.56  -1.97  -0.87  -0.49   -0.2   0.18   1062    1.0\n",
      "log_lik[9]  -0.21    0.02   0.45   -1.3  -0.42  -0.14   0.08   0.43    589    1.0\n",
      "lp__         3.87    0.08   1.71  -0.49   3.14   4.31   5.07   5.72    485    1.0\n",
      "\n",
      "Samples were drawn using NUTS at Thu Dec  7 13:24:54 2017.\n",
      "For each parameter, n_eff is a crude measure of effective sample size,\n",
      "and Rhat is the potential scale reduction factor on split chains (at \n",
      "convergence, Rhat=1).\n",
      "Inference for Stan model: anon_model_570544391b1e440e4aa07d2bfc218092.\n",
      "4 chains, each with iter=2000; warmup=1000; thin=1; \n",
      "post-warmup draws per chain=1000, total post-warmup draws=4000.\n",
      "\n",
      "             mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\n",
      "alpha      -473.2    4.75  79.12 -653.0 -519.6 -469.9 -419.8 -326.0    277   1.01\n",
      "beta         0.23  2.4e-3   0.04   0.16   0.21   0.23   0.26   0.32    277   1.01\n",
      "sigma        0.36    0.05   0.12    0.2   0.27   0.34   0.43   0.63      6   1.19\n",
      "mu[0]       -8.47  9.5e-3   0.21  -8.94   -8.6  -8.46  -8.33  -8.06    512   1.01\n",
      "mu[1]       -8.24  7.5e-3   0.18  -8.64  -8.35  -8.23  -8.12  -7.89    592   1.01\n",
      "mu[2]        -8.0  5.6e-3   0.15  -8.33   -8.1   -8.0  -7.91  -7.71    761   1.01\n",
      "mu[3]       -7.77  3.7e-3   0.13  -8.06  -7.85  -7.77  -7.69  -7.52   1304   1.01\n",
      "mu[4]       -7.54  2.2e-3   0.12  -7.79  -7.61  -7.54  -7.47   -7.3   2955    1.0\n",
      "mu[5]       -7.31  2.1e-3   0.12  -7.55  -7.38  -7.31  -7.24  -7.07   3176    1.0\n",
      "mu[6]       -7.07  3.3e-3   0.13  -7.32  -7.15  -7.08   -7.0   -6.8   1521    1.0\n",
      "mu[7]       -6.84  5.5e-3   0.15  -7.13  -6.94  -6.85  -6.75  -6.52    754    1.0\n",
      "mu[8]       -6.61  7.5e-3   0.18  -6.94  -6.72  -6.62  -6.51  -6.22    573   1.01\n",
      "mu[9]       -6.38  9.5e-3   0.21  -6.77  -6.51  -6.38  -6.25  -5.91    491   1.01\n",
      "ypred       -6.14    0.01   0.45  -7.02  -6.41  -6.15  -5.88  -5.21   1332   1.01\n",
      "log_lik[0]  -0.59    0.02   0.69  -2.34  -0.89  -0.45  -0.14   0.34   1020    1.0\n",
      "log_lik[1]  -0.08     0.1    0.4  -0.95  -0.31  -0.05    0.2   0.57     17   1.06\n",
      "log_lik[2]  -0.46    0.01   0.48  -1.67   -0.7  -0.38  -0.14   0.24   1093    1.0\n",
      "log_lik[3]   0.08    0.13   0.32  -0.55  -0.14   0.08   0.34   0.65      6   1.18\n",
      "log_lik[4]  -0.14    0.08   0.29  -0.74  -0.33  -0.14   0.06   0.39     14   1.07\n",
      "log_lik[5]   0.05    0.12   0.31  -0.56  -0.16   0.04   0.29   0.59      7   1.17\n",
      "log_lik[6]   0.03    0.12   0.31  -0.58  -0.17   0.03   0.27   0.59      7   1.16\n",
      "log_lik[7]   -0.3    0.01   0.39  -1.21   -0.5  -0.25  -0.04   0.33   1456   1.01\n",
      "log_lik[8]  -0.14    0.07   0.38  -1.01  -0.36  -0.11   0.13    0.5     27   1.04\n",
      "log_lik[9]  -0.82    0.03   0.82  -2.87  -1.18  -0.63  -0.26   0.24    968   1.01\n",
      "lp__         5.75    0.47   1.57   1.93   4.85   6.11   6.96   7.76     11   1.09\n",
      "\n",
      "Samples were drawn using NUTS at Thu Dec  7 13:25:16 2017.\n",
      "For each parameter, n_eff is a crude measure of effective sample size,\n",
      "and Rhat is the potential scale reduction factor on split chains (at \n",
      "convergence, Rhat=1).\n",
      "Inference for Stan model: anon_model_570544391b1e440e4aa07d2bfc218092.\n",
      "4 chains, each with iter=2000; warmup=1000; thin=1; \n",
      "post-warmup draws per chain=1000, total post-warmup draws=4000.\n",
      "\n",
      "             mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\n",
      "alpha       12.84    4.74 104.99 -206.4 -50.14   12.1  74.24 236.82    490   1.01\n",
      "beta        -0.01  2.4e-3   0.05  -0.12  -0.04  -0.01   0.02    0.1    490   1.01\n",
      "sigma        0.44  4.6e-3   0.14   0.25   0.34   0.41    0.5   0.79    882    1.0\n",
      "mu[0]       -7.99    0.01   0.27  -8.53  -8.16   -8.0  -7.83  -7.43    654   1.01\n",
      "mu[1]        -8.0  8.4e-3   0.23  -8.46  -8.14   -8.0  -7.87  -7.52    752   1.01\n",
      "mu[2]       -8.01  5.5e-3   0.19   -8.4  -8.13  -8.02   -7.9  -7.61   1235   1.01\n",
      "mu[3]       -8.02  3.7e-3   0.16  -8.35  -8.12  -8.03  -7.93   -7.7   1896    1.0\n",
      "mu[4]       -8.04  2.4e-3   0.15  -8.32  -8.13  -8.04  -7.94  -7.75   3710    1.0\n",
      "mu[5]       -8.05  2.4e-3   0.15  -8.34  -8.13  -8.04  -7.95  -7.75   3712    1.0\n",
      "mu[6]       -8.06  3.9e-3   0.17  -8.39  -8.15  -8.05  -7.95  -7.74   1838    1.0\n",
      "mu[7]       -8.07  6.2e-3    0.2  -8.48  -8.18  -8.06  -7.95  -7.68   1017    1.0\n",
      "mu[8]       -8.08  8.7e-3   0.24  -8.56  -8.22  -8.07  -7.94  -7.61    732   1.01\n",
      "mu[9]       -8.09    0.01   0.28  -8.66  -8.25  -8.08  -7.92  -7.53    641   1.01\n",
      "ypred        -8.1    0.01   0.56  -9.25  -8.43  -8.09  -7.76   -7.0   1434    1.0\n",
      "log_lik[0]  -0.27    0.02   0.41  -1.28  -0.49   -0.2 9.6e-3   0.35    626   1.01\n",
      "log_lik[1]  -0.26    0.01   0.36  -1.08  -0.45  -0.21  -0.01   0.32   1144    1.0\n",
      "log_lik[2]  -0.78    0.01   0.51  -2.08  -1.05  -0.69  -0.42  -0.05   1884    1.0\n",
      "log_lik[3]  -0.25  9.5e-3   0.31  -0.92  -0.44  -0.22  -0.03   0.28   1032   1.01\n",
      "log_lik[4]  -0.11  9.4e-3   0.29  -0.76  -0.28  -0.08   0.09   0.39    944    1.0\n",
      "log_lik[5]  -0.83  7.7e-3   0.43  -1.88  -1.05  -0.76  -0.53  -0.19   3056    1.0\n",
      "log_lik[6]  -0.12  9.9e-3    0.3   -0.8   -0.3  -0.09   0.09    0.4    908   1.01\n",
      "log_lik[7]  -0.31  9.1e-3   0.34  -1.08  -0.51  -0.27  -0.07   0.25   1414    1.0\n",
      "log_lik[8]  -0.42    0.01   0.43  -1.48  -0.64  -0.36  -0.13   0.22   1481    1.0\n",
      "log_lik[9]  -1.63    0.03   1.14  -4.52   -2.2  -1.38   -0.8   -0.2   1416    1.0\n",
      "lp__         3.33    0.06   1.49  -0.62   2.63   3.74   4.45   5.03    573   1.01\n",
      "\n",
      "Samples were drawn using NUTS at Thu Dec  7 13:25:41 2017.\n",
      "For each parameter, n_eff is a crude measure of effective sample size,\n",
      "and Rhat is the potential scale reduction factor on split chains (at \n",
      "convergence, Rhat=1).\n",
      "Inference for Stan model: anon_model_570544391b1e440e4aa07d2bfc218092.\n",
      "4 chains, each with iter=2000; warmup=1000; thin=1; \n",
      "post-warmup draws per chain=1000, total post-warmup draws=4000.\n",
      "\n",
      "             mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\n",
      "alpha      -108.1   10.68 313.98 -753.2 -308.5 -112.1  93.65 519.69    864    1.0\n",
      "beta         0.05  5.3e-3   0.16  -0.26  -0.05   0.05   0.15   0.37    864    1.0\n",
      "sigma        1.38    0.01   0.41   0.83   1.09    1.3   1.58   2.41   1022    1.0\n",
      "mu[0]      -10.02    0.03   0.83 -11.68 -10.54 -10.02  -9.51  -8.36   1075    1.0\n",
      "mu[1]       -9.97    0.02    0.7 -11.41 -10.41  -9.97  -9.54  -8.58   1199    1.0\n",
      "mu[2]       -9.93    0.02   0.59 -11.13 -10.29  -9.92  -9.56  -8.77   1456    1.0\n",
      "mu[3]       -9.88    0.01    0.5 -10.87 -10.19  -9.87  -9.57  -8.92   2115    1.0\n",
      "mu[4]       -9.83  7.5e-3   0.44 -10.73 -10.11  -9.82  -9.55  -8.96   3506    1.0\n",
      "mu[5]       -9.78  7.4e-3   0.44 -10.68 -10.06  -9.78   -9.5   -8.9   3564    1.0\n",
      "mu[6]       -9.73    0.01   0.49 -10.72 -10.04  -9.73  -9.42  -8.73   2259    1.0\n",
      "mu[7]       -9.68    0.01   0.58 -10.86 -10.05  -9.68  -9.31  -8.49   1556    1.0\n",
      "mu[8]       -9.63    0.02    0.7 -11.08 -10.06  -9.63  -9.19  -8.24   1262    1.0\n",
      "mu[9]       -9.58    0.02   0.83 -11.29  -10.1  -9.58  -9.07  -7.92   1119    1.0\n",
      "ypred       -9.54    0.04   1.74 -13.01 -10.62  -9.55  -8.48  -5.99   1889    1.0\n",
      "log_lik[0]  -1.78    0.02   0.58  -3.26  -2.06  -1.66  -1.37   -1.0   1334    1.0\n",
      "log_lik[1]  -1.81    0.01   0.52  -3.09  -2.07  -1.72  -1.44  -1.06   1465    1.0\n",
      "log_lik[2]  -1.32  9.1e-3    0.3  -1.96  -1.51  -1.29   -1.1  -0.81   1083    1.0\n",
      "log_lik[3]  -1.75  7.6e-3   0.37  -2.58  -1.96  -1.71  -1.49  -1.16   2290    1.0\n",
      "log_lik[4]  -1.51  6.4e-3   0.28   -2.1   -1.7  -1.48  -1.31  -1.01   1975    1.0\n",
      "log_lik[5]  -1.38  7.5e-3   0.28  -1.96  -1.56  -1.36  -1.18  -0.89   1370    1.0\n",
      "log_lik[6]  -1.87  8.2e-3    0.4  -2.83  -2.09  -1.82  -1.59  -1.25   2399    1.0\n",
      "log_lik[7]  -1.36  9.0e-3   0.31  -2.03  -1.56  -1.34  -1.14  -0.82   1193    1.0\n",
      "log_lik[8]  -1.93    0.02   0.59   -3.4  -2.21   -1.8  -1.52  -1.14   1414    1.0\n",
      "log_lik[9]  -1.86    0.02   0.64  -3.45  -2.14  -1.71  -1.41  -1.03   1258    1.0\n",
      "lp__        -7.08    0.05   1.33 -10.43  -7.76  -6.73  -6.09  -5.51    827    1.0\n",
      "\n",
      "Samples were drawn using NUTS at Thu Dec  7 13:26:09 2017.\n",
      "For each parameter, n_eff is a crude measure of effective sample size,\n",
      "and Rhat is the potential scale reduction factor on split chains (at \n",
      "convergence, Rhat=1).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference for Stan model: anon_model_570544391b1e440e4aa07d2bfc218092.\n",
      "4 chains, each with iter=2000; warmup=1000; thin=1; \n",
      "post-warmup draws per chain=1000, total post-warmup draws=4000.\n",
      "\n",
      "             mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\n",
      "alpha       29.23    1.69  42.44  -51.2   3.59  29.07  54.79 115.83    628   1.01\n",
      "beta        -0.02  8.4e-4   0.02  -0.06  -0.03  -0.02-5.7e-3   0.02    628   1.01\n",
      "sigma        0.18  2.2e-3   0.06   0.11   0.14   0.17    0.2   0.35    747    1.0\n",
      "mu[0]       -7.77  3.7e-3   0.11  -7.99  -7.83  -7.77   -7.7  -7.53    938   1.01\n",
      "mu[1]       -7.78  3.0e-3    0.1  -7.97  -7.84  -7.78  -7.73  -7.59   1049   1.01\n",
      "mu[2]        -7.8  2.3e-3   0.08  -7.96  -7.85   -7.8  -7.76  -7.64   1279   1.01\n",
      "mu[3]       -7.82  1.5e-3   0.07  -7.95  -7.86  -7.82  -7.78  -7.69   2064    1.0\n",
      "mu[4]       -7.84  1.1e-3   0.06  -7.96  -7.88  -7.84   -7.8  -7.72   3203    1.0\n",
      "mu[5]       -7.86  1.0e-3   0.06  -7.98  -7.89  -7.86  -7.82  -7.74   3747    1.0\n",
      "mu[6]       -7.88  1.3e-3   0.07  -8.01  -7.92  -7.88  -7.84  -7.74   2641    1.0\n",
      "mu[7]        -7.9  1.9e-3   0.08  -8.05  -7.94   -7.9  -7.85  -7.74   1715    1.0\n",
      "mu[8]       -7.91  2.8e-3   0.09   -8.1  -7.97  -7.91  -7.86  -7.73   1122   1.01\n",
      "mu[9]       -7.93  3.6e-3   0.11  -8.15   -8.0  -7.93  -7.86  -7.71    965   1.01\n",
      "ypred       -7.95  5.2e-3   0.23  -8.42  -8.09  -7.95  -7.81   -7.5   2029    1.0\n",
      "log_lik[0]   0.05    0.02   0.71  -1.77  -0.25    0.2   0.56   0.98   1683    1.0\n",
      "log_lik[1]   0.71    0.01   0.35  -0.11    0.5   0.75   0.95   1.27    888    1.0\n",
      "log_lik[2]    0.7    0.01   0.33  -0.05    0.5   0.74   0.93   1.24    947    1.0\n",
      "log_lik[3]   0.75    0.01   0.31   0.03   0.57   0.79   0.97   1.27    879    1.0\n",
      "log_lik[4]   0.17  7.1e-3   0.39  -0.74  -0.04   0.22   0.44   0.76   2995    1.0\n",
      "log_lik[5]   0.51  7.5e-3    0.3  -0.15   0.32   0.53   0.72   1.01   1590    1.0\n",
      "log_lik[6]  -0.29    0.01   0.62  -1.85   -0.6  -0.17   0.14   0.56   2482    1.0\n",
      "log_lik[7]   0.26    0.01   0.45  -0.82   0.02   0.32   0.57   0.94   1992    1.0\n",
      "log_lik[8]   0.63    0.01   0.37  -0.21   0.42   0.68   0.89   1.22   1124    1.0\n",
      "log_lik[9]   0.39    0.01   0.52  -0.95   0.13   0.49   0.75   1.12   1354    1.0\n",
      "lp__         11.3    0.06   1.51   7.29  10.67  11.68  12.41   13.0    727    1.0\n",
      "\n",
      "Samples were drawn using NUTS at Thu Dec  7 13:26:40 2017.\n",
      "For each parameter, n_eff is a crude measure of effective sample size,\n",
      "and Rhat is the potential scale reduction factor on split chains (at \n",
      "convergence, Rhat=1).\n",
      "Inference for Stan model: anon_model_570544391b1e440e4aa07d2bfc218092.\n",
      "4 chains, each with iter=2000; warmup=1000; thin=1; \n",
      "post-warmup draws per chain=1000, total post-warmup draws=4000.\n",
      "\n",
      "             mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\n",
      "alpha      -199.9    4.98 137.74 -492.9 -278.8 -196.9 -115.2  65.56    765    1.0\n",
      "beta          0.1  2.5e-3   0.07  -0.04   0.05    0.1   0.14   0.24    765    1.0\n",
      "sigma        0.61  8.1e-3   0.19   0.36   0.48   0.58   0.71   1.08    554   1.01\n",
      "mu[0]       -6.21    0.01   0.37  -6.99  -6.42   -6.2  -5.99  -5.52    921    1.0\n",
      "mu[1]       -6.12  9.8e-3   0.31  -6.78   -6.3   -6.1  -5.92  -5.53   1030    1.0\n",
      "mu[2]       -6.02  7.5e-3   0.27  -6.59  -6.17  -6.01  -5.85  -5.52   1258    1.0\n",
      "mu[3]       -5.92  5.4e-3   0.23  -6.42  -6.06  -5.92  -5.78  -5.49   1813    1.0\n",
      "mu[4]       -5.83  3.6e-3   0.21  -6.27  -5.95  -5.83  -5.69  -5.42   3375    1.0\n",
      "mu[5]       -5.73  3.5e-3   0.21  -6.17  -5.85  -5.73   -5.6  -5.32   3511    1.0\n",
      "mu[6]       -5.63  4.8e-3   0.23   -6.1  -5.77  -5.63  -5.49  -5.17   2339    1.0\n",
      "mu[7]       -5.54  6.7e-3   0.27  -6.08   -5.7  -5.54  -5.38   -5.0   1610    1.0\n",
      "mu[8]       -5.44  8.9e-3   0.32  -6.07  -5.63  -5.45  -5.25  -4.81   1280    1.0\n",
      "mu[9]       -5.34    0.01   0.37  -6.08  -5.56  -5.36  -5.12   -4.6   1107    1.0\n",
      "ypred       -5.25    0.02   0.77  -6.78  -5.73  -5.26  -4.78  -3.69   2273    1.0\n",
      "log_lik[0]  -2.62    0.04   1.47  -6.22  -3.39  -2.28  -1.51  -0.78   1158    1.0\n",
      "log_lik[1]  -0.68    0.01    0.4  -1.67  -0.89  -0.63  -0.41  -0.08    759    1.0\n",
      "log_lik[2]  -0.94    0.01   0.44  -2.04  -1.17  -0.88  -0.64  -0.29   1289    1.0\n",
      "log_lik[3]  -0.73    0.01   0.32  -1.46  -0.93   -0.7   -0.5   -0.2    951    1.0\n",
      "log_lik[4]  -0.72  9.2e-3    0.3  -1.39   -0.9   -0.7  -0.51  -0.22   1051    1.0\n",
      "log_lik[5]  -0.45    0.01   0.29  -1.08  -0.63  -0.43  -0.24   0.06    589   1.01\n",
      "log_lik[6]  -0.49    0.01   0.29  -1.12  -0.67  -0.46  -0.28   0.01    626   1.01\n",
      "log_lik[7]  -0.48    0.01   0.31  -1.15  -0.67  -0.46  -0.27   0.04    582   1.01\n",
      "log_lik[8]  -0.54    0.01   0.34  -1.28  -0.73   -0.5   -0.3   0.01    611   1.01\n",
      "log_lik[9]  -0.67    0.02   0.43   -1.7  -0.89  -0.61  -0.38  -0.05    795    1.0\n",
      "lp__         0.34    0.06   1.43  -3.25  -0.31   0.72   1.39   1.95    552   1.01\n",
      "\n",
      "Samples were drawn using NUTS at Thu Dec  7 13:27:04 2017.\n",
      "For each parameter, n_eff is a crude measure of effective sample size,\n",
      "and Rhat is the potential scale reduction factor on split chains (at \n",
      "convergence, Rhat=1).\n",
      "Inference for Stan model: anon_model_570544391b1e440e4aa07d2bfc218092.\n",
      "4 chains, each with iter=2000; warmup=1000; thin=1; \n",
      "post-warmup draws per chain=1000, total post-warmup draws=4000.\n",
      "\n",
      "             mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\n",
      "alpha        -5.4    0.59  16.61 -39.09 -15.33  -5.58   4.77  28.34    780   1.01\n",
      "beta      -9.8e-4  3.0e-4 8.3e-3  -0.02-6.1e-3-8.9e-4 4.0e-3   0.02    780   1.01\n",
      "sigma        0.07  7.6e-4   0.02   0.04   0.05   0.06   0.08   0.13    828    1.0\n",
      "mu[0]       -7.36  1.4e-3   0.04  -7.45  -7.39  -7.36  -7.33  -7.27    950    1.0\n",
      "mu[1]       -7.36  1.1e-3   0.04  -7.44  -7.38  -7.36  -7.34  -7.29   1058    1.0\n",
      "mu[2]       -7.36  8.5e-4   0.03  -7.43  -7.38  -7.36  -7.34   -7.3   1286    1.0\n",
      "mu[3]       -7.36  6.0e-4   0.03  -7.41  -7.38  -7.36  -7.35  -7.31   1833    1.0\n",
      "mu[4]       -7.36  4.0e-4   0.02  -7.41  -7.38  -7.36  -7.35  -7.32   3275    1.0\n",
      "mu[5]       -7.37  3.9e-4   0.02  -7.41  -7.38  -7.37  -7.35  -7.32   3407    1.0\n",
      "mu[6]       -7.37  5.4e-4   0.03  -7.42  -7.38  -7.37  -7.35  -7.32   2244    1.0\n",
      "mu[7]       -7.37  7.9e-4   0.03  -7.43  -7.39  -7.37  -7.35  -7.31   1496    1.0\n",
      "mu[8]       -7.37  1.1e-3   0.04  -7.44  -7.39  -7.37  -7.35   -7.3   1158   1.01\n",
      "mu[9]       -7.37  1.4e-3   0.04  -7.46   -7.4  -7.37  -7.34  -7.28   1023   1.01\n",
      "ypred       -7.37  1.8e-3   0.09  -7.54  -7.42  -7.37  -7.32   -7.2   2224    1.0\n",
      "log_lik[0]   1.62    0.01   0.39   0.67   1.43   1.68   1.88   2.22    778    1.0\n",
      "log_lik[1]   1.44    0.01   0.43   0.41   1.21   1.51   1.74   2.08   1001    1.0\n",
      "log_lik[2]   0.94    0.01   0.57  -0.45   0.67   1.05   1.35   1.73   1555    1.0\n",
      "log_lik[3]   1.55  8.9e-3    0.3   0.89   1.37   1.58   1.77   2.07   1172    1.0\n",
      "log_lik[4]   1.68  9.2e-3   0.29   1.05   1.51   1.71   1.88   2.17    972    1.0\n",
      "log_lik[5]   1.51  7.4e-3   0.29   0.92   1.33   1.53   1.72   2.01   1499    1.0\n",
      "log_lik[6]   1.69  9.4e-3    0.3   1.04   1.51   1.72    1.9   2.19   1000    1.0\n",
      "log_lik[7]   0.82    0.01   0.63  -0.77   0.49   0.95   1.26   1.68   1821    1.0\n",
      "log_lik[8]   0.59    0.02   0.83  -1.47   0.17   0.77   1.19    1.7   1401    1.0\n",
      "log_lik[9]   1.59    0.01   0.41   0.63   1.38   1.66   1.87   2.19    871    1.0\n",
      "lp__        19.91    0.05   1.48   16.0  19.27  20.32  20.98  21.58    786    1.0\n",
      "\n",
      "Samples were drawn using NUTS at Thu Dec  7 13:27:35 2017.\n",
      "For each parameter, n_eff is a crude measure of effective sample size,\n",
      "and Rhat is the potential scale reduction factor on split chains (at \n",
      "convergence, Rhat=1).\n",
      "Inference for Stan model: anon_model_570544391b1e440e4aa07d2bfc218092.\n",
      "4 chains, each with iter=2000; warmup=1000; thin=1; \n",
      "post-warmup draws per chain=1000, total post-warmup draws=4000.\n",
      "\n",
      "             mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\n",
      "alpha      -53.71    1.64   47.7 -144.7  -86.1 -53.56 -23.72  47.08    844    1.0\n",
      "beta         0.02  8.2e-4   0.02  -0.03 8.4e-3   0.02   0.04   0.07    844    1.0\n",
      "sigma        0.22  2.2e-3   0.07   0.13   0.17    0.2   0.24   0.39    951    1.0\n",
      "mu[0]       -7.01  3.9e-3   0.13  -7.27  -7.09  -7.01  -6.94  -6.76   1030    1.0\n",
      "mu[1]       -6.99  3.2e-3   0.11   -7.2  -7.06  -6.99  -6.92  -6.77   1147    1.0\n",
      "mu[2]       -6.97  2.4e-3   0.09  -7.15  -7.02  -6.96  -6.91  -6.78   1381    1.0\n",
      "mu[3]       -6.94  1.8e-3   0.08   -7.1  -6.99  -6.94  -6.89  -6.79   1883    1.0\n",
      "mu[4]       -6.92  1.1e-3   0.07  -7.06  -6.96  -6.92  -6.88  -6.78   4000    1.0\n",
      "mu[5]        -6.9  1.3e-3   0.07  -7.04  -6.94   -6.9  -6.85  -6.75   3106    1.0\n",
      "mu[6]       -6.87  1.7e-3   0.08  -7.03  -6.92  -6.87  -6.82  -6.71   2142    1.0\n",
      "mu[7]       -6.85  2.4e-3   0.09  -7.04  -6.91  -6.85  -6.79  -6.66   1573    1.0\n",
      "mu[8]       -6.83  3.1e-3   0.11  -7.05  -6.89  -6.82  -6.76  -6.61   1299    1.0\n",
      "mu[9]        -6.8  3.9e-3   0.13  -7.07  -6.88   -6.8  -6.72  -6.55   1126    1.0\n",
      "ypred       -6.78  5.7e-3   0.27  -7.34  -6.95  -6.78  -6.61  -6.25   2289    1.0\n",
      "log_lik[0]  -0.05    0.02   0.64  -1.76  -0.34   0.09    0.4   0.81    906    1.0\n",
      "log_lik[1]   0.52    0.01   0.33  -0.19   0.33   0.55   0.76   1.08   1014    1.0\n",
      "log_lik[2]  -0.75    0.02    0.8   -2.7  -1.14  -0.57  -0.19   0.33   1569    1.0\n",
      "log_lik[3]   0.47  8.7e-3   0.29  -0.15   0.29    0.5   0.68   0.96   1112    1.0\n",
      "log_lik[4]   0.61  9.4e-3   0.29  -0.03   0.43   0.63   0.81   1.12    966    1.0\n",
      "log_lik[5]   0.48  8.1e-3   0.28  -0.14    0.3    0.5   0.68   0.95   1202    1.0\n",
      "log_lik[6]  -0.19    0.01   0.48   -1.4  -0.43  -0.11   0.15   0.52   1941    1.0\n",
      "log_lik[7]   0.03    0.01   0.46  -1.11  -0.21   0.09   0.35   0.71   1983    1.0\n",
      "log_lik[8]   0.52    0.01   0.33  -0.22   0.32   0.55   0.76   1.07    844    1.0\n",
      "log_lik[9]   0.48    0.01   0.36  -0.34   0.26   0.52   0.74   1.07    878    1.0\n",
      "lp__         9.73    0.05   1.37   6.27   9.05  10.06  10.77  11.37    775    1.0\n",
      "\n",
      "Samples were drawn using NUTS at Thu Dec  7 13:28:00 2017.\n",
      "For each parameter, n_eff is a crude measure of effective sample size,\n",
      "and Rhat is the potential scale reduction factor on split chains (at \n",
      "convergence, Rhat=1).\n",
      "Inference for Stan model: anon_model_570544391b1e440e4aa07d2bfc218092.\n",
      "4 chains, each with iter=2000; warmup=1000; thin=1; \n",
      "post-warmup draws per chain=1000, total post-warmup draws=4000.\n",
      "\n",
      "             mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\n",
      "alpha       30.21     1.1  32.21 -36.82  11.91  31.24  48.78  92.46    858    1.0\n",
      "beta        -0.02  5.5e-4   0.02  -0.05  -0.03  -0.02  -0.01   0.01    858    1.0\n",
      "sigma        0.14  1.5e-3   0.05   0.08   0.11   0.13   0.16   0.26    897   1.01\n",
      "mu[0]        -8.4  2.5e-3   0.08  -8.57  -8.45  -8.39  -8.35  -8.24   1137    1.0\n",
      "mu[1]       -8.42  2.0e-3   0.07  -8.56  -8.46  -8.41  -8.37  -8.28   1297    1.0\n",
      "mu[2]       -8.44  1.5e-3   0.06  -8.56  -8.47  -8.43   -8.4  -8.32   1646    1.0\n",
      "mu[3]       -8.45  1.0e-3   0.05  -8.56  -8.49  -8.45  -8.42  -8.36   2386    1.0\n",
      "mu[4]       -8.47  7.6e-4   0.05  -8.57   -8.5  -8.47  -8.45  -8.38   3697    1.0\n",
      "mu[5]       -8.49  7.7e-4   0.05  -8.58  -8.52  -8.49  -8.46   -8.4   3622    1.0\n",
      "mu[6]       -8.51  1.1e-3   0.05  -8.61  -8.54  -8.51  -8.48  -8.41   2075    1.0\n",
      "mu[7]       -8.53  1.6e-3   0.06  -8.65  -8.57  -8.53  -8.49  -8.41   1459    1.0\n",
      "mu[8]       -8.55  2.1e-3   0.07  -8.69   -8.6  -8.55  -8.51   -8.4   1199    1.0\n",
      "mu[9]       -8.57  2.6e-3   0.09  -8.74  -8.62  -8.57  -8.52   -8.4   1073    1.0\n",
      "ypred       -8.59  4.0e-3   0.18  -8.94  -8.69  -8.59  -8.48  -8.22   2038    1.0\n",
      "log_lik[0]   0.93    0.01   0.37   0.04   0.73   0.98   1.19    1.5    915   1.01\n",
      "log_lik[1]  -0.33    0.02   0.92  -2.61  -0.82  -0.12   0.35   0.87   1881    1.0\n",
      "log_lik[2]   0.58  9.7e-3   0.42  -0.42   0.35   0.64   0.89   1.22   1855    1.0\n",
      "log_lik[3]   1.01  9.7e-3    0.3    0.3   0.83   1.04   1.22   1.51    982   1.01\n",
      "log_lik[4]   -0.3    0.01   0.69  -2.01  -0.64  -0.15    0.2   0.63   2313    1.0\n",
      "log_lik[5]   0.86  8.3e-3   0.29    0.2   0.68   0.89   1.07   1.36   1257   1.01\n",
      "log_lik[6]   1.04    0.01    0.3   0.35   0.85   1.07   1.25   1.54    887   1.01\n",
      "log_lik[7]    1.0    0.01   0.32   0.28   0.81   1.04   1.23   1.52    874   1.01\n",
      "log_lik[8]   0.97    0.01   0.34    0.2   0.78   1.02   1.22   1.52    912   1.01\n",
      "log_lik[9]   0.79    0.01   0.45  -0.29   0.55   0.87   1.11   1.45   1013   1.01\n",
      "lp__        13.72    0.06   1.49   9.65  13.08  14.13  14.81  15.38    720   1.01\n",
      "\n",
      "Samples were drawn using NUTS at Thu Dec  7 13:28:26 2017.\n",
      "For each parameter, n_eff is a crude measure of effective sample size,\n",
      "and Rhat is the potential scale reduction factor on split chains (at \n",
      "convergence, Rhat=1).\n",
      "Inference for Stan model: anon_model_570544391b1e440e4aa07d2bfc218092.\n",
      "4 chains, each with iter=2000; warmup=1000; thin=1; \n",
      "post-warmup draws per chain=1000, total post-warmup draws=4000.\n",
      "\n",
      "             mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\n",
      "alpha      -13.08     5.6 155.12 -315.2 -107.5 -15.65  75.79  305.8    767   1.01\n",
      "beta       2.7e-3  2.8e-3   0.08  -0.16  -0.04 3.9e-3   0.05   0.15    767   1.01\n",
      "sigma        0.64  8.6e-3    0.2   0.38    0.5   0.59   0.74   1.16    571   1.01\n",
      "mu[0]       -7.76    0.01    0.4  -8.57   -8.0  -7.76  -7.53  -6.94    995   1.01\n",
      "mu[1]       -7.76    0.01   0.34  -8.43  -7.96  -7.76  -7.56  -7.07   1129    1.0\n",
      "mu[2]       -7.76  7.6e-3   0.28  -8.33  -7.92  -7.76  -7.59  -7.16   1412    1.0\n",
      "mu[3]       -7.75  5.2e-3   0.24  -8.24   -7.9  -7.75  -7.61  -7.27   2136    1.0\n",
      "mu[4]       -7.75  3.6e-3   0.22  -8.19  -7.88  -7.75  -7.62  -7.32   3480    1.0\n",
      "mu[5]       -7.75  3.8e-3   0.22  -8.17  -7.88  -7.75  -7.61  -7.31   3294    1.0\n",
      "mu[6]       -7.75  5.6e-3   0.24  -8.25  -7.89  -7.75   -7.6  -7.28   1931    1.0\n",
      "mu[7]       -7.74  8.0e-3   0.29  -8.34  -7.91  -7.74  -7.57  -7.19   1334    1.0\n",
      "mu[8]       -7.74    0.01   0.35  -8.47  -7.94  -7.74  -7.53  -7.08   1089   1.01\n",
      "mu[9]       -7.74    0.01   0.41   -8.6  -7.97  -7.74  -7.49  -6.96    972   1.01\n",
      "ypred       -7.73    0.02   0.82  -9.46  -8.21  -7.73  -7.25  -6.15   2031    1.0\n",
      "log_lik[0]  -0.63    0.02    0.4  -1.54  -0.84  -0.57  -0.36  -0.02    709    1.0\n",
      "log_lik[1]  -0.56    0.01   0.35  -1.35  -0.76  -0.52  -0.32 3.5e-3    669    1.0\n",
      "log_lik[2]  -0.58    0.01   0.33  -1.29  -0.77  -0.54  -0.34  -0.04    775    1.0\n",
      "log_lik[3]  -1.96    0.02   0.82  -3.92  -2.37  -1.77  -1.37  -0.88   1734    1.0\n",
      "log_lik[4]  -1.19  7.4e-3    0.4  -2.18  -1.41  -1.14   -0.9  -0.58   2980    1.0\n",
      "log_lik[5]  -0.57    0.01   0.29  -1.21  -0.76  -0.55  -0.36  -0.07    816    1.0\n",
      "log_lik[6]  -0.64    0.01   0.31  -1.31  -0.84  -0.62  -0.43  -0.12    901    1.0\n",
      "log_lik[7]  -0.78    0.01   0.37  -1.67  -0.98  -0.74  -0.52  -0.18   1332   1.01\n",
      "log_lik[8]  -1.35    0.02   0.69   -3.2  -1.68  -1.19  -0.88  -0.45   1654    1.0\n",
      "log_lik[9]  -0.61    0.02   0.39  -1.55  -0.82  -0.55  -0.34  -0.04    649   1.01\n",
      "lp__        -0.17    0.06   1.51  -4.14  -0.87   0.22   0.94   1.54    567    1.0\n",
      "\n",
      "Samples were drawn using NUTS at Thu Dec  7 13:28:56 2017.\n",
      "For each parameter, n_eff is a crude measure of effective sample size,\n",
      "and Rhat is the potential scale reduction factor on split chains (at \n",
      "convergence, Rhat=1).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference for Stan model: anon_model_570544391b1e440e4aa07d2bfc218092.\n",
      "4 chains, each with iter=2000; warmup=1000; thin=1; \n",
      "post-warmup draws per chain=1000, total post-warmup draws=4000.\n",
      "\n",
      "             mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\n",
      "alpha       10.91    2.66  81.67 -154.4 -38.03   9.68  63.69 173.75    946    1.0\n",
      "beta      -9.5e-3  1.3e-3   0.04  -0.09  -0.04-9.0e-3   0.01   0.07    946    1.0\n",
      "sigma        0.34  3.4e-3    0.1   0.21   0.27   0.32   0.39    0.6    932    1.0\n",
      "mu[0]       -8.19  6.4e-3   0.22  -8.63  -8.32  -8.19  -8.05  -7.76   1141    1.0\n",
      "mu[1]        -8.2  5.1e-3   0.18  -8.57  -8.31   -8.2  -8.08  -7.84   1257    1.0\n",
      "mu[2]       -8.21  3.9e-3   0.15  -8.52   -8.3  -8.21  -8.11  -7.91   1493    1.0\n",
      "mu[3]       -8.22  2.8e-3   0.13  -8.48  -8.29  -8.22  -8.14  -7.97   2034    1.0\n",
      "mu[4]       -8.23  1.9e-3   0.11  -8.46  -8.29  -8.23  -8.16   -8.0   3469    1.0\n",
      "mu[5]       -8.24  1.8e-3   0.11  -8.46  -8.31  -8.23  -8.17  -8.01   3683    1.0\n",
      "mu[6]       -8.25  2.5e-3   0.12  -8.49  -8.32  -8.24  -8.17   -8.0   2379    1.0\n",
      "mu[7]       -8.26  3.6e-3   0.15  -8.56  -8.35  -8.25  -8.17  -7.96   1678    1.0\n",
      "mu[8]       -8.27  4.8e-3   0.18  -8.63  -8.37  -8.27  -8.16  -7.91   1368    1.0\n",
      "mu[9]       -8.28  6.0e-3   0.21   -8.7   -8.4  -8.27  -8.14  -7.86   1217    1.0\n",
      "ypred       -8.28  9.7e-3   0.44  -9.18  -8.55  -8.28  -8.01  -7.43   2065    1.0\n",
      "log_lik[0]  -0.38    0.02   0.59  -1.85  -0.65  -0.25   0.02   0.41   1259    1.0\n",
      "log_lik[1]   0.04    0.01   0.34  -0.75  -0.15   0.08   0.28   0.58    880    1.0\n",
      "log_lik[2]  -0.51    0.01    0.5  -1.76  -0.75  -0.41  -0.16   0.19   1676    1.0\n",
      "log_lik[3]   0.12  9.3e-3   0.29   -0.5  -0.06   0.15   0.33    0.6    967    1.0\n",
      "log_lik[4]   0.14  8.9e-3   0.28  -0.47  -0.04   0.16   0.34   0.61    994    1.0\n",
      "log_lik[5]   0.12  8.8e-3   0.28  -0.49  -0.06   0.14   0.32   0.59   1007    1.0\n",
      "log_lik[6]  -0.05  8.3e-3    0.3  -0.71  -0.23  -0.02   0.16   0.45   1307    1.0\n",
      "log_lik[7]  -0.44    0.01   0.46  -1.54  -0.69  -0.37  -0.13   0.24   1854    1.0\n",
      "log_lik[8]  -0.83    0.02   0.73  -2.59  -1.19  -0.68  -0.32   0.12   1547    1.0\n",
      "log_lik[9]  -0.76    0.02   0.81  -2.91   -1.1  -0.57   -0.2   0.24   1829    1.0\n",
      "lp__         5.53    0.06   1.45   1.76   4.85    5.9   6.59   7.17    678    1.0\n",
      "\n",
      "Samples were drawn using NUTS at Thu Dec  7 13:29:21 2017.\n",
      "For each parameter, n_eff is a crude measure of effective sample size,\n",
      "and Rhat is the potential scale reduction factor on split chains (at \n",
      "convergence, Rhat=1).\n",
      "Inference for Stan model: anon_model_570544391b1e440e4aa07d2bfc218092.\n",
      "4 chains, each with iter=2000; warmup=1000; thin=1; \n",
      "post-warmup draws per chain=1000, total post-warmup draws=4000.\n",
      "\n",
      "             mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\n",
      "alpha      170.69    4.21 125.53  -81.5  93.06 171.22 249.25 419.34    891    1.0\n",
      "beta        -0.09  2.1e-3   0.06  -0.21  -0.13  -0.09  -0.05   0.04    891    1.0\n",
      "sigma        0.56  8.7e-3   0.18   0.34   0.45   0.52   0.63   0.99    441   1.01\n",
      "mu[0]       -7.95  9.8e-3   0.34  -8.62  -8.16  -7.95  -7.73  -7.28   1187    1.0\n",
      "mu[1]       -8.04  7.8e-3   0.29  -8.61  -8.22  -8.04  -7.85  -7.46   1345    1.0\n",
      "mu[2]       -8.13  5.9e-3   0.24  -8.61  -8.28  -8.13  -7.98  -7.65   1666    1.0\n",
      "mu[3]       -8.22  4.2e-3   0.21  -8.62  -8.34  -8.22  -8.09   -7.8   2412    1.0\n",
      "mu[4]       -8.31  3.0e-3   0.18  -8.67  -8.42  -8.31   -8.2  -7.92   3745    1.0\n",
      "mu[5]       -8.39  3.0e-3   0.18  -8.75  -8.51  -8.39  -8.29  -8.01   3696    1.0\n",
      "mu[6]       -8.48  4.4e-3    0.2  -8.88  -8.61  -8.48  -8.36  -8.06   2093    1.0\n",
      "mu[7]       -8.57  6.1e-3   0.24  -9.04  -8.73  -8.57  -8.43  -8.09   1503    1.0\n",
      "mu[8]       -8.66  8.0e-3   0.28  -9.23  -8.85  -8.66  -8.48  -8.12   1241    1.0\n",
      "mu[9]       -8.75    0.01   0.33  -9.42  -8.96  -8.75  -8.54   -8.1   1112    1.0\n",
      "ypred       -8.83    0.01   0.71 -10.22  -9.26  -8.82   -8.4  -7.43   2322    1.0\n",
      "log_lik[0]  -0.56    0.01    0.4  -1.57  -0.76  -0.49  -0.27   0.03    997   1.01\n",
      "log_lik[1]   -0.5    0.01   0.35  -1.35   -0.7  -0.46  -0.25   0.05    790   1.01\n",
      "log_lik[2]  -0.53    0.01   0.33  -1.29  -0.72   -0.5   -0.3 2.7e-3    920   1.01\n",
      "log_lik[3]  -0.46    0.01   0.29  -1.08  -0.64  -0.43  -0.25   0.03    706   1.01\n",
      "log_lik[4]  -0.35    0.01   0.28  -0.97  -0.52  -0.33  -0.15   0.13    600   1.01\n",
      "log_lik[5]  -1.04  7.7e-3   0.41  -1.98  -1.27  -0.98  -0.76  -0.44   2773    1.0\n",
      "log_lik[6]  -0.77  8.3e-3   0.35  -1.56  -0.97  -0.73  -0.52  -0.22   1797    1.0\n",
      "log_lik[7]  -0.45    0.01   0.31  -1.12  -0.63  -0.42  -0.23   0.07    729   1.01\n",
      "log_lik[8]  -1.33    0.02   0.72  -3.13  -1.68  -1.18  -0.82  -0.35   1490    1.0\n",
      "log_lik[9]  -1.43    0.02    0.9  -3.66  -1.87  -1.23  -0.79  -0.32   1401    1.0\n",
      "lp__         1.14    0.07   1.43  -2.57   0.52   1.51   2.18   2.76    434   1.01\n",
      "\n",
      "Samples were drawn using NUTS at Thu Dec  7 13:29:46 2017.\n",
      "For each parameter, n_eff is a crude measure of effective sample size,\n",
      "and Rhat is the potential scale reduction factor on split chains (at \n",
      "convergence, Rhat=1).\n",
      "Inference for Stan model: anon_model_570544391b1e440e4aa07d2bfc218092.\n",
      "4 chains, each with iter=2000; warmup=1000; thin=1; \n",
      "post-warmup draws per chain=1000, total post-warmup draws=4000.\n",
      "\n",
      "             mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\n",
      "alpha      -108.2    1.65  45.56 -199.3 -137.0 -108.6 -77.71 -18.86    765   1.01\n",
      "beta         0.05  8.2e-4   0.02 5.7e-3   0.04   0.05   0.06    0.1    765   1.01\n",
      "sigma         0.2  2.6e-3   0.07   0.12   0.15   0.18   0.23   0.37    635   1.01\n",
      "mu[0]       -7.61  3.7e-3   0.12  -7.87  -7.69  -7.61  -7.53  -7.38   1100    1.0\n",
      "mu[1]       -7.56  3.0e-3    0.1  -7.78  -7.63  -7.56   -7.5  -7.36   1240    1.0\n",
      "mu[2]       -7.51  2.3e-3   0.09   -7.7  -7.57  -7.51  -7.46  -7.34   1525    1.0\n",
      "mu[3]       -7.46  1.6e-3   0.08  -7.62  -7.51  -7.46  -7.41  -7.31   2186    1.0\n",
      "mu[4]       -7.41  1.2e-3   0.07  -7.55  -7.45  -7.41  -7.37  -7.28   3311    1.0\n",
      "mu[5]       -7.36  1.2e-3   0.07   -7.5   -7.4  -7.36  -7.32  -7.22   3310    1.0\n",
      "mu[6]       -7.31  1.6e-3   0.08  -7.46  -7.36  -7.31  -7.27  -7.16   2119    1.0\n",
      "mu[7]       -7.26  2.2e-3   0.09  -7.43  -7.32  -7.26  -7.21  -7.08   1539    1.0\n",
      "mu[8]       -7.21  3.2e-3    0.1  -7.42  -7.28  -7.21  -7.15   -7.0   1068    1.0\n",
      "mu[9]       -7.16  4.0e-3   0.12   -7.4  -7.24  -7.16  -7.08  -6.91    955    1.0\n",
      "ypred       -7.11  5.8e-3   0.25  -7.61  -7.26  -7.11  -6.96  -6.59   1950    1.0\n",
      "log_lik[0]  -0.29    0.02   0.85  -2.44  -0.69  -0.07   0.32   0.79   1561    1.0\n",
      "log_lik[1]    0.3    0.01   0.47  -0.83   0.06   0.37   0.62   0.99   1151    1.0\n",
      "log_lik[2]   0.65    0.01   0.32  -0.05   0.46   0.69   0.88   1.18    659   1.01\n",
      "log_lik[3]   0.55    0.01   0.31  -0.12   0.36   0.57   0.77   1.08    940   1.01\n",
      "log_lik[4]   0.11  6.8e-3   0.38  -0.77  -0.09   0.16   0.38   0.71   3144    1.0\n",
      "log_lik[5]   0.69    0.01   0.31   0.02    0.5   0.72   0.91   1.19    713   1.01\n",
      "log_lik[6]   0.45  9.4e-3   0.32  -0.26   0.25   0.49   0.68    1.0   1186    1.0\n",
      "log_lik[7]   0.63    0.01   0.33  -0.09   0.43   0.66   0.86   1.15    796   1.01\n",
      "log_lik[8]   0.27    0.01   0.48  -0.96   0.02   0.34    0.6   0.97   1525    1.0\n",
      "log_lik[9]  -0.41    0.02   0.91  -2.67  -0.85  -0.19   0.25   0.75   1504    1.0\n",
      "lp__        10.49    0.06    1.5   6.73   9.78  10.89  11.59  12.23    559   1.01\n",
      "\n",
      "Samples were drawn using NUTS at Thu Dec  7 13:30:12 2017.\n",
      "For each parameter, n_eff is a crude measure of effective sample size,\n",
      "and Rhat is the potential scale reduction factor on split chains (at \n",
      "convergence, Rhat=1).\n",
      "Inference for Stan model: anon_model_570544391b1e440e4aa07d2bfc218092.\n",
      "4 chains, each with iter=2000; warmup=1000; thin=1; \n",
      "post-warmup draws per chain=1000, total post-warmup draws=4000.\n",
      "\n",
      "             mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\n",
      "alpha      -91.85    0.54  15.98 -121.7 -101.7 -92.24 -82.33 -60.34    873   1.01\n",
      "beta         0.04  2.7e-4 8.0e-3   0.03   0.04   0.04   0.05   0.06    873   1.01\n",
      "sigma        0.07  9.2e-4   0.02   0.04   0.06   0.07   0.08   0.14    688    1.0\n",
      "mu[0]       -8.02  1.3e-3   0.04  -8.11  -8.05  -8.02   -8.0  -7.93   1143    1.0\n",
      "mu[1]       -7.98  1.0e-3   0.04  -8.05   -8.0  -7.98  -7.96  -7.91   1287    1.0\n",
      "mu[2]       -7.94  7.9e-4   0.03   -8.0  -7.96  -7.94  -7.92  -7.88   1566    1.0\n",
      "mu[3]        -7.9  5.8e-4   0.03  -7.95  -7.91   -7.9  -7.88  -7.84   2171    1.0\n",
      "mu[4]       -7.86  4.2e-4   0.02   -7.9  -7.87  -7.86  -7.84  -7.81   3333    1.0\n",
      "mu[5]       -7.81  4.2e-4   0.02  -7.86  -7.83  -7.81   -7.8  -7.76   3407    1.0\n",
      "mu[6]       -7.77  5.9e-4   0.03  -7.83  -7.79  -7.77  -7.76  -7.72   2010    1.0\n",
      "mu[7]       -7.73  8.2e-4   0.03  -7.79  -7.75  -7.73  -7.71  -7.67   1438    1.0\n",
      "mu[8]       -7.69  1.1e-3   0.04  -7.76  -7.71  -7.69  -7.67  -7.62   1184    1.0\n",
      "mu[9]       -7.65  1.3e-3   0.04  -7.73  -7.67  -7.65  -7.62  -7.56   1058   1.01\n",
      "ypred        -7.6  2.0e-3   0.09   -7.8  -7.66   -7.6  -7.55  -7.43   2034    1.0\n",
      "log_lik[0]   1.43    0.01   0.44   0.38    1.2   1.51   1.73   2.08   1045    1.0\n",
      "log_lik[1]   0.56    0.02    0.8  -1.38   0.18   0.73   1.12   1.61   1688    1.0\n",
      "log_lik[2]    1.2  9.6e-3   0.43   0.18   0.97   1.27    1.5   1.84   1981    1.0\n",
      "log_lik[3]   1.67    0.01    0.3   0.97   1.48    1.7    1.9   2.18    759    1.0\n",
      "log_lik[4]   1.63    0.01   0.29   0.94   1.44   1.65   1.84   2.12    846    1.0\n",
      "log_lik[5]   1.69    0.01    0.3    1.0    1.5   1.71   1.91   2.19    778    1.0\n",
      "log_lik[6]   1.59 10.0e-3    0.3    0.9    1.4   1.61    1.8   2.11    916    1.0\n",
      "log_lik[7]   1.43    0.01   0.35   0.63   1.22   1.47   1.68   1.98   1177    1.0\n",
      "log_lik[8]   0.57    0.02   0.77  -1.36   0.17   0.73   1.13   1.61   1707    1.0\n",
      "log_lik[9]   1.15    0.02   0.59  -0.34   0.88   1.27   1.55   1.92   1366    1.0\n",
      "lp__        19.45    0.06   1.47  15.53   18.8  19.81  20.51  21.11    603    1.0\n",
      "\n",
      "Samples were drawn using NUTS at Thu Dec  7 13:30:37 2017.\n",
      "For each parameter, n_eff is a crude measure of effective sample size,\n",
      "and Rhat is the potential scale reduction factor on split chains (at \n",
      "convergence, Rhat=1).\n",
      "Inference for Stan model: anon_model_570544391b1e440e4aa07d2bfc218092.\n",
      "4 chains, each with iter=2000; warmup=1000; thin=1; \n",
      "post-warmup draws per chain=1000, total post-warmup draws=4000.\n",
      "\n",
      "             mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\n",
      "alpha      -96.21    2.79  79.72 -253.6 -148.0 -96.81 -43.59  57.35    814    1.0\n",
      "beta         0.04  1.4e-3   0.04  -0.03   0.02   0.04   0.07   0.12    814    1.0\n",
      "sigma        0.37  4.5e-3   0.12   0.23    0.3   0.35   0.42   0.64    749   1.01\n",
      "mu[0]       -8.19  6.1e-3   0.22  -8.63  -8.33  -8.19  -8.05  -7.75   1297    1.0\n",
      "mu[1]       -8.15  4.9e-3   0.19  -8.52  -8.26  -8.15  -8.03  -7.76   1472    1.0\n",
      "mu[2]        -8.1  3.8e-3   0.16  -8.42   -8.2   -8.1   -8.0  -7.78   1781    1.0\n",
      "mu[3]       -8.06  2.8e-3   0.14  -8.33  -8.14  -8.06  -7.97  -7.78   2417    1.0\n",
      "mu[4]       -8.01  2.2e-3   0.13  -8.26  -8.09  -8.01  -7.94  -7.77   3472    1.0\n",
      "mu[5]       -7.97  2.1e-3   0.13  -8.22  -8.05  -7.97  -7.89  -7.73   3750    1.0\n",
      "mu[6]       -7.93  2.8e-3   0.14   -8.2  -8.01  -7.92  -7.84  -7.66   2426    1.0\n",
      "mu[7]       -7.88  3.8e-3   0.16   -8.2  -7.98  -7.88  -7.78  -7.58   1675    1.0\n",
      "mu[8]       -7.84  5.0e-3   0.18  -8.21  -7.95  -7.84  -7.72  -7.48   1350    1.0\n",
      "mu[9]       -7.79  6.2e-3   0.22  -8.22  -7.93  -7.79  -7.66  -7.37   1190    1.0\n",
      "ypred       -7.75  9.3e-3   0.45  -8.65  -8.03  -7.76  -7.47  -6.85   2409    1.0\n",
      "log_lik[0]  -0.24    0.02   0.45  -1.38  -0.45  -0.16   0.07   0.41    893    1.0\n",
      "log_lik[1]   -0.3    0.01   0.44  -1.35  -0.52  -0.23 9.5e-3   0.33   1420    1.0\n",
      "log_lik[2]   0.02    0.01    0.3  -0.62  -0.15   0.05   0.23   0.51    852   1.01\n",
      "log_lik[3]  -0.26  8.3e-3   0.33  -1.05  -0.45  -0.22  -0.03   0.26   1587    1.0\n",
      "log_lik[4]   0.05  9.5e-3   0.28  -0.55  -0.11   0.08   0.25   0.52    903   1.01\n",
      "log_lik[5]  -1.44    0.02   0.75  -3.33  -1.82  -1.29   -0.9  -0.41   1960    1.0\n",
      "log_lik[6]   0.02  9.2e-3   0.29  -0.58  -0.15   0.04   0.22    0.5    969    1.0\n",
      "log_lik[7]  -0.77    0.01   0.56  -2.14  -1.06  -0.66  -0.37   0.03   1857    1.0\n",
      "log_lik[8]  -0.19    0.01   0.39  -1.14  -0.39  -0.14   0.08   0.39   1181    1.0\n",
      "log_lik[9]  -0.22    0.01   0.44  -1.35  -0.43  -0.15   0.08   0.42   1052    1.0\n",
      "lp__         4.83    0.05   1.42   1.23   4.25   5.17   5.82   6.42    702   1.01\n",
      "\n",
      "Samples were drawn using NUTS at Thu Dec  7 13:31:02 2017.\n",
      "For each parameter, n_eff is a crude measure of effective sample size,\n",
      "and Rhat is the potential scale reduction factor on split chains (at \n",
      "convergence, Rhat=1).\n",
      "Inference for Stan model: anon_model_570544391b1e440e4aa07d2bfc218092.\n",
      "4 chains, each with iter=2000; warmup=1000; thin=1; \n",
      "post-warmup draws per chain=1000, total post-warmup draws=4000.\n",
      "\n",
      "             mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\n",
      "alpha       59.86    1.15   27.4   7.25   43.3  59.68  75.25  116.5    567   1.01\n",
      "beta        -0.03  5.7e-4   0.01  -0.06  -0.04  -0.03  -0.02-6.9e-3    567   1.01\n",
      "sigma        0.11  1.7e-3   0.04   0.07   0.09    0.1   0.13   0.22    512   1.01\n",
      "mu[0]       -6.46  2.6e-3   0.07   -6.6   -6.5  -6.46  -6.42  -6.31    782   1.01\n",
      "mu[1]       -6.49  2.0e-3   0.06  -6.61  -6.53  -6.49  -6.46  -6.37    905    1.0\n",
      "mu[2]       -6.53  1.5e-3   0.05  -6.63  -6.56  -6.53   -6.5  -6.43   1172    1.0\n",
      "mu[3]       -6.56  9.4e-4   0.04  -6.64  -6.58  -6.56  -6.53  -6.47   2062    1.0\n",
      "mu[4]       -6.59  6.4e-4   0.04  -6.67  -6.61  -6.59  -6.57  -6.52   3668    1.0\n",
      "mu[5]       -6.63  6.4e-4   0.04   -6.7  -6.65  -6.63   -6.6  -6.55   3662    1.0\n",
      "mu[6]       -6.66  1.1e-3   0.04  -6.74  -6.68  -6.66  -6.63  -6.57   1472    1.0\n",
      "mu[7]       -6.69  1.7e-3   0.05   -6.8  -6.72  -6.69  -6.66  -6.59    963    1.0\n",
      "mu[8]       -6.72  2.2e-3   0.06  -6.85  -6.76  -6.72  -6.69  -6.61    780   1.01\n",
      "mu[9]       -6.76  2.8e-3   0.07   -6.9   -6.8  -6.76  -6.72  -6.62    695   1.01\n",
      "ypred       -6.79  4.0e-3   0.14  -7.08  -6.87  -6.79  -6.71   -6.5   1344    1.0\n",
      "log_lik[0]   1.09    0.02   0.42   0.07   0.87   1.16   1.37    1.7    427   1.01\n",
      "log_lik[1]   1.18    0.01   0.35   0.36   0.98   1.22   1.43   1.73    563   1.01\n",
      "log_lik[2]    0.4    0.01   0.58  -0.97    0.1   0.51   0.81   1.23   2140    1.0\n",
      "log_lik[3]   0.96  8.3e-3   0.33    0.2   0.76   0.99   1.19    1.5   1577    1.0\n",
      "log_lik[4]   0.64  6.6e-3   0.38  -0.27   0.42    0.7   0.91   1.24   3372    1.0\n",
      "log_lik[5]   0.96  8.7e-3    0.3   0.28   0.78   0.99   1.17   1.46   1203    1.0\n",
      "log_lik[6]   0.99    0.01   0.32   0.25    0.8   1.03   1.22   1.51   1001    1.0\n",
      "log_lik[7]   1.19    0.01   0.33   0.42   0.99   1.22   1.42   1.72    624   1.01\n",
      "log_lik[8]   0.94    0.01   0.43   -0.1   0.72   1.02   1.24   1.58    928    1.0\n",
      "log_lik[9]   0.27    0.02   0.84  -1.94  -0.12   0.46   0.88   1.35   1282    1.0\n",
      "lp__        15.57    0.07   1.56  11.48  14.87  15.94  16.69  17.29    507   1.01\n",
      "\n",
      "Samples were drawn using NUTS at Thu Dec  7 13:31:29 2017.\n",
      "For each parameter, n_eff is a crude measure of effective sample size,\n",
      "and Rhat is the potential scale reduction factor on split chains (at \n",
      "convergence, Rhat=1).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference for Stan model: anon_model_570544391b1e440e4aa07d2bfc218092.\n",
      "4 chains, each with iter=2000; warmup=1000; thin=1; \n",
      "post-warmup draws per chain=1000, total post-warmup draws=4000.\n",
      "\n",
      "             mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\n",
      "alpha      252.91    6.12 190.16 -141.9  138.9 258.22 368.68 627.66    965    1.0\n",
      "beta        -0.13  3.1e-3   0.09  -0.32  -0.19  -0.13  -0.07   0.07    965    1.0\n",
      "sigma        0.85  8.7e-3   0.26    0.5   0.66    0.8   0.97   1.46    895    1.0\n",
      "mu[0]       -7.59    0.01   0.52  -8.62  -7.91  -7.59  -7.28  -6.54   1307    1.0\n",
      "mu[1]       -7.72    0.01   0.44  -8.61   -8.0  -7.72  -7.45   -6.8   1474    1.0\n",
      "mu[2]       -7.85  8.8e-3   0.37  -8.61  -8.09  -7.85  -7.63  -7.06   1797    1.0\n",
      "mu[3]       -7.98  6.5e-3   0.32  -8.65  -8.18  -7.98  -7.79   -7.3   2463    1.0\n",
      "mu[4]       -8.11  5.0e-3   0.29  -8.72  -8.29  -8.11  -7.93   -7.5   3499    1.0\n",
      "mu[5]       -8.24  5.0e-3   0.29  -8.85  -8.42  -8.24  -8.07  -7.62   3430    1.0\n",
      "mu[6]       -8.37  6.6e-3   0.32  -9.04  -8.57  -8.37  -8.18  -7.71   2381    1.0\n",
      "mu[7]        -8.5  9.0e-3   0.37  -9.27  -8.73  -8.51  -8.28  -7.76   1717    1.0\n",
      "mu[8]       -8.63    0.01   0.44   -9.5   -8.9  -8.64  -8.37  -7.73   1393    1.0\n",
      "mu[9]       -8.76    0.01   0.51  -9.78  -9.07  -8.78  -8.45  -7.68   1235    1.0\n",
      "ypred        -8.9    0.02   1.05 -11.02  -9.54   -8.9  -8.26  -6.78   2224    1.0\n",
      "log_lik[0]  -0.91    0.01   0.38  -1.82  -1.12  -0.85  -0.65  -0.32    963    1.0\n",
      "log_lik[1]  -0.84    0.01   0.33  -1.56  -1.03   -0.8  -0.61  -0.29    896    1.0\n",
      "log_lik[2]  -1.07  9.3e-3   0.37  -1.93  -1.28  -1.02  -0.83  -0.49   1546    1.0\n",
      "log_lik[3]  -0.91  8.6e-3    0.3  -1.57   -1.1  -0.88   -0.7  -0.41   1191    1.0\n",
      "log_lik[4]  -1.65  9.4e-3   0.49  -2.86   -1.9  -1.57  -1.31  -0.95   2661    1.0\n",
      "log_lik[5]  -0.88  8.3e-3   0.28  -1.51  -1.06  -0.85  -0.69   -0.4   1131    1.0\n",
      "log_lik[6]  -1.33  7.9e-3    0.4  -2.28  -1.54  -1.27  -1.05  -0.71   2537    1.0\n",
      "log_lik[7]  -0.86 10.0e-3   0.31  -1.56  -1.05  -0.83  -0.65  -0.34    973    1.0\n",
      "log_lik[8]   -1.3    0.01   0.51  -2.59  -1.54   -1.2  -0.96  -0.57   1774    1.0\n",
      "log_lik[9]   -1.9    0.02    0.9  -4.22  -2.32   -1.7  -1.26  -0.77   1760    1.0\n",
      "lp__        -2.67    0.05   1.41  -6.41  -3.34  -2.31  -1.64  -1.02    835    1.0\n",
      "\n",
      "Samples were drawn using NUTS at Thu Dec  7 13:31:53 2017.\n",
      "For each parameter, n_eff is a crude measure of effective sample size,\n",
      "and Rhat is the potential scale reduction factor on split chains (at \n",
      "convergence, Rhat=1).\n"
     ]
    }
   ],
   "source": [
    "for convergence in samples:\n",
    "    print(convergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0 of 4000 iterations ended with a divergence (0.175%)\n",
      "Try running with larger adapt_delta to remove the divergences\n",
      "2.0 of 4000 iterations ended with a divergence (0.05%)\n",
      "Try running with larger adapt_delta to remove the divergences\n",
      "87.0 of 4000 iterations ended with a divergence (2.175%)\n",
      "Try running with larger adapt_delta to remove the divergences\n",
      "54.0 of 4000 iterations ended with a divergence (1.35%)\n",
      "Try running with larger adapt_delta to remove the divergences\n",
      "6.0 of 4000 iterations ended with a divergence (0.15%)\n",
      "Try running with larger adapt_delta to remove the divergences\n",
      "7.0 of 4000 iterations ended with a divergence (0.175%)\n",
      "Try running with larger adapt_delta to remove the divergences\n",
      "0.0 of 4000 iterations ended with a divergence (0.0%)\n",
      "25.0 of 4000 iterations ended with a divergence (0.625%)\n",
      "Try running with larger adapt_delta to remove the divergences\n",
      "0.0 of 4000 iterations ended with a divergence (0.0%)\n",
      "216.0 of 4000 iterations ended with a divergence (5.4%)\n",
      "Try running with larger adapt_delta to remove the divergences\n",
      "3.0 of 4000 iterations ended with a divergence (0.075%)\n",
      "Try running with larger adapt_delta to remove the divergences\n",
      "0.0 of 4000 iterations ended with a divergence (0.0%)\n",
      "0.0 of 4000 iterations ended with a divergence (0.0%)\n",
      "0.0 of 4000 iterations ended with a divergence (0.0%)\n",
      "18.0 of 4000 iterations ended with a divergence (0.45%)\n",
      "Try running with larger adapt_delta to remove the divergences\n",
      "0.0 of 4000 iterations ended with a divergence (0.0%)\n",
      "2.0 of 4000 iterations ended with a divergence (0.05%)\n",
      "Try running with larger adapt_delta to remove the divergences\n",
      "0.0 of 4000 iterations ended with a divergence (0.0%)\n",
      "19.0 of 4000 iterations ended with a divergence (0.475%)\n",
      "Try running with larger adapt_delta to remove the divergences\n",
      "26.0 of 4000 iterations ended with a divergence (0.65%)\n",
      "Try running with larger adapt_delta to remove the divergences\n",
      "17.0 of 4000 iterations ended with a divergence (0.425%)\n",
      "Try running with larger adapt_delta to remove the divergences\n",
      "14.0 of 4000 iterations ended with a divergence (0.35%)\n",
      "Try running with larger adapt_delta to remove the divergences\n",
      "33.0 of 4000 iterations ended with a divergence (0.825%)\n",
      "Try running with larger adapt_delta to remove the divergences\n",
      "7.0 of 4000 iterations ended with a divergence (0.175%)\n",
      "Try running with larger adapt_delta to remove the divergences\n",
      "16.0 of 4000 iterations ended with a divergence (0.4%)\n",
      "Try running with larger adapt_delta to remove the divergences\n"
     ]
    }
   ],
   "source": [
    "for convergence in samples:\n",
    "    stan_utility.check_div(convergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567 of 4000 iterations saturated the maximum tree depth of 10 (14.175%)\n",
      "Run again with max_depth set to a larger value to avoid saturation\n",
      "536 of 4000 iterations saturated the maximum tree depth of 10 (13.4%)\n",
      "Run again with max_depth set to a larger value to avoid saturation\n",
      "700 of 4000 iterations saturated the maximum tree depth of 10 (17.5%)\n",
      "Run again with max_depth set to a larger value to avoid saturation\n",
      "576 of 4000 iterations saturated the maximum tree depth of 10 (14.4%)\n",
      "Run again with max_depth set to a larger value to avoid saturation\n",
      "550 of 4000 iterations saturated the maximum tree depth of 10 (13.75%)\n",
      "Run again with max_depth set to a larger value to avoid saturation\n",
      "500 of 4000 iterations saturated the maximum tree depth of 10 (12.5%)\n",
      "Run again with max_depth set to a larger value to avoid saturation\n",
      "584 of 4000 iterations saturated the maximum tree depth of 10 (14.6%)\n",
      "Run again with max_depth set to a larger value to avoid saturation\n",
      "615 of 4000 iterations saturated the maximum tree depth of 10 (15.375%)\n",
      "Run again with max_depth set to a larger value to avoid saturation\n",
      "489 of 4000 iterations saturated the maximum tree depth of 10 (12.225%)\n",
      "Run again with max_depth set to a larger value to avoid saturation\n",
      "531 of 4000 iterations saturated the maximum tree depth of 10 (13.275%)\n",
      "Run again with max_depth set to a larger value to avoid saturation\n",
      "563 of 4000 iterations saturated the maximum tree depth of 10 (14.075%)\n",
      "Run again with max_depth set to a larger value to avoid saturation\n",
      "485 of 4000 iterations saturated the maximum tree depth of 10 (12.125%)\n",
      "Run again with max_depth set to a larger value to avoid saturation\n",
      "453 of 4000 iterations saturated the maximum tree depth of 10 (11.325%)\n",
      "Run again with max_depth set to a larger value to avoid saturation\n",
      "520 of 4000 iterations saturated the maximum tree depth of 10 (13.0%)\n",
      "Run again with max_depth set to a larger value to avoid saturation\n",
      "530 of 4000 iterations saturated the maximum tree depth of 10 (13.25%)\n",
      "Run again with max_depth set to a larger value to avoid saturation\n",
      "513 of 4000 iterations saturated the maximum tree depth of 10 (12.825%)\n",
      "Run again with max_depth set to a larger value to avoid saturation\n",
      "516 of 4000 iterations saturated the maximum tree depth of 10 (12.9%)\n",
      "Run again with max_depth set to a larger value to avoid saturation\n",
      "433 of 4000 iterations saturated the maximum tree depth of 10 (10.825%)\n",
      "Run again with max_depth set to a larger value to avoid saturation\n",
      "678 of 4000 iterations saturated the maximum tree depth of 10 (16.95%)\n",
      "Run again with max_depth set to a larger value to avoid saturation\n",
      "763 of 4000 iterations saturated the maximum tree depth of 10 (19.075%)\n",
      "Run again with max_depth set to a larger value to avoid saturation\n",
      "566 of 4000 iterations saturated the maximum tree depth of 10 (14.15%)\n",
      "Run again with max_depth set to a larger value to avoid saturation\n",
      "577 of 4000 iterations saturated the maximum tree depth of 10 (14.425%)\n",
      "Run again with max_depth set to a larger value to avoid saturation\n",
      "626 of 4000 iterations saturated the maximum tree depth of 10 (15.65%)\n",
      "Run again with max_depth set to a larger value to avoid saturation\n",
      "585 of 4000 iterations saturated the maximum tree depth of 10 (14.625%)\n",
      "Run again with max_depth set to a larger value to avoid saturation\n",
      "634 of 4000 iterations saturated the maximum tree depth of 10 (15.85%)\n",
      "Run again with max_depth set to a larger value to avoid saturation\n"
     ]
    }
   ],
   "source": [
    "for convergence in samples:\n",
    "    stan_utility.check_treedepth(convergence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every model separately we have that each value of $\\widehat{R}$ is very close to 1.0 and in any case under 1.1, therefore we can say that each model has converged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Inference for Stan model: anon_model_f33a8b74a230c2fdd82c5bbc279eeecb.\n",
       "4 chains, each with iter=2000; warmup=1000; thin=1; \n",
       "post-warmup draws per chain=1000, total post-warmup draws=4000.\n",
       "\n",
       "               mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\n",
       "alpha[0]      -7.86  2.9e-3   0.09  -8.05  -7.92  -7.86   -7.8  -7.68    996   1.01\n",
       "alpha[1]      -8.42  5.5e-3   0.15  -8.73  -8.52  -8.42  -8.32  -8.12    758   1.01\n",
       "alpha[2]      -6.62  7.6e-3   0.14  -6.93  -6.69  -6.61  -6.53  -6.36    338   1.02\n",
       "alpha[3]      -8.19    0.01   0.31  -8.78   -8.4   -8.2   -8.0  -7.56    631   1.01\n",
       "alpha[4]      -7.49    0.01   0.24  -7.98  -7.63  -7.48  -7.34  -7.02    466   1.01\n",
       "alpha[5]      -7.12    0.02   0.26   -7.7  -7.26   -7.1  -6.95  -6.63    288   1.02\n",
       "alpha[6]      -5.97    0.02   0.35  -6.83  -6.12  -5.93  -5.74  -5.39    241   1.02\n",
       "alpha[7]      -6.56    0.02   0.35  -7.37  -6.75  -6.53  -6.32  -5.94    288   1.03\n",
       "alpha[8]      -7.34    0.02   0.39  -8.16  -7.57  -7.33  -7.09   -6.6    329   1.02\n",
       "alpha[9]      -7.35    0.02   0.46   -8.3  -7.62  -7.33  -7.06  -6.48    381   1.01\n",
       "alpha[10]     -7.86    0.02   0.43  -8.73  -8.13  -7.86  -7.58  -6.98    343   1.02\n",
       "alpha[11]     -9.08    0.03   0.64 -10.31   -9.5   -9.1  -8.67  -7.76    369   1.01\n",
       "alpha[12]     -7.67    0.03   0.49  -8.65  -7.99  -7.67  -7.35   -6.7    315   1.02\n",
       "alpha[13]      -6.0    0.04   0.65  -7.47  -6.37  -5.94  -5.56  -4.89    219   1.03\n",
       "alpha[14]     -7.24    0.03   0.54  -8.35  -7.58  -7.23  -6.88  -6.19    301   1.02\n",
       "alpha[15]     -6.88    0.04   0.59  -8.14  -7.25  -6.86  -6.46   -5.8    254   1.02\n",
       "alpha[16]     -8.11    0.03   0.61  -9.35   -8.5  -8.11  -7.72  -6.88    306   1.01\n",
       "alpha[17]     -7.52    0.04   0.63   -8.8  -7.93  -7.52   -7.1  -6.28    317   1.02\n",
       "alpha[18]     -7.87    0.03   0.65  -9.16  -8.28  -7.85  -7.45  -6.57    343   1.01\n",
       "alpha[19]     -7.92    0.04   0.69  -9.35  -8.35  -7.93  -7.48  -6.51    318   1.02\n",
       "alpha[20]     -7.23    0.04    0.7  -8.72  -7.65  -7.21  -6.77  -5.87    272   1.02\n",
       "alpha[21]     -7.53    0.04   0.71  -8.95  -7.97  -7.54  -7.07  -6.15    309   1.02\n",
       "alpha[22]     -7.63    0.04   0.73   -9.1   -8.1  -7.64  -7.15  -6.22    301   1.02\n",
       "alpha[23]     -6.68    0.05   0.81  -8.37   -7.2  -6.66  -6.16  -5.18    232   1.03\n",
       "alpha[24]     -7.74    0.04   0.77  -9.27  -8.24  -7.75  -7.24  -6.18    348   1.01\n",
       "mu_alpha      -7.44    0.03    0.4  -8.28  -7.69  -7.43  -7.17  -6.67    239   1.02\n",
       "sigma_alpha    0.87  5.4e-3   0.17   0.57   0.76   0.86   0.97   1.26   1042    1.0\n",
       "beta[0]       -0.01  2.3e-3   0.04   -0.1  -0.04  -0.01   0.01   0.08    364   1.01\n",
       "beta[1]       -0.01  2.2e-3   0.04   -0.1  -0.04  -0.01   0.01   0.07    382   1.02\n",
       "beta[2]     -6.2e-3  2.5e-3   0.04  -0.09  -0.03-8.9e-3   0.02    0.1    312   1.02\n",
       "beta[3]       -0.02  2.2e-3   0.04   -0.1  -0.04  -0.01 9.7e-3   0.07    375   1.02\n",
       "beta[4]       -0.01  2.2e-3   0.04  -0.09  -0.04  -0.01   0.01   0.08    377   1.02\n",
       "beta[5]     -7.5e-3  2.5e-3   0.04  -0.09  -0.03-9.3e-3   0.02   0.09    275   1.02\n",
       "beta[6]      4.5e-3  3.2e-3   0.05  -0.08  -0.03-1.5e-3   0.03   0.13    240   1.02\n",
       "beta[7]     -7.3e-4  2.6e-3   0.04  -0.08  -0.03-4.6e-3   0.02    0.1    279   1.03\n",
       "beta[8]     -8.8e-3  2.3e-3   0.04  -0.09  -0.04  -0.01   0.01   0.08    311   1.02\n",
       "beta[9]     -8.7e-3  2.3e-3   0.04  -0.09  -0.04  -0.01   0.01   0.08    310   1.02\n",
       "beta[10]      -0.02  2.1e-3   0.04  -0.09  -0.04  -0.02 8.2e-3   0.06    327   1.02\n",
       "beta[11]      -0.03  2.8e-3   0.05  -0.14  -0.06  -0.03-4.3e-3   0.04    272   1.02\n",
       "beta[12]      -0.01  2.1e-3   0.04  -0.09  -0.04  -0.01   0.01   0.06    313   1.02\n",
       "beta[13]       0.01  3.2e-3   0.05  -0.06  -0.02 5.5e-3   0.04   0.12    206   1.03\n",
       "beta[14]    -8.1e-3  2.1e-3   0.04  -0.08  -0.03-9.0e-3   0.01   0.07    301   1.02\n",
       "beta[15]    -2.0e-3  2.3e-3   0.04  -0.07  -0.03-3.8e-3   0.02   0.08    254   1.02\n",
       "beta[16]      -0.02  2.1e-3   0.04  -0.09  -0.04  -0.02 9.4e-4   0.05    305   1.01\n",
       "beta[17]      -0.01  1.9e-3   0.03  -0.08  -0.04  -0.01 8.9e-3   0.06    311   1.02\n",
       "beta[18]      -0.02  1.8e-3   0.03  -0.09  -0.04  -0.02 2.9e-3   0.05    341   1.01\n",
       "beta[19]      -0.02  1.9e-3   0.03  -0.09  -0.04  -0.02 1.0e-3   0.05    314   1.02\n",
       "beta[20]    -7.6e-3  2.0e-3   0.03  -0.07  -0.03-7.7e-3   0.01   0.06    268   1.02\n",
       "beta[21]      -0.01  1.8e-3   0.03  -0.08  -0.03  -0.01 5.9e-3   0.05    311   1.02\n",
       "beta[22]      -0.02  1.8e-3   0.03  -0.08  -0.04  -0.01 4.2e-3   0.05    299   1.02\n",
       "beta[23]     2.9e-3  2.2e-3   0.03  -0.06  -0.02 1.9e-3   0.02   0.07    233   1.03\n",
       "beta[24]      -0.02  1.7e-3   0.03  -0.08  -0.04  -0.02 2.7e-3   0.05    323   1.02\n",
       "mu_beta       -0.01  2.0e-3   0.03  -0.07  -0.03  -0.01 7.4e-3   0.05    204   1.03\n",
       "sigma_beta     0.03  1.9e-3   0.02 2.1e-3   0.01   0.02   0.04   0.07     98   1.06\n",
       "sigma[0]       0.24  9.5e-4   0.06   0.16    0.2   0.23   0.27   0.39   4000    1.0\n",
       "sigma[1]       0.38  2.3e-3   0.09   0.24   0.31   0.36   0.43    0.6   1581    1.0\n",
       "sigma[2]       0.11  7.3e-4   0.03   0.07   0.09   0.11   0.13   0.19   1746    1.0\n",
       "sigma[3]       0.83  3.6e-3    0.2   0.55   0.69    0.8   0.92    1.3   3079    1.0\n",
       "sigma[4]       0.35  1.8e-3   0.08   0.24   0.29   0.34    0.4   0.55   2143    1.0\n",
       "sigma[5]       0.16  7.1e-4   0.04    0.1   0.13   0.15   0.18   0.26   3155    1.0\n",
       "sigma[6]       0.09  6.6e-4   0.03   0.06   0.07   0.09   0.11   0.16   1744    1.0\n",
       "sigma[7]       0.13  6.2e-4   0.03   0.08    0.1   0.12   0.14   0.21   2807    1.0\n",
       "sigma[8]       0.44  1.9e-3    0.1   0.29   0.37   0.42   0.49   0.68   2898    1.0\n",
       "sigma[9]       0.73  4.2e-3   0.18   0.49   0.61    0.7   0.82   1.16   1763    1.0\n",
       "sigma[10]      0.35  1.6e-3   0.09   0.23   0.29   0.34    0.4   0.56   2924    1.0\n",
       "sigma[11]      1.14  7.0e-3    0.3    0.7   0.92   1.08   1.29   1.87   1810    1.0\n",
       "sigma[12]      0.17  1.0e-3   0.04   0.11   0.14   0.16   0.19   0.28   1824    1.0\n",
       "sigma[13]      0.56  3.7e-3   0.14   0.37   0.46   0.54   0.63   0.89   1445    1.0\n",
       "sigma[14]      0.08  5.1e-4   0.02   0.05   0.06   0.08   0.09   0.14   2312    1.0\n",
       "sigma[15]       0.2  8.9e-4   0.05   0.13   0.17   0.19   0.23   0.32   3244    1.0\n",
       "sigma[16]      0.15  6.1e-4   0.04   0.09   0.12   0.14   0.16   0.24   4000    1.0\n",
       "sigma[17]      0.51  1.9e-3   0.12   0.33   0.43   0.49   0.57   0.82   4000    1.0\n",
       "sigma[18]      0.28  1.1e-3   0.07   0.19   0.24   0.27   0.32   0.45   4000    1.0\n",
       "sigma[19]      0.52  2.0e-3   0.12   0.34   0.43    0.5   0.58   0.83   4000    1.0\n",
       "sigma[20]      0.23  1.1e-3   0.06   0.15   0.19   0.22   0.26   0.38   2590    1.0\n",
       "sigma[21]      0.16  1.0e-3   0.04    0.1   0.13   0.15   0.18   0.26   1627    1.0\n",
       "sigma[22]      0.34  1.3e-3   0.08   0.22   0.28   0.32   0.38   0.52   4000    1.0\n",
       "sigma[23]      0.15  9.0e-4   0.04    0.1   0.12   0.15   0.17   0.25   1966    1.0\n",
       "sigma[24]      0.77  3.4e-3   0.18   0.51   0.64   0.74   0.86   1.22   2777    1.0\n",
       "nu_0           0.46  6.7e-4   0.04   0.39   0.44   0.46   0.48   0.53   2769    1.0\n",
       "sigma_0        4.25    0.03   1.24   2.27   3.35   4.09   4.98   7.09   2426    1.0\n",
       "mu[0]         -7.87  1.2e-3   0.08  -8.03  -7.92  -7.87  -7.83  -7.72   4000    1.0\n",
       "mu[1]         -7.87  1.2e-3   0.08  -8.03  -7.92  -7.87  -7.83  -7.72   4000    1.0\n",
       "mu[2]         -7.87  1.2e-3   0.08  -8.03  -7.92  -7.87  -7.83  -7.72   4000    1.0\n",
       "mu[3]         -7.87  1.2e-3   0.08  -8.03  -7.92  -7.87  -7.83  -7.72   4000    1.0\n",
       "mu[4]         -7.87  1.2e-3   0.08  -8.03  -7.92  -7.87  -7.83  -7.72   4000    1.0\n",
       "mu[5]         -7.87  1.2e-3   0.08  -8.03  -7.92  -7.87  -7.83  -7.72   4000    1.0\n",
       "mu[6]         -7.87  1.2e-3   0.08  -8.03  -7.92  -7.87  -7.83  -7.72   4000    1.0\n",
       "mu[7]         -7.87  1.2e-3   0.08  -8.03  -7.92  -7.87  -7.83  -7.72   4000    1.0\n",
       "mu[8]         -7.87  1.2e-3   0.08  -8.03  -7.92  -7.87  -7.83  -7.72   4000    1.0\n",
       "mu[9]         -7.87  1.2e-3   0.08  -8.03  -7.92  -7.87  -7.83  -7.72   4000    1.0\n",
       "mu[10]        -8.45  1.9e-3   0.12  -8.68  -8.53  -8.45  -8.37  -8.21   4000    1.0\n",
       "mu[11]        -8.45  1.9e-3   0.12  -8.68  -8.53  -8.45  -8.37  -8.21   4000    1.0\n",
       "mu[12]        -8.45  1.9e-3   0.12  -8.68  -8.53  -8.45  -8.37  -8.21   4000    1.0\n",
       "mu[13]        -8.45  1.9e-3   0.12  -8.68  -8.53  -8.45  -8.37  -8.21   4000    1.0\n",
       "mu[14]        -8.45  1.9e-3   0.12  -8.68  -8.53  -8.45  -8.37  -8.21   4000    1.0\n",
       "mu[15]        -8.45  1.9e-3   0.12  -8.68  -8.53  -8.45  -8.37  -8.21   4000    1.0\n",
       "mu[16]        -8.45  1.9e-3   0.12  -8.68  -8.53  -8.45  -8.37  -8.21   4000    1.0\n",
       "mu[17]        -8.45  1.9e-3   0.12  -8.68  -8.53  -8.45  -8.37  -8.21   4000    1.0\n",
       "mu[18]        -8.45  1.9e-3   0.12  -8.68  -8.53  -8.45  -8.37  -8.21   4000    1.0\n",
       "mu[19]        -8.45  1.9e-3   0.12  -8.68  -8.53  -8.45  -8.37  -8.21   4000    1.0\n",
       "mu[20]        -6.64  1.1e-3   0.04  -6.72  -6.66  -6.64  -6.61  -6.56   1364    1.0\n",
       "mu[21]        -6.64  1.1e-3   0.04  -6.72  -6.66  -6.64  -6.61  -6.56   1364    1.0\n",
       "mu[22]        -6.64  1.1e-3   0.04  -6.72  -6.66  -6.64  -6.61  -6.56   1364    1.0\n",
       "mu[23]        -6.64  1.1e-3   0.04  -6.72  -6.66  -6.64  -6.61  -6.56   1364    1.0\n",
       "mu[24]        -6.64  1.1e-3   0.04  -6.72  -6.66  -6.64  -6.61  -6.56   1364    1.0\n",
       "mu[25]        -6.64  1.1e-3   0.04  -6.72  -6.66  -6.64  -6.61  -6.56   1364    1.0\n",
       "mu[26]        -6.64  1.1e-3   0.04  -6.72  -6.66  -6.64  -6.61  -6.56   1364    1.0\n",
       "mu[27]        -6.64  1.1e-3   0.04  -6.72  -6.66  -6.64  -6.61  -6.56   1364    1.0\n",
       "mu[28]        -6.64  1.1e-3   0.04  -6.72  -6.66  -6.64  -6.61  -6.56   1364    1.0\n",
       "mu[29]        -6.64  1.1e-3   0.04  -6.72  -6.66  -6.64  -6.61  -6.56   1364    1.0\n",
       "mu[30]        -8.26  4.1e-3   0.26  -8.77  -8.42  -8.26   -8.1  -7.72   4000    1.0\n",
       "mu[31]        -8.26  4.1e-3   0.26  -8.77  -8.42  -8.26   -8.1  -7.72   4000    1.0\n",
       "mu[32]        -8.26  4.1e-3   0.26  -8.77  -8.42  -8.26   -8.1  -7.72   4000    1.0\n",
       "mu[33]        -8.26  4.1e-3   0.26  -8.77  -8.42  -8.26   -8.1  -7.72   4000    1.0\n",
       "mu[34]        -8.26  4.1e-3   0.26  -8.77  -8.42  -8.26   -8.1  -7.72   4000    1.0\n",
       "mu[35]        -8.26  4.1e-3   0.26  -8.77  -8.42  -8.26   -8.1  -7.72   4000    1.0\n",
       "mu[36]        -8.26  4.1e-3   0.26  -8.77  -8.42  -8.26   -8.1  -7.72   4000    1.0\n",
       "mu[37]        -8.26  4.1e-3   0.26  -8.77  -8.42  -8.26   -8.1  -7.72   4000    1.0\n",
       "mu[38]        -8.26  4.1e-3   0.26  -8.77  -8.42  -8.26   -8.1  -7.72   4000    1.0\n",
       "mu[39]        -8.26  4.1e-3   0.26  -8.77  -8.42  -8.26   -8.1  -7.72   4000    1.0\n",
       "mu[40]        -7.54  1.8e-3   0.11  -7.77  -7.61  -7.54  -7.47  -7.31   4000    1.0\n",
       "mu[41]        -7.54  1.8e-3   0.11  -7.77  -7.61  -7.54  -7.47  -7.31   4000    1.0\n",
       "mu[42]        -7.54  1.8e-3   0.11  -7.77  -7.61  -7.54  -7.47  -7.31   4000    1.0\n",
       "mu[43]        -7.54  1.8e-3   0.11  -7.77  -7.61  -7.54  -7.47  -7.31   4000    1.0\n",
       "mu[44]        -7.54  1.8e-3   0.11  -7.77  -7.61  -7.54  -7.47  -7.31   4000    1.0\n",
       "mu[45]        -7.54  1.8e-3   0.11  -7.77  -7.61  -7.54  -7.47  -7.31   4000    1.0\n",
       "mu[46]        -7.54  1.8e-3   0.11  -7.77  -7.61  -7.54  -7.47  -7.31   4000    1.0\n",
       "mu[47]        -7.54  1.8e-3   0.11  -7.77  -7.61  -7.54  -7.47  -7.31   4000    1.0\n",
       "mu[48]        -7.54  1.8e-3   0.11  -7.77  -7.61  -7.54  -7.47  -7.31   4000    1.0\n",
       "mu[49]        -7.54  1.8e-3   0.11  -7.77  -7.61  -7.54  -7.47  -7.31   4000    1.0\n",
       "mu[50]        -7.16  8.3e-4   0.05  -7.27   -7.2  -7.16  -7.13  -7.05   4000    1.0\n",
       "mu[51]        -7.16  8.3e-4   0.05  -7.27   -7.2  -7.16  -7.13  -7.05   4000    1.0\n",
       "mu[52]        -7.16  8.3e-4   0.05  -7.27   -7.2  -7.16  -7.13  -7.05   4000    1.0\n",
       "mu[53]        -7.16  8.3e-4   0.05  -7.27   -7.2  -7.16  -7.13  -7.05   4000    1.0\n",
       "mu[54]        -7.16  8.3e-4   0.05  -7.27   -7.2  -7.16  -7.13  -7.05   4000    1.0\n",
       "mu[55]        -7.16  8.3e-4   0.05  -7.27   -7.2  -7.16  -7.13  -7.05   4000    1.0\n",
       "mu[56]        -7.16  8.3e-4   0.05  -7.27   -7.2  -7.16  -7.13  -7.05   4000    1.0\n",
       "mu[57]        -7.16  8.3e-4   0.05  -7.27   -7.2  -7.16  -7.13  -7.05   4000    1.0\n",
       "mu[58]        -7.16  8.3e-4   0.05  -7.27   -7.2  -7.16  -7.13  -7.05   4000    1.0\n",
       "mu[59]        -7.16  8.3e-4   0.05  -7.27   -7.2  -7.16  -7.13  -7.05   4000    1.0\n",
       "mu[60]        -5.94  4.9e-4   0.03   -6.0  -5.96  -5.94  -5.92  -5.87   4000    1.0\n",
       "mu[61]        -5.94  4.9e-4   0.03   -6.0  -5.96  -5.94  -5.92  -5.87   4000    1.0\n",
       "mu[62]        -5.94  4.9e-4   0.03   -6.0  -5.96  -5.94  -5.92  -5.87   4000    1.0\n",
       "mu[63]        -5.94  4.9e-4   0.03   -6.0  -5.96  -5.94  -5.92  -5.87   4000    1.0\n",
       "mu[64]        -5.94  4.9e-4   0.03   -6.0  -5.96  -5.94  -5.92  -5.87   4000    1.0\n",
       "mu[65]        -5.94  4.9e-4   0.03   -6.0  -5.96  -5.94  -5.92  -5.87   4000    1.0\n",
       "mu[66]        -5.94  4.9e-4   0.03   -6.0  -5.96  -5.94  -5.92  -5.87   4000    1.0\n",
       "mu[67]        -5.94  4.9e-4   0.03   -6.0  -5.96  -5.94  -5.92  -5.87   4000    1.0\n",
       "mu[68]        -5.94  4.9e-4   0.03   -6.0  -5.96  -5.94  -5.92  -5.87   4000    1.0\n",
       "mu[69]        -5.94  4.9e-4   0.03   -6.0  -5.96  -5.94  -5.92  -5.87   4000    1.0\n",
       "mu[70]        -6.56  7.2e-4   0.04  -6.65  -6.59  -6.56  -6.54  -6.48   3394    1.0\n",
       "mu[71]        -6.56  7.2e-4   0.04  -6.65  -6.59  -6.56  -6.54  -6.48   3394    1.0\n",
       "mu[72]        -6.56  7.2e-4   0.04  -6.65  -6.59  -6.56  -6.54  -6.48   3394    1.0\n",
       "mu[73]        -6.56  7.2e-4   0.04  -6.65  -6.59  -6.56  -6.54  -6.48   3394    1.0\n",
       "mu[74]        -6.56  7.2e-4   0.04  -6.65  -6.59  -6.56  -6.54  -6.48   3394    1.0\n",
       "mu[75]        -6.56  7.2e-4   0.04  -6.65  -6.59  -6.56  -6.54  -6.48   3394    1.0\n",
       "mu[76]        -6.56  7.2e-4   0.04  -6.65  -6.59  -6.56  -6.54  -6.48   3394    1.0\n",
       "mu[77]        -6.56  7.2e-4   0.04  -6.65  -6.59  -6.56  -6.54  -6.48   3394    1.0\n",
       "mu[78]        -6.56  7.2e-4   0.04  -6.65  -6.59  -6.56  -6.54  -6.48   3394    1.0\n",
       "mu[79]        -6.56  7.2e-4   0.04  -6.65  -6.59  -6.56  -6.54  -6.48   3394    1.0\n",
       "mu[80]        -7.42  2.2e-3   0.14   -7.7  -7.51  -7.42  -7.33  -7.16   4000    1.0\n",
       "mu[81]        -7.42  2.2e-3   0.14   -7.7  -7.51  -7.42  -7.33  -7.16   4000    1.0\n",
       "mu[82]        -7.42  2.2e-3   0.14   -7.7  -7.51  -7.42  -7.33  -7.16   4000    1.0\n",
       "mu[83]        -7.42  2.2e-3   0.14   -7.7  -7.51  -7.42  -7.33  -7.16   4000    1.0\n",
       "mu[84]        -7.42  2.2e-3   0.14   -7.7  -7.51  -7.42  -7.33  -7.16   4000    1.0\n",
       "mu[85]        -7.42  2.2e-3   0.14   -7.7  -7.51  -7.42  -7.33  -7.16   4000    1.0\n",
       "mu[86]        -7.42  2.2e-3   0.14   -7.7  -7.51  -7.42  -7.33  -7.16   4000    1.0\n",
       "mu[87]        -7.42  2.2e-3   0.14   -7.7  -7.51  -7.42  -7.33  -7.16   4000    1.0\n",
       "mu[88]        -7.42  2.2e-3   0.14   -7.7  -7.51  -7.42  -7.33  -7.16   4000    1.0\n",
       "mu[89]        -7.42  2.2e-3   0.14   -7.7  -7.51  -7.42  -7.33  -7.16   4000    1.0\n",
       "mu[90]        -7.44  3.6e-3   0.23  -7.89  -7.58  -7.43  -7.28   -7.0   4000    1.0\n",
       "mu[91]        -7.44  3.6e-3   0.23  -7.89  -7.58  -7.43  -7.28   -7.0   4000    1.0\n",
       "mu[92]        -7.44  3.6e-3   0.23  -7.89  -7.58  -7.43  -7.28   -7.0   4000    1.0\n",
       "mu[93]        -7.44  3.6e-3   0.23  -7.89  -7.58  -7.43  -7.28   -7.0   4000    1.0\n",
       "mu[94]        -7.44  3.6e-3   0.23  -7.89  -7.58  -7.43  -7.28   -7.0   4000    1.0\n",
       "mu[95]        -7.44  3.6e-3   0.23  -7.89  -7.58  -7.43  -7.28   -7.0   4000    1.0\n",
       "mu[96]        -7.44  3.6e-3   0.23  -7.89  -7.58  -7.43  -7.28   -7.0   4000    1.0\n",
       "mu[97]        -7.44  3.6e-3   0.23  -7.89  -7.58  -7.43  -7.28   -7.0   4000    1.0\n",
       "mu[98]        -7.44  3.6e-3   0.23  -7.89  -7.58  -7.43  -7.28   -7.0   4000    1.0\n",
       "mu[99]        -7.44  3.6e-3   0.23  -7.89  -7.58  -7.43  -7.28   -7.0   4000    1.0\n",
       "mu[100]       -8.03  1.8e-3   0.11  -8.26  -8.11  -8.03  -7.96   -7.8   4000    1.0\n",
       "mu[101]       -8.03  1.8e-3   0.11  -8.26  -8.11  -8.03  -7.96   -7.8   4000    1.0\n",
       "mu[102]       -8.03  1.8e-3   0.11  -8.26  -8.11  -8.03  -7.96   -7.8   4000    1.0\n",
       "mu[103]       -8.03  1.8e-3   0.11  -8.26  -8.11  -8.03  -7.96   -7.8   4000    1.0\n",
       "mu[104]       -8.03  1.8e-3   0.11  -8.26  -8.11  -8.03  -7.96   -7.8   4000    1.0\n",
       "mu[105]       -8.03  1.8e-3   0.11  -8.26  -8.11  -8.03  -7.96   -7.8   4000    1.0\n",
       "mu[106]       -8.03  1.8e-3   0.11  -8.26  -8.11  -8.03  -7.96   -7.8   4000    1.0\n",
       "mu[107]       -8.03  1.8e-3   0.11  -8.26  -8.11  -8.03  -7.96   -7.8   4000    1.0\n",
       "mu[108]       -8.03  1.8e-3   0.11  -8.26  -8.11  -8.03  -7.96   -7.8   4000    1.0\n",
       "mu[109]       -8.03  1.8e-3   0.11  -8.26  -8.11  -8.03  -7.96   -7.8   4000    1.0\n",
       "mu[110]        -9.5  6.1e-3   0.39 -10.19  -9.75  -9.53  -9.27  -8.65   4000    1.0\n",
       "mu[111]        -9.5  6.1e-3   0.39 -10.19  -9.75  -9.53  -9.27  -8.65   4000    1.0\n",
       "mu[112]        -9.5  6.1e-3   0.39 -10.19  -9.75  -9.53  -9.27  -8.65   4000    1.0\n",
       "mu[113]        -9.5  6.1e-3   0.39 -10.19  -9.75  -9.53  -9.27  -8.65   4000    1.0\n",
       "mu[114]        -9.5  6.1e-3   0.39 -10.19  -9.75  -9.53  -9.27  -8.65   4000    1.0\n",
       "mu[115]        -9.5  6.1e-3   0.39 -10.19  -9.75  -9.53  -9.27  -8.65   4000    1.0\n",
       "mu[116]        -9.5  6.1e-3   0.39 -10.19  -9.75  -9.53  -9.27  -8.65   4000    1.0\n",
       "mu[117]        -9.5  6.1e-3   0.39 -10.19  -9.75  -9.53  -9.27  -8.65   4000    1.0\n",
       "mu[118]        -9.5  6.1e-3   0.39 -10.19  -9.75  -9.53  -9.27  -8.65   4000    1.0\n",
       "mu[119]        -9.5  6.1e-3   0.39 -10.19  -9.75  -9.53  -9.27  -8.65   4000    1.0\n",
       "mu[120]       -7.85  9.0e-4   0.06  -7.95  -7.88  -7.85  -7.81  -7.74   3923    1.0\n",
       "mu[121]       -7.85  9.0e-4   0.06  -7.95  -7.88  -7.85  -7.81  -7.74   3923    1.0\n",
       "mu[122]       -7.85  9.0e-4   0.06  -7.95  -7.88  -7.85  -7.81  -7.74   3923    1.0\n",
       "mu[123]       -7.85  9.0e-4   0.06  -7.95  -7.88  -7.85  -7.81  -7.74   3923    1.0\n",
       "mu[124]       -7.85  9.0e-4   0.06  -7.95  -7.88  -7.85  -7.81  -7.74   3923    1.0\n",
       "mu[125]       -7.85  9.0e-4   0.06  -7.95  -7.88  -7.85  -7.81  -7.74   3923    1.0\n",
       "mu[126]       -7.85  9.0e-4   0.06  -7.95  -7.88  -7.85  -7.81  -7.74   3923    1.0\n",
       "mu[127]       -7.85  9.0e-4   0.06  -7.95  -7.88  -7.85  -7.81  -7.74   3923    1.0\n",
       "mu[128]       -7.85  9.0e-4   0.06  -7.95  -7.88  -7.85  -7.81  -7.74   3923    1.0\n",
       "mu[129]       -7.85  9.0e-4   0.06  -7.95  -7.88  -7.85  -7.81  -7.74   3923    1.0\n",
       "mu[130]       -5.84  2.9e-3   0.18  -6.23  -5.95  -5.84  -5.73   -5.5   4000    1.0\n",
       "mu[131]       -5.84  2.9e-3   0.18  -6.23  -5.95  -5.84  -5.73   -5.5   4000    1.0\n",
       "mu[132]       -5.84  2.9e-3   0.18  -6.23  -5.95  -5.84  -5.73   -5.5   4000    1.0\n",
       "mu[133]       -5.84  2.9e-3   0.18  -6.23  -5.95  -5.84  -5.73   -5.5   4000    1.0\n",
       "mu[134]       -5.84  2.9e-3   0.18  -6.23  -5.95  -5.84  -5.73   -5.5   4000    1.0\n",
       "mu[135]       -5.84  2.9e-3   0.18  -6.23  -5.95  -5.84  -5.73   -5.5   4000    1.0\n",
       "mu[136]       -5.84  2.9e-3   0.18  -6.23  -5.95  -5.84  -5.73   -5.5   4000    1.0\n",
       "mu[137]       -5.84  2.9e-3   0.18  -6.23  -5.95  -5.84  -5.73   -5.5   4000    1.0\n",
       "mu[138]       -5.84  2.9e-3   0.18  -6.23  -5.95  -5.84  -5.73   -5.5   4000    1.0\n",
       "mu[139]       -5.84  2.9e-3   0.18  -6.23  -5.95  -5.84  -5.73   -5.5   4000    1.0\n",
       "mu[140]       -7.36  4.2e-4   0.03  -7.42  -7.38  -7.36  -7.35  -7.31   3780    1.0\n",
       "mu[141]       -7.36  4.2e-4   0.03  -7.42  -7.38  -7.36  -7.35  -7.31   3780    1.0\n",
       "mu[142]       -7.36  4.2e-4   0.03  -7.42  -7.38  -7.36  -7.35  -7.31   3780    1.0\n",
       "mu[143]       -7.36  4.2e-4   0.03  -7.42  -7.38  -7.36  -7.35  -7.31   3780    1.0\n",
       "mu[144]       -7.36  4.2e-4   0.03  -7.42  -7.38  -7.36  -7.35  -7.31   3780    1.0\n",
       "mu[145]       -7.36  4.2e-4   0.03  -7.42  -7.38  -7.36  -7.35  -7.31   3780    1.0\n",
       "mu[146]       -7.36  4.2e-4   0.03  -7.42  -7.38  -7.36  -7.35  -7.31   3780    1.0\n",
       "mu[147]       -7.36  4.2e-4   0.03  -7.42  -7.38  -7.36  -7.35  -7.31   3780    1.0\n",
       "mu[148]       -7.36  4.2e-4   0.03  -7.42  -7.38  -7.36  -7.35  -7.31   3780    1.0\n",
       "mu[149]       -7.36  4.2e-4   0.03  -7.42  -7.38  -7.36  -7.35  -7.31   3780    1.0\n",
       "mu[150]       -6.91  1.0e-3   0.06  -7.04  -6.95  -6.91  -6.87  -6.78   4000    1.0\n",
       "mu[151]       -6.91  1.0e-3   0.06  -7.04  -6.95  -6.91  -6.87  -6.78   4000    1.0\n",
       "mu[152]       -6.91  1.0e-3   0.06  -7.04  -6.95  -6.91  -6.87  -6.78   4000    1.0\n",
       "mu[153]       -6.91  1.0e-3   0.06  -7.04  -6.95  -6.91  -6.87  -6.78   4000    1.0\n",
       "mu[154]       -6.91  1.0e-3   0.06  -7.04  -6.95  -6.91  -6.87  -6.78   4000    1.0\n",
       "mu[155]       -6.91  1.0e-3   0.06  -7.04  -6.95  -6.91  -6.87  -6.78   4000    1.0\n",
       "mu[156]       -6.91  1.0e-3   0.06  -7.04  -6.95  -6.91  -6.87  -6.78   4000    1.0\n",
       "mu[157]       -6.91  1.0e-3   0.06  -7.04  -6.95  -6.91  -6.87  -6.78   4000    1.0\n",
       "mu[158]       -6.91  1.0e-3   0.06  -7.04  -6.95  -6.91  -6.87  -6.78   4000    1.0\n",
       "mu[159]       -6.91  1.0e-3   0.06  -7.04  -6.95  -6.91  -6.87  -6.78   4000    1.0\n",
       "mu[160]       -8.48  7.9e-4   0.05  -8.58  -8.51  -8.48  -8.45  -8.39   3599    1.0\n",
       "mu[161]       -8.48  7.9e-4   0.05  -8.58  -8.51  -8.48  -8.45  -8.39   3599    1.0\n",
       "mu[162]       -8.48  7.9e-4   0.05  -8.58  -8.51  -8.48  -8.45  -8.39   3599    1.0\n",
       "mu[163]       -8.48  7.9e-4   0.05  -8.58  -8.51  -8.48  -8.45  -8.39   3599    1.0\n",
       "mu[164]       -8.48  7.9e-4   0.05  -8.58  -8.51  -8.48  -8.45  -8.39   3599    1.0\n",
       "mu[165]       -8.48  7.9e-4   0.05  -8.58  -8.51  -8.48  -8.45  -8.39   3599    1.0\n",
       "mu[166]       -8.48  7.9e-4   0.05  -8.58  -8.51  -8.48  -8.45  -8.39   3599    1.0\n",
       "mu[167]       -8.48  7.9e-4   0.05  -8.58  -8.51  -8.48  -8.45  -8.39   3599    1.0\n",
       "mu[168]       -8.48  7.9e-4   0.05  -8.58  -8.51  -8.48  -8.45  -8.39   3599    1.0\n",
       "mu[169]       -8.48  7.9e-4   0.05  -8.58  -8.51  -8.48  -8.45  -8.39   3599    1.0\n",
       "mu[170]       -7.74  2.6e-3   0.16  -8.07  -7.85  -7.75  -7.64   -7.4   4000    1.0\n",
       "mu[171]       -7.74  2.6e-3   0.16  -8.07  -7.85  -7.75  -7.64   -7.4   4000    1.0\n",
       "mu[172]       -7.74  2.6e-3   0.16  -8.07  -7.85  -7.75  -7.64   -7.4   4000    1.0\n",
       "mu[173]       -7.74  2.6e-3   0.16  -8.07  -7.85  -7.75  -7.64   -7.4   4000    1.0\n",
       "mu[174]       -7.74  2.6e-3   0.16  -8.07  -7.85  -7.75  -7.64   -7.4   4000    1.0\n",
       "mu[175]       -7.74  2.6e-3   0.16  -8.07  -7.85  -7.75  -7.64   -7.4   4000    1.0\n",
       "mu[176]       -7.74  2.6e-3   0.16  -8.07  -7.85  -7.75  -7.64   -7.4   4000    1.0\n",
       "mu[177]       -7.74  2.6e-3   0.16  -8.07  -7.85  -7.75  -7.64   -7.4   4000    1.0\n",
       "mu[178]       -7.74  2.6e-3   0.16  -8.07  -7.85  -7.75  -7.64   -7.4   4000    1.0\n",
       "mu[179]       -7.74  2.6e-3   0.16  -8.07  -7.85  -7.75  -7.64   -7.4   4000    1.0\n",
       "mu[180]       -8.23  1.5e-3   0.09  -8.39  -8.28  -8.23  -8.17  -8.05   3789    1.0\n",
       "mu[181]       -8.23  1.5e-3   0.09  -8.39  -8.28  -8.23  -8.17  -8.05   3789    1.0\n",
       "mu[182]       -8.23  1.5e-3   0.09  -8.39  -8.28  -8.23  -8.17  -8.05   3789    1.0\n",
       "mu[183]       -8.23  1.5e-3   0.09  -8.39  -8.28  -8.23  -8.17  -8.05   3789    1.0\n",
       "mu[184]       -8.23  1.5e-3   0.09  -8.39  -8.28  -8.23  -8.17  -8.05   3789    1.0\n",
       "mu[185]       -8.23  1.5e-3   0.09  -8.39  -8.28  -8.23  -8.17  -8.05   3789    1.0\n",
       "mu[186]       -8.23  1.5e-3   0.09  -8.39  -8.28  -8.23  -8.17  -8.05   3789    1.0\n",
       "mu[187]       -8.23  1.5e-3   0.09  -8.39  -8.28  -8.23  -8.17  -8.05   3789    1.0\n",
       "mu[188]       -8.23  1.5e-3   0.09  -8.39  -8.28  -8.23  -8.17  -8.05   3789    1.0\n",
       "mu[189]       -8.23  1.5e-3   0.09  -8.39  -8.28  -8.23  -8.17  -8.05   3789    1.0\n",
       "mu[190]       -8.34  2.7e-3   0.17  -8.66  -8.44  -8.34  -8.22  -8.01   3771    1.0\n",
       "mu[191]       -8.34  2.7e-3   0.17  -8.66  -8.44  -8.34  -8.22  -8.01   3771    1.0\n",
       "mu[192]       -8.34  2.7e-3   0.17  -8.66  -8.44  -8.34  -8.22  -8.01   3771    1.0\n",
       "mu[193]       -8.34  2.7e-3   0.17  -8.66  -8.44  -8.34  -8.22  -8.01   3771    1.0\n",
       "mu[194]       -8.34  2.7e-3   0.17  -8.66  -8.44  -8.34  -8.22  -8.01   3771    1.0\n",
       "mu[195]       -8.34  2.7e-3   0.17  -8.66  -8.44  -8.34  -8.22  -8.01   3771    1.0\n",
       "mu[196]       -8.34  2.7e-3   0.17  -8.66  -8.44  -8.34  -8.22  -8.01   3771    1.0\n",
       "mu[197]       -8.34  2.7e-3   0.17  -8.66  -8.44  -8.34  -8.22  -8.01   3771    1.0\n",
       "mu[198]       -8.34  2.7e-3   0.17  -8.66  -8.44  -8.34  -8.22  -8.01   3771    1.0\n",
       "mu[199]       -8.34  2.7e-3   0.17  -8.66  -8.44  -8.34  -8.22  -8.01   3771    1.0\n",
       "mu[200]       -7.39  1.2e-3   0.07  -7.53  -7.43  -7.39  -7.34  -7.24   3618    1.0\n",
       "mu[201]       -7.39  1.2e-3   0.07  -7.53  -7.43  -7.39  -7.34  -7.24   3618    1.0\n",
       "mu[202]       -7.39  1.2e-3   0.07  -7.53  -7.43  -7.39  -7.34  -7.24   3618    1.0\n",
       "mu[203]       -7.39  1.2e-3   0.07  -7.53  -7.43  -7.39  -7.34  -7.24   3618    1.0\n",
       "mu[204]       -7.39  1.2e-3   0.07  -7.53  -7.43  -7.39  -7.34  -7.24   3618    1.0\n",
       "mu[205]       -7.39  1.2e-3   0.07  -7.53  -7.43  -7.39  -7.34  -7.24   3618    1.0\n",
       "mu[206]       -7.39  1.2e-3   0.07  -7.53  -7.43  -7.39  -7.34  -7.24   3618    1.0\n",
       "mu[207]       -7.39  1.2e-3   0.07  -7.53  -7.43  -7.39  -7.34  -7.24   3618    1.0\n",
       "mu[208]       -7.39  1.2e-3   0.07  -7.53  -7.43  -7.39  -7.34  -7.24   3618    1.0\n",
       "mu[209]       -7.39  1.2e-3   0.07  -7.53  -7.43  -7.39  -7.34  -7.24   3618    1.0\n",
       "mu[210]       -7.84  9.4e-4   0.05  -7.94  -7.87  -7.84   -7.8  -7.73   3259    1.0\n",
       "mu[211]       -7.84  9.4e-4   0.05  -7.94  -7.87  -7.84   -7.8  -7.73   3259    1.0\n",
       "mu[212]       -7.84  9.4e-4   0.05  -7.94  -7.87  -7.84   -7.8  -7.73   3259    1.0\n",
       "mu[213]       -7.84  9.4e-4   0.05  -7.94  -7.87  -7.84   -7.8  -7.73   3259    1.0\n",
       "mu[214]       -7.84  9.4e-4   0.05  -7.94  -7.87  -7.84   -7.8  -7.73   3259    1.0\n",
       "mu[215]       -7.84  9.4e-4   0.05  -7.94  -7.87  -7.84   -7.8  -7.73   3259    1.0\n",
       "mu[216]       -7.84  9.4e-4   0.05  -7.94  -7.87  -7.84   -7.8  -7.73   3259    1.0\n",
       "mu[217]       -7.84  9.4e-4   0.05  -7.94  -7.87  -7.84   -7.8  -7.73   3259    1.0\n",
       "mu[218]       -7.84  9.4e-4   0.05  -7.94  -7.87  -7.84   -7.8  -7.73   3259    1.0\n",
       "mu[219]       -7.84  9.4e-4   0.05  -7.94  -7.87  -7.84   -7.8  -7.73   3259    1.0\n",
       "mu[220]       -7.98  1.7e-3   0.11  -8.21  -8.05  -7.99  -7.92  -7.76   4000    1.0\n",
       "mu[221]       -7.98  1.7e-3   0.11  -8.21  -8.05  -7.99  -7.92  -7.76   4000    1.0\n",
       "mu[222]       -7.98  1.7e-3   0.11  -8.21  -8.05  -7.99  -7.92  -7.76   4000    1.0\n",
       "mu[223]       -7.98  1.7e-3   0.11  -8.21  -8.05  -7.99  -7.92  -7.76   4000    1.0\n",
       "mu[224]       -7.98  1.7e-3   0.11  -8.21  -8.05  -7.99  -7.92  -7.76   4000    1.0\n",
       "mu[225]       -7.98  1.7e-3   0.11  -8.21  -8.05  -7.99  -7.92  -7.76   4000    1.0\n",
       "mu[226]       -7.98  1.7e-3   0.11  -8.21  -8.05  -7.99  -7.92  -7.76   4000    1.0\n",
       "mu[227]       -7.98  1.7e-3   0.11  -8.21  -8.05  -7.99  -7.92  -7.76   4000    1.0\n",
       "mu[228]       -7.98  1.7e-3   0.11  -8.21  -8.05  -7.99  -7.92  -7.76   4000    1.0\n",
       "mu[229]       -7.98  1.7e-3   0.11  -8.21  -8.05  -7.99  -7.92  -7.76   4000    1.0\n",
       "mu[230]       -6.61  7.7e-4   0.05  -6.71  -6.64  -6.61  -6.58  -6.51   4000    1.0\n",
       "mu[231]       -6.61  7.7e-4   0.05  -6.71  -6.64  -6.61  -6.58  -6.51   4000    1.0\n",
       "mu[232]       -6.61  7.7e-4   0.05  -6.71  -6.64  -6.61  -6.58  -6.51   4000    1.0\n",
       "mu[233]       -6.61  7.7e-4   0.05  -6.71  -6.64  -6.61  -6.58  -6.51   4000    1.0\n",
       "mu[234]       -6.61  7.7e-4   0.05  -6.71  -6.64  -6.61  -6.58  -6.51   4000    1.0\n",
       "mu[235]       -6.61  7.7e-4   0.05  -6.71  -6.64  -6.61  -6.58  -6.51   4000    1.0\n",
       "mu[236]       -6.61  7.7e-4   0.05  -6.71  -6.64  -6.61  -6.58  -6.51   4000    1.0\n",
       "mu[237]       -6.61  7.7e-4   0.05  -6.71  -6.64  -6.61  -6.58  -6.51   4000    1.0\n",
       "mu[238]       -6.61  7.7e-4   0.05  -6.71  -6.64  -6.61  -6.58  -6.51   4000    1.0\n",
       "mu[239]       -6.61  7.7e-4   0.05  -6.71  -6.64  -6.61  -6.58  -6.51   4000    1.0\n",
       "mu[240]       -8.15  3.9e-3   0.25  -8.65  -8.31  -8.16   -8.0  -7.64   4000    1.0\n",
       "mu[241]       -8.15  3.9e-3   0.25  -8.65  -8.31  -8.16   -8.0  -7.64   4000    1.0\n",
       "mu[242]       -8.15  3.9e-3   0.25  -8.65  -8.31  -8.16   -8.0  -7.64   4000    1.0\n",
       "mu[243]       -8.15  3.9e-3   0.25  -8.65  -8.31  -8.16   -8.0  -7.64   4000    1.0\n",
       "mu[244]       -8.15  3.9e-3   0.25  -8.65  -8.31  -8.16   -8.0  -7.64   4000    1.0\n",
       "mu[245]       -8.15  3.9e-3   0.25  -8.65  -8.31  -8.16   -8.0  -7.64   4000    1.0\n",
       "mu[246]       -8.15  3.9e-3   0.25  -8.65  -8.31  -8.16   -8.0  -7.64   4000    1.0\n",
       "mu[247]       -8.15  3.9e-3   0.25  -8.65  -8.31  -8.16   -8.0  -7.64   4000    1.0\n",
       "mu[248]       -8.15  3.9e-3   0.25  -8.65  -8.31  -8.16   -8.0  -7.64   4000    1.0\n",
       "mu[249]       -8.15  3.9e-3   0.25  -8.65  -8.31  -8.16   -8.0  -7.64   4000    1.0\n",
       "ypred[0]      -8.17    0.06   1.11 -10.42  -8.84  -8.18  -7.54  -5.72    391   1.01\n",
       "ypred[1]      -8.77    0.05   1.09 -10.98  -9.41  -8.79  -8.12  -6.57    455   1.01\n",
       "ypred[2]      -6.78    0.06   1.03  -8.68  -7.42  -6.84  -6.27  -4.42    318   1.02\n",
       "ypred[3]      -8.59    0.05   1.26 -10.99  -9.44  -8.58  -7.77  -6.07    720   1.01\n",
       "ypred[4]      -7.77    0.05   0.96  -9.63  -8.39   -7.8  -7.18  -5.66    446   1.01\n",
       "ypred[5]      -7.31    0.05   0.86  -8.93  -7.86  -7.36  -6.84  -5.35    287   1.02\n",
       "ypred[6]      -5.85    0.06   0.95   -7.4  -6.46  -5.97  -5.42  -3.51    242   1.02\n",
       "ypred[7]      -6.58    0.05   0.81  -7.99  -7.11  -6.65  -6.15  -4.75    283   1.03\n",
       "ypred[8]      -7.56    0.04   0.84   -9.2   -8.1  -7.56  -7.04  -5.79    465   1.02\n",
       "ypred[9]      -7.58    0.04   1.04  -9.57  -8.26  -7.59  -6.91  -5.46    704   1.01\n",
       "ypred[10]     -8.28    0.03    0.7  -9.68  -8.73  -8.28  -7.81  -6.88    466   1.01\n",
       "ypred[11]     -9.97    0.04    1.4 -12.73  -10.9  -9.96  -9.09  -7.08   1143    1.0\n",
       "ypred[12]     -8.03    0.03   0.52  -9.04  -8.37  -8.03  -7.69  -6.98    363   1.02\n",
       "ypred[13]     -5.69    0.04   0.83  -7.22  -6.25  -5.73  -5.19  -3.93    431   1.01\n",
       "ypred[14]     -7.45    0.02   0.41  -8.24  -7.73  -7.46   -7.2  -6.61    309   1.02\n",
       "ypred[15]     -6.93    0.02   0.43  -7.75  -7.22  -6.94  -6.66  -6.01    335   1.02\n",
       "ypred[16]     -8.68    0.02   0.36   -9.4  -8.91  -8.68  -8.45  -7.97    369   1.01\n",
       "ypred[17]     -7.84    0.02   0.62  -9.11  -8.23  -7.84  -7.43   -6.6   1180   1.01\n",
       "ypred[18]     -8.36    0.01   0.39  -9.12  -8.62  -8.37  -8.11   -7.6    745   1.01\n",
       "ypred[19]     -8.45    0.01   0.59  -9.62  -8.82  -8.45  -8.08   -7.3   2522    1.0\n",
       "ypred[20]     -7.42    0.01    0.3   -8.0  -7.62  -7.42  -7.23  -6.82    792    1.0\n",
       "ypred[21]     -7.89  7.6e-3   0.21  -8.31  -8.03  -7.89  -7.76  -7.46    784   1.01\n",
       "ypred[22]     -8.04  8.8e-3   0.38  -8.78  -8.28  -8.04   -7.8  -7.27   1904    1.0\n",
       "ypred[23]      -6.6  4.9e-3   0.18  -6.96  -6.72  -6.61  -6.49  -6.25   1308   1.01\n",
       "ypred[24]     -8.19    0.01   0.85  -9.89  -8.71  -8.18  -7.65  -6.46   4000    1.0\n",
       "log_lik[0]    -1.27    0.01   0.83  -3.33   -1.7   -1.1   -0.7  -0.12   4000    1.0\n",
       "log_lik[1]     0.46  4.4e-3   0.24  -0.08   0.31   0.48   0.62   0.88   2989    1.0\n",
       "log_lik[2]    -0.02  5.2e-3   0.33   -0.8   -0.2   0.02    0.2   0.51   4000    1.0\n",
       "log_lik[3]     0.47  4.5e-3   0.24  -0.07   0.32   0.49   0.64    0.9   2953    1.0\n",
       "log_lik[4]     -0.4  7.4e-3   0.47  -1.57  -0.64  -0.32  -0.08   0.27   4000    1.0\n",
       "log_lik[5]     0.19  4.3e-3   0.27  -0.44   0.03   0.22   0.39   0.65   4000    1.0\n",
       "log_lik[6]     0.17  4.4e-3   0.28  -0.48 3.9e-3   0.19   0.36   0.64   4000    1.0\n",
       "log_lik[7]     0.37  3.9e-3   0.25  -0.18   0.23   0.39   0.55   0.81   4000    1.0\n",
       "log_lik[8]      0.3  4.0e-3   0.26  -0.27   0.15   0.32   0.48   0.75   4000    1.0\n",
       "log_lik[9]    -0.09  5.5e-3   0.35  -0.92  -0.27  -0.04   0.16   0.47   4000    1.0\n",
       "log_lik[10]   -0.34  4.7e-3    0.3   -1.0  -0.52  -0.32  -0.13   0.14   4000    1.0\n",
       "log_lik[11]   -1.01  8.2e-3   0.52  -2.33  -1.28  -0.92  -0.64  -0.26   4000    1.0\n",
       "log_lik[12]   -0.43  5.0e-3   0.32  -1.14  -0.62  -0.39   -0.2   0.08   4000    1.0\n",
       "log_lik[13]    -0.2  4.2e-3   0.27  -0.78  -0.37  -0.19-6.2e-3   0.26   4000    1.0\n",
       "log_lik[14]    0.03  7.2e-3   0.24  -0.49  -0.12   0.05    0.2   0.48   1108    1.0\n",
       "log_lik[15]   -0.27  4.4e-3   0.28  -0.88  -0.45  -0.25  -0.06    0.2   4000    1.0\n",
       "log_lik[16]   -0.05  6.7e-3   0.24  -0.58   -0.2  -0.04   0.11   0.39   1298    1.0\n",
       "log_lik[17]   -1.47    0.01   0.74  -3.25  -1.88  -1.32  -0.92  -0.47   4000    1.0\n",
       "log_lik[18]    -0.7  6.5e-3   0.41  -1.66  -0.93  -0.63  -0.41  -0.09   4000    1.0\n",
       "log_lik[19]    -0.2  4.1e-3   0.26  -0.76  -0.35  -0.18  -0.02   0.25   4000    1.0\n",
       "log_lik[20]    0.97  8.0e-3   0.29   0.32   0.79    1.0   1.18   1.46   1307    1.0\n",
       "log_lik[21]    0.71  8.0e-3   0.35   -0.1   0.51   0.76   0.95   1.26   1916    1.0\n",
       "log_lik[22]    0.72  8.0e-3   0.34  -0.07   0.53   0.77   0.97   1.27   1875    1.0\n",
       "log_lik[23]    0.97  8.0e-3   0.29   0.32   0.79    1.0   1.17   1.46   1312    1.0\n",
       "log_lik[24]     1.2  7.1e-3   0.26   0.65   1.04   1.22   1.39   1.66   1329    1.0\n",
       "log_lik[25]    1.21  7.2e-3   0.26   0.66   1.05   1.23    1.4   1.67   1304    1.0\n",
       "log_lik[26]    0.43  7.1e-3   0.45  -0.67    0.2    0.5   0.75   1.09   4000    1.0\n",
       "log_lik[27]    0.19  8.7e-3   0.55  -1.17   -0.1   0.29   0.59   0.97   4000    1.0\n",
       "log_lik[28]    1.23  7.4e-3   0.26   0.66   1.06   1.25   1.41   1.68   1253    1.0\n",
       "log_lik[29]    1.22  7.2e-3   0.26   0.66   1.05   1.24    1.4   1.67   1299    1.0\n",
       "log_lik[30]   -0.93  5.6e-3   0.25  -1.49  -1.09   -0.9  -0.75  -0.51   2078    1.0\n",
       "log_lik[31]   -2.77    0.01   0.85  -4.81  -3.25  -2.63  -2.15  -1.52   4000    1.0\n",
       "log_lik[32]    -0.8  4.1e-3   0.22  -1.28  -0.94  -0.78  -0.65   -0.4   2982    1.0\n",
       "log_lik[33]   -0.76  4.5e-3   0.24  -1.26  -0.91  -0.75   -0.6  -0.36   2790    1.0\n",
       "log_lik[34]   -1.29  5.2e-3   0.33  -2.03  -1.48  -1.25  -1.05  -0.77   4000    1.0\n",
       "log_lik[35]   -1.17  4.6e-3   0.29  -1.85  -1.33  -1.14  -0.96  -0.71   4000    1.0\n",
       "log_lik[36]   -0.92  3.7e-3   0.23  -1.45  -1.06  -0.91  -0.76  -0.52   4000    1.0\n",
       "log_lik[37]   -0.85  4.1e-3   0.23  -1.34  -0.98  -0.83  -0.69  -0.45   3089    1.0\n",
       "log_lik[38]   -2.15    0.01   0.67  -3.73  -2.51  -2.03  -1.66  -1.21   4000    1.0\n",
       "log_lik[39]   -1.17  4.7e-3    0.3  -1.83  -1.35  -1.14  -0.96  -0.69   4000    1.0\n",
       "log_lik[40]    0.01  5.4e-3   0.24  -0.51  -0.13   0.03   0.18   0.43   1968    1.0\n",
       "log_lik[41]    0.03  5.4e-3   0.24  -0.49  -0.11   0.05    0.2   0.45   1935    1.0\n",
       "log_lik[42]   -1.24    0.01   0.64  -2.81  -1.58  -1.12  -0.78  -0.34   4000    1.0\n",
       "log_lik[43]   -0.02  5.4e-3   0.24  -0.55  -0.16 8.8e-4   0.15   0.41   2026    1.0\n",
       "log_lik[44]  3.5e-3  5.4e-3   0.24  -0.52  -0.14   0.02   0.17   0.43   1985    1.0\n",
       "log_lik[45]   -0.04  4.7e-3   0.24  -0.57  -0.19  -0.03   0.13   0.38   2731    1.0\n",
       "log_lik[46]   -0.33  5.0e-3   0.31  -1.05   -0.5  -0.29  -0.12   0.18   4000    1.0\n",
       "log_lik[47]   -0.29  5.2e-3    0.3  -0.97  -0.45  -0.26  -0.08    0.2   3251    1.0\n",
       "log_lik[48]   -0.42  5.3e-3   0.33  -1.25  -0.59  -0.37  -0.19   0.11   4000    1.0\n",
       "log_lik[49]   -1.64    0.01   0.79  -3.64  -2.07   -1.5  -1.07  -0.49   4000    1.0\n",
       "log_lik[50]   -0.72    0.01   0.78   -2.6  -1.14  -0.57  -0.15   0.35   4000    1.0\n",
       "log_lik[51]    0.88  4.8e-3   0.24   0.33   0.73    0.9   1.05   1.32   2630    1.0\n",
       "log_lik[52]    0.77  5.3e-3   0.26    0.2   0.61   0.79   0.95   1.21   2288    1.0\n",
       "log_lik[53]    0.71  5.5e-3   0.26   0.12   0.54   0.73   0.89   1.16   2294    1.0\n",
       "log_lik[54]    0.07  7.0e-3   0.44   -1.0  -0.17   0.14   0.39   0.73   4000    1.0\n",
       "log_lik[55]    0.49  6.1e-3   0.31  -0.23   0.31   0.52    0.7   0.99   2555    1.0\n",
       "log_lik[56]    0.54  5.3e-3   0.29  -0.11   0.38   0.57   0.74   1.02   3058    1.0\n",
       "log_lik[57]    0.69  5.6e-3   0.27   0.09   0.52   0.71   0.88   1.14   2304    1.0\n",
       "log_lik[58]    0.86  4.8e-3   0.24   0.31   0.71   0.88   1.03   1.29   2641    1.0\n",
       "log_lik[59]    0.56  5.2e-3   0.29  -0.09   0.39   0.58   0.76   1.03   3037    1.0\n",
       "log_lik[60]    1.37  7.0e-3   0.27   0.78    1.2    1.4   1.57   1.85   1542    1.0\n",
       "log_lik[61]    1.42  7.4e-3   0.28   0.82   1.25   1.44   1.62    1.9   1424    1.0\n",
       "log_lik[62]    1.38  7.1e-3   0.28   0.78    1.2    1.4   1.58   1.86   1533    1.0\n",
       "log_lik[63]    0.33  8.9e-3   0.56  -1.06   0.04   0.43   0.73   1.14   4000    1.0\n",
       "log_lik[64]    1.31  6.5e-3   0.28   0.72   1.13   1.33   1.51   1.78   1811    1.0\n",
       "log_lik[65]    1.38  7.1e-3   0.28   0.78    1.2    1.4   1.58   1.86   1536    1.0\n",
       "log_lik[66]     1.4  7.2e-3   0.28   0.79   1.22   1.42   1.59   1.87   1483    1.0\n",
       "log_lik[67]    0.47  9.5e-3   0.52  -0.81   0.22   0.57   0.84    1.2   3017    1.0\n",
       "log_lik[68]    1.27  6.3e-3   0.28   0.68   1.09   1.29   1.47   1.75   1928    1.0\n",
       "log_lik[69]    1.01  5.8e-3   0.32   0.27   0.84   1.05   1.23   1.52   2964    1.0\n",
       "log_lik[70]  7.2e-3  8.6e-3   0.54  -1.28   -0.3    0.1   0.39   0.82   4000    1.0\n",
       "log_lik[71]    0.51  6.2e-3   0.36  -0.34   0.31   0.56   0.76   1.08   3437    1.0\n",
       "log_lik[72]     0.9  5.2e-3   0.26   0.28   0.75   0.93   1.09   1.35   2536    1.0\n",
       "log_lik[73]    1.12  5.1e-3   0.26   0.56   0.97   1.14   1.31   1.57   2478    1.0\n",
       "log_lik[74]    1.09  5.0e-3   0.26   0.52   0.93   1.11   1.27   1.53   2611    1.0\n",
       "log_lik[75]    1.13  5.2e-3   0.26   0.57   0.97   1.14    1.3   1.58   2423    1.0\n",
       "log_lik[76]    1.09  5.2e-3   0.25   0.53   0.93   1.11   1.26   1.52   2345    1.0\n",
       "log_lik[77]    0.96  4.9e-3   0.26   0.37    0.8   0.98   1.14   1.41   2921    1.0\n",
       "log_lik[78]    0.59  6.1e-3   0.34  -0.22   0.41   0.64   0.83   1.12   3121    1.0\n",
       "log_lik[79]    0.18  8.3e-3   0.49  -1.05  -0.08   0.27   0.53    0.9   3540    1.0\n",
       "log_lik[80]   -1.52    0.01   0.66   -3.1  -1.88   -1.4  -1.04  -0.59   4000    1.0\n",
       "log_lik[81]   -0.94  6.8e-3   0.43  -1.99  -1.17  -0.86  -0.63  -0.29   4000    1.0\n",
       "log_lik[82]   -0.89  6.6e-3   0.41  -1.89  -1.12  -0.82   -0.6  -0.27   4000    1.0\n",
       "log_lik[83]   -0.37  4.6e-3   0.26  -0.96  -0.54  -0.35  -0.19   0.08   3297    1.0\n",
       "log_lik[84]   -0.39  4.6e-3   0.27  -0.98  -0.55  -0.36   -0.2   0.07   3319    1.0\n",
       "log_lik[85]    -0.4  4.7e-3   0.27   -1.0  -0.56  -0.37  -0.21   0.06   3329    1.0\n",
       "log_lik[86]   -0.93  6.8e-3   0.43  -1.95  -1.17  -0.86  -0.63   -0.3   4000    1.0\n",
       "log_lik[87]   -0.13  4.3e-3   0.23  -0.63  -0.27  -0.11   0.03   0.29   2803    1.0\n",
       "log_lik[88]   -0.12  4.3e-3   0.23  -0.62  -0.26  -0.11   0.04   0.29   2818    1.0\n",
       "log_lik[89]   -0.47  4.9e-3   0.28   -1.1  -0.64  -0.43  -0.27   0.01   3329    1.0\n",
       "log_lik[90]   -2.62    0.01    0.9  -4.72  -3.09  -2.47  -1.95  -1.37   4000    1.0\n",
       "log_lik[91]   -1.09  4.9e-3   0.31  -1.78  -1.28  -1.05  -0.87  -0.59   4000    1.0\n",
       "log_lik[92]   -0.69  5.7e-3   0.23  -1.23  -0.84  -0.68  -0.53  -0.29   1687    1.0\n",
       "log_lik[93]   -0.73  5.7e-3   0.24  -1.26  -0.87  -0.71  -0.56  -0.33   1734    1.0\n",
       "log_lik[94]   -0.74  5.7e-3   0.24  -1.28  -0.89  -0.72  -0.57  -0.34   1763    1.0\n",
       "log_lik[95]   -0.63  5.6e-3   0.23  -1.15  -0.77  -0.61  -0.47  -0.23   1749    1.0\n",
       "log_lik[96]   -0.86  4.8e-3   0.26   -1.4  -1.02  -0.84  -0.68  -0.42   2915    1.0\n",
       "log_lik[97]   -1.41  6.5e-3   0.41  -2.38  -1.64  -1.36  -1.11   -0.8   4000    1.0\n",
       "log_lik[98]    -1.7  8.1e-3   0.51  -2.92  -1.98  -1.61  -1.33  -0.96   4000    1.0\n",
       "log_lik[99]   -1.08  4.9e-3   0.31  -1.79  -1.26  -1.05  -0.86  -0.59   4000    1.0\n",
       "log_lik[100]   0.06  4.5e-3   0.24  -0.44  -0.09   0.08   0.23   0.47   2841    1.0\n",
       "log_lik[101]  -0.06  4.8e-3   0.25   -0.6  -0.21  -0.04   0.12   0.38   2736    1.0\n",
       "log_lik[102]  -0.87  7.9e-3    0.5   -2.1  -1.13  -0.78  -0.52  -0.15   4000    1.0\n",
       "log_lik[103]  -0.09  4.4e-3   0.25  -0.65  -0.24  -0.06   0.09   0.35   3254    1.0\n",
       "log_lik[104]   0.08  4.6e-3   0.24  -0.43  -0.06    0.1   0.25    0.5   2634    1.0\n",
       "log_lik[105]  -0.97  8.7e-3   0.54  -2.25  -1.27  -0.88  -0.58  -0.19   3801    1.0\n",
       "log_lik[106]   0.08  4.5e-3   0.24  -0.42  -0.07    0.1   0.25   0.49   2777    1.0\n",
       "log_lik[107]  -0.21  4.8e-3   0.28  -0.83  -0.37  -0.18  -0.01   0.26   3302    1.0\n",
       "log_lik[108]  -0.35  5.4e-3   0.31  -1.05  -0.54  -0.32  -0.13   0.16   3396    1.0\n",
       "log_lik[109]   -1.6    0.01    0.8  -3.49  -2.04  -1.45  -1.02  -0.49   4000    1.0\n",
       "log_lik[110]  -1.27  4.3e-3   0.24  -1.81  -1.41  -1.25   -1.1  -0.85   3054    1.0\n",
       "log_lik[111]  -1.37  4.7e-3   0.27  -1.99  -1.51  -1.34  -1.19  -0.93   3218    1.0\n",
       "log_lik[112]  -1.27  6.9e-3    0.3  -1.91  -1.46  -1.24  -1.05  -0.76   1934    1.0\n",
       "log_lik[113]  -2.17  7.9e-3    0.5  -3.31  -2.45  -2.11  -1.83  -1.37   4000    1.0\n",
       "log_lik[114]   -1.7  5.8e-3   0.37   -2.5  -1.91  -1.66  -1.44  -1.08   4000    1.0\n",
       "log_lik[115]   -1.4  6.4e-3   0.32  -2.07   -1.6  -1.37  -1.17  -0.87   2478    1.0\n",
       "log_lik[116]  -2.18  7.9e-3    0.5  -3.32  -2.45  -2.11  -1.83  -1.38   4000    1.0\n",
       "log_lik[117]  -1.24  6.9e-3    0.3  -1.89  -1.43  -1.21  -1.03  -0.74   1876    1.0\n",
       "log_lik[118]  -1.82  7.3e-3   0.46  -2.98  -2.03  -1.71  -1.49   -1.2   4000    1.0\n",
       "log_lik[119]  -1.71  6.5e-3   0.41  -2.74   -1.9  -1.62  -1.42  -1.15   4000    1.0\n",
       "log_lik[120]  -0.53    0.01   0.67  -2.13  -0.87   -0.4  -0.05    0.4   4000    1.0\n",
       "log_lik[121]   0.79  5.8e-3   0.25   0.24   0.64   0.81   0.97   1.22   1829    1.0\n",
       "log_lik[122]   0.83  6.1e-3   0.25   0.27   0.67   0.85    1.0   1.26   1729    1.0\n",
       "log_lik[123]   0.83  6.1e-3   0.25   0.27   0.67   0.85    1.0   1.27   1730    1.0\n",
       "log_lik[124]   0.25  6.3e-3   0.36  -0.57   0.06    0.3    0.5    0.8   3210    1.0\n",
       "log_lik[125]   0.58  5.4e-3   0.27   0.01   0.42   0.61   0.78   1.04   2471    1.0\n",
       "log_lik[126]  -0.56    0.01   0.67  -2.13  -0.92  -0.44  -0.07   0.36   4000    1.0\n",
       "log_lik[127]   0.58  5.4e-3   0.27 1.9e-3   0.41    0.6   0.77   1.04   2479    1.0\n",
       "log_lik[128]   0.48  5.7e-3   0.29  -0.17   0.31   0.51   0.69   0.97   2666    1.0\n",
       "log_lik[129]    0.8  5.9e-3   0.25   0.25   0.65   0.83   0.98   1.23   1805    1.0\n",
       "log_lik[130]   -4.4    0.03   1.89  -8.89  -5.46  -4.07  -2.99  -1.71   4000    1.0\n",
       "log_lik[131]  -0.37  6.5e-3   0.25  -0.92  -0.52  -0.35   -0.2   0.05   1430    1.0\n",
       "log_lik[132]   -0.6  6.3e-3   0.28  -1.21  -0.76  -0.57  -0.41  -0.13   1945    1.0\n",
       "log_lik[133]  -0.56  6.3e-3   0.27  -1.18  -0.72  -0.53  -0.37   -0.1   1885    1.0\n",
       "log_lik[134]  -0.69  6.4e-3   0.29  -1.35  -0.86  -0.66  -0.48  -0.21   2123    1.0\n",
       "log_lik[135]   -0.4  6.7e-3   0.25  -0.97  -0.56  -0.38  -0.22   0.02   1426    1.0\n",
       "log_lik[136]  -0.37  6.5e-3   0.25  -0.92  -0.53  -0.35   -0.2   0.05   1429    1.0\n",
       "log_lik[137]  -0.46  6.7e-3   0.26  -1.05  -0.62  -0.44  -0.28  -0.02   1482    1.0\n",
       "log_lik[138]  -0.49  6.7e-3   0.26  -1.08  -0.65  -0.47   -0.3  -0.04   1518    1.0\n",
       "log_lik[139]  -0.45  6.7e-3   0.26  -1.03  -0.61  -0.43  -0.26  -0.01   1466    1.0\n",
       "log_lik[140]    1.6  6.1e-3   0.29   0.99   1.42   1.62    1.8   2.08   2189    1.0\n",
       "log_lik[141]   1.45  5.9e-3   0.28   0.86   1.28   1.47   1.65   1.93   2199    1.0\n",
       "log_lik[142]   1.01  6.0e-3   0.36   0.17    0.8   1.05   1.26   1.56   3672    1.0\n",
       "log_lik[143]   1.47  5.9e-3   0.28   0.88    1.3   1.49   1.67   1.95   2179    1.0\n",
       "log_lik[144]   1.55  6.1e-3   0.28   0.94   1.37   1.57   1.75   2.03   2133    1.0\n",
       "log_lik[145]   1.43  5.4e-3   0.28   0.84   1.26   1.45   1.63    1.9   2646    1.0\n",
       "log_lik[146]   1.56  6.1e-3   0.28   0.95   1.38   1.58   1.76   2.04   2133    1.0\n",
       "log_lik[147]   0.99  6.1e-3   0.37   0.14   0.79   1.04   1.25   1.55   3693    1.0\n",
       "log_lik[148]   0.75  7.7e-3   0.46  -0.38   0.52   0.83   1.06   1.41   3583    1.0\n",
       "log_lik[149]   1.59  6.0e-3   0.28   0.98   1.41   1.61   1.79   2.08   2244    1.0\n",
       "log_lik[150]  -0.66    0.01   0.64  -2.21   -1.0  -0.54   -0.2   0.26   4000    1.0\n",
       "log_lik[151]   0.64  4.5e-3   0.24   0.12   0.49   0.65   0.81   1.08   2836    1.0\n",
       "log_lik[152]   -0.3  7.8e-3   0.49  -1.49  -0.56  -0.21   0.05   0.41   4000    1.0\n",
       "log_lik[153]   0.43  4.6e-3   0.26  -0.16   0.27   0.45   0.61   0.87   3229    1.0\n",
       "log_lik[154]   0.66  4.6e-3   0.24   0.15   0.51   0.67   0.83    1.1   2795    1.0\n",
       "log_lik[155]   0.56  4.5e-3   0.24   0.03   0.42   0.58   0.73   0.99   2960    1.0\n",
       "log_lik[156]  -0.46  8.8e-3   0.56   -1.8  -0.76  -0.35  -0.06   0.33   4000    1.0\n",
       "log_lik[157]   0.38  4.6e-3   0.27  -0.22   0.22    0.4   0.57   0.83   3333    1.0\n",
       "log_lik[158]   0.46  4.9e-3   0.26  -0.12   0.31   0.49   0.65   0.91   2831    1.0\n",
       "log_lik[159]   0.55  4.8e-3   0.25-8.8e-3   0.39   0.57   0.73   0.98   2768    1.0\n",
       "log_lik[160]   0.83  5.4e-3   0.26   0.28   0.67   0.86   1.02   1.27   2343    1.0\n",
       "log_lik[161]   0.51  5.6e-3   0.32  -0.24   0.33   0.54   0.73   1.05   3389    1.0\n",
       "log_lik[162]   0.24  7.2e-3   0.43   -0.8   0.02   0.31   0.52   0.87   3485    1.0\n",
       "log_lik[163]   0.89  5.4e-3   0.26   0.34   0.72   0.91   1.08   1.32   2286    1.0\n",
       "log_lik[164]  -0.27    0.01   0.64  -1.89  -0.59  -0.13   0.18    0.6   4000    1.0\n",
       "log_lik[165]   0.78  4.9e-3   0.27   0.19   0.62   0.81   0.97   1.24   2978    1.0\n",
       "log_lik[166]   0.98  5.4e-3   0.26   0.43   0.81    1.0   1.16   1.43   2331    1.0\n",
       "log_lik[167]   0.86  5.1e-3   0.26   0.31    0.7   0.89   1.04   1.32   2590    1.0\n",
       "log_lik[168]   0.88  5.1e-3   0.26   0.33   0.72   0.91   1.06   1.33   2550    1.0\n",
       "log_lik[169]    0.3  6.5e-3   0.39  -0.63   0.09   0.35   0.57   0.92   3620    1.0\n",
       "log_lik[170]  -0.31  4.2e-3   0.24  -0.83  -0.45  -0.29  -0.14   0.11   3136    1.0\n",
       "log_lik[171]  -0.27  4.2e-3   0.24  -0.78  -0.41  -0.26   -0.1   0.14   3139    1.0\n",
       "log_lik[172]  -0.33  4.2e-3   0.24  -0.85  -0.47  -0.31  -0.17   0.08   3196    1.0\n",
       "log_lik[173]  -2.43    0.02   0.97  -4.83  -2.94  -2.27  -1.72  -1.05   4000    1.0\n",
       "log_lik[174]  -1.28  8.0e-3    0.5  -2.52  -1.54   -1.2  -0.91  -0.54   4000    1.0\n",
       "log_lik[175]  -0.39  4.2e-3   0.24  -0.93  -0.54  -0.37  -0.23   0.02   3254    1.0\n",
       "log_lik[176]  -0.48  4.4e-3   0.25  -1.04  -0.63  -0.46  -0.31  -0.05   3359    1.0\n",
       "log_lik[177]  -0.65  4.8e-3   0.29   -1.3  -0.82  -0.61  -0.44  -0.17   3573    1.0\n",
       "log_lik[178]  -1.39  8.6e-3   0.55  -2.73  -1.68   -1.3   -1.0   -0.6   4000    1.0\n",
       "log_lik[179]  -0.27  4.2e-3   0.24  -0.78  -0.41  -0.26   -0.1   0.14   3140    1.0\n",
       "log_lik[180]  -0.07  5.3e-3    0.3  -0.75  -0.25  -0.03   0.14   0.41   3177    1.0\n",
       "log_lik[181]   0.27  4.8e-3   0.23  -0.23   0.13   0.28   0.43   0.67   2262    1.0\n",
       "log_lik[182]  -0.61  7.5e-3   0.48  -1.77  -0.88  -0.53  -0.27   0.09   4000    1.0\n",
       "log_lik[183]   0.32  4.9e-3   0.23  -0.18   0.18   0.34   0.48   0.72   2145    1.0\n",
       "log_lik[184]   0.32  4.9e-3   0.23  -0.17   0.18   0.34   0.48   0.72   2188    1.0\n",
       "log_lik[185]    0.3  4.8e-3   0.23  -0.19   0.17   0.32   0.46    0.7   2286    1.0\n",
       "log_lik[186]   0.12  5.1e-3   0.25  -0.42  -0.03   0.14   0.29   0.55   2359    1.0\n",
       "log_lik[187]  -0.57  7.4e-3   0.45  -1.67   -0.8   -0.5  -0.24    0.1   3720    1.0\n",
       "log_lik[188]  -1.15    0.01   0.67  -2.82   -1.5  -1.03  -0.66   -0.2   4000    1.0\n",
       "log_lik[189]   -0.5  7.3e-3   0.43  -1.56  -0.74  -0.43  -0.19   0.15   3518    1.0\n",
       "log_lik[190]  -1.04  6.9e-3   0.42  -2.05  -1.26  -0.98  -0.74  -0.42   3709    1.0\n",
       "log_lik[191]  -0.31  4.2e-3   0.23   -0.8  -0.45  -0.29  -0.14   0.09   3071    1.0\n",
       "log_lik[192]   -0.3  4.4e-3   0.23  -0.81  -0.44  -0.28  -0.13   0.11   2854    1.0\n",
       "log_lik[193]  -0.53  4.6e-3   0.26  -1.11  -0.68  -0.51  -0.35  -0.08   3263    1.0\n",
       "log_lik[194]  -0.29  4.2e-3   0.23  -0.79  -0.43  -0.27  -0.13   0.11   3016    1.0\n",
       "log_lik[195]   -1.2  9.8e-3   0.47  -2.33  -1.45  -1.12  -0.87   -0.5   2342    1.0\n",
       "log_lik[196]  -0.49  4.5e-3   0.26  -1.06  -0.64  -0.47  -0.32  -0.06   3240    1.0\n",
       "log_lik[197]   -0.3  4.4e-3   0.23  -0.81  -0.44  -0.28  -0.13   0.11   2852    1.0\n",
       "log_lik[198]  -0.55  4.7e-3   0.27  -1.16  -0.71  -0.53  -0.37  -0.11   3281    1.0\n",
       "log_lik[199]  -2.96    0.02   1.18  -5.75   -3.6  -2.75  -2.11  -1.29   4000    1.0\n",
       "log_lik[200]  -1.69    0.02   1.02  -4.27  -2.21   -1.5  -0.97  -0.29   4000    1.0\n",
       "log_lik[201]    0.5  5.2e-3   0.24  -0.03   0.35   0.52   0.67   0.93   2099    1.0\n",
       "log_lik[202]   0.35  5.2e-3   0.25  -0.21    0.2   0.37   0.53    0.8   2321    1.0\n",
       "log_lik[203]   0.23  5.1e-3   0.27  -0.35   0.07   0.25   0.42    0.7   2896    1.0\n",
       "log_lik[204]   0.23  5.0e-3   0.27  -0.37   0.06   0.26   0.42    0.7   2988    1.0\n",
       "log_lik[205]   0.51  5.1e-3   0.24-2.8e-3   0.36   0.53   0.68   0.93   2215    1.0\n",
       "log_lik[206]   0.12  5.3e-3    0.3  -0.55  -0.06   0.16   0.33   0.61   3158    1.0\n",
       "log_lik[207]   0.43  4.9e-3   0.24  -0.09   0.28   0.45    0.6   0.88   2503    1.0\n",
       "log_lik[208]  -0.58  8.7e-3   0.55  -1.93  -0.87  -0.48  -0.19   0.18   4000    1.0\n",
       "log_lik[209]   0.51  5.2e-3   0.24  -0.03   0.36   0.53   0.68   0.93   2111    1.0\n",
       "log_lik[210]   0.35  6.2e-3   0.35  -0.49   0.15    0.4   0.59   0.89   3250    1.0\n",
       "log_lik[211]  -0.44    0.01   0.65  -2.05  -0.78  -0.31   0.05   0.48   4000    1.0\n",
       "log_lik[212]   0.85  5.6e-3   0.25   0.32   0.69   0.87   1.02   1.27   1962    1.0\n",
       "log_lik[213]   0.81  5.2e-3   0.25   0.27   0.65   0.83   0.98   1.24   2384    1.0\n",
       "log_lik[214]   0.85  5.6e-3   0.25   0.32   0.69   0.87   1.02   1.27   1960    1.0\n",
       "log_lik[215]   0.88  6.5e-3   0.25   0.33   0.72    0.9   1.05    1.3   1465    1.0\n",
       "log_lik[216]    0.7  5.4e-3   0.26   0.13   0.53   0.72   0.88   1.15   2397    1.0\n",
       "log_lik[217]    0.8  7.0e-3   0.25   0.26   0.65   0.82   0.97   1.24   1293    1.0\n",
       "log_lik[218]  -0.45    0.01   0.65  -2.11  -0.78  -0.34   0.02   0.47   4000    1.0\n",
       "log_lik[219]    0.5  5.3e-3    0.3  -0.18   0.32   0.53   0.72   1.01   3237    1.0\n",
       "log_lik[220]   0.15  4.6e-3   0.24  -0.37-1.0e-3   0.16   0.32   0.57   2640    1.0\n",
       "log_lik[221]  -0.73  7.3e-3   0.46  -1.84  -0.99  -0.66  -0.41  -0.05   4000    1.0\n",
       "log_lik[222]   0.09  4.7e-3   0.24  -0.45  -0.06   0.11   0.26   0.52   2703    1.0\n",
       "log_lik[223]  -0.04  4.7e-3   0.25   -0.6  -0.19  -0.01   0.14    0.4   2927    1.0\n",
       "log_lik[224]   0.15  4.6e-3   0.24  -0.37-1.8e-3   0.16   0.32   0.57   2640    1.0\n",
       "log_lik[225]  -1.49    0.01   0.76  -3.35  -1.87  -1.36  -0.93   -0.4   4000    1.0\n",
       "log_lik[226]   0.15  4.6e-3   0.24  -0.38-6.2e-3   0.16   0.32   0.57   2641    1.0\n",
       "log_lik[227]  -1.27    0.01   0.68  -2.92  -1.62  -1.15  -0.78  -0.29   4000    1.0\n",
       "log_lik[228]  -0.49  6.4e-3   0.38  -1.42  -0.68  -0.43  -0.22   0.09   3495    1.0\n",
       "log_lik[229]   0.15  4.6e-3   0.24  -0.37-3.7e-3   0.16   0.32   0.57   2640    1.0\n",
       "log_lik[230]   0.58  5.5e-3   0.29  -0.09    0.4   0.61   0.78   1.07   2855    1.0\n",
       "log_lik[231]   0.57  5.5e-3   0.29   -0.1   0.39    0.6   0.77   1.06   2886    1.0\n",
       "log_lik[232]  -0.19  8.7e-3   0.55  -1.53  -0.48  -0.09    0.2   0.61   4000    1.0\n",
       "log_lik[233]   0.54  5.5e-3    0.3  -0.15   0.36   0.57   0.74   1.04   2957    1.0\n",
       "log_lik[234]   0.72  5.1e-3   0.26   0.16   0.56   0.74    0.9   1.16   2550    1.0\n",
       "log_lik[235]   0.72  5.1e-3   0.26   0.16   0.56   0.74    0.9   1.16   2546    1.0\n",
       "log_lik[236]   0.57  5.5e-3   0.29  -0.09   0.41    0.6   0.77   1.05   2724    1.0\n",
       "log_lik[237]   0.86  5.4e-3   0.25   0.34    0.7   0.88   1.03   1.29   2101    1.0\n",
       "log_lik[238]   0.08    0.01   0.46  -1.06  -0.14   0.16   0.41   0.75   1881    1.0\n",
       "log_lik[239]   0.93  6.3e-3   0.25   0.39   0.77   0.94   1.11   1.37   1602    1.0\n",
       "log_lik[240]   -1.2  5.8e-3   0.34  -2.01  -1.37  -1.14  -0.96  -0.68   3342    1.0\n",
       "log_lik[241]  -0.87  4.4e-3   0.25  -1.45  -1.02  -0.85   -0.7  -0.46   3120    1.0\n",
       "log_lik[242]  -0.74  5.0e-3   0.24  -1.27  -0.88  -0.72  -0.58  -0.33   2312    1.0\n",
       "log_lik[243]  -0.99  4.7e-3   0.27  -1.63  -1.14  -0.96   -0.8  -0.54   3352    1.0\n",
       "log_lik[244]  -1.59  7.2e-3   0.45  -2.63  -1.83  -1.52  -1.27  -0.89   4000    1.0\n",
       "log_lik[245]  -0.75  4.4e-3   0.23  -1.28  -0.89  -0.74  -0.58  -0.37   2728    1.0\n",
       "log_lik[246]  -1.01  4.8e-3   0.28  -1.67  -1.16  -0.98  -0.81  -0.55   3386    1.0\n",
       "log_lik[247]  -0.69  4.8e-3   0.23   -1.2  -0.82  -0.67  -0.52  -0.29   2296    1.0\n",
       "log_lik[248]  -0.74  4.4e-3   0.23  -1.26  -0.88  -0.73  -0.57  -0.36   2687    1.0\n",
       "log_lik[249]  -3.48    0.02   1.17  -6.08  -4.16  -3.29  -2.63  -1.74   4000    1.0\n",
       "lp__          248.8    2.56  21.85  216.0  233.0 244.93 260.58 304.29     73   1.08\n",
       "\n",
       "Samples were drawn using NUTS at Thu Dec  7 11:47:44 2017.\n",
       "For each parameter, n_eff is a crude measure of effective sample size,\n",
       "and Rhat is the potential scale reduction factor on split chains (at \n",
       "convergence, Rhat=1)."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_hierarchical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the hierarchical model we have that each value for $\\widehat{R}$ is very close to 1.0 and in any case under 1.10 . Therefors, we can say that the model is converging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior predictive checking\n",
    "\n",
    "### Separate Model\n",
    "In order to check how well the different models are fitting the data, we will use leave-one-out (LOO) cross validation method. In particular, we will use PSIS-LOO (Pareto smoothed importance sampling LOO) code for computing approximate LOO-CV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective number of parameteres:  59.5482042314\n",
      "-------------------------------------\n",
      "Group  0\n",
      "K values > 0.7: 1\n",
      "K values > 0.5: 1\n",
      "p_eff for group  0 :  2.08025049099\n",
      "p_eff avg:  2.38192816926\n",
      "-------------------------------------\n",
      "Group  1\n",
      "K values > 0.7: 0\n",
      "K values > 0.5: 3\n",
      "p_eff for group  1 :  2.45018378005\n",
      "p_eff avg:  2.38192816926\n",
      "-------------------------------------\n",
      "Group  2\n",
      "K values > 0.7: 0\n",
      "K values > 0.5: 1\n",
      "p_eff for group  2 :  2.22291210158\n",
      "p_eff avg:  2.38192816926\n",
      "-------------------------------------\n",
      "Group  3\n",
      "K values > 0.7: 1\n",
      "K values > 0.5: 2\n",
      "p_eff for group  3 :  2.7532234454\n",
      "p_eff avg:  2.38192816926\n",
      "-------------------------------------\n",
      "Group  4\n",
      "K values > 0.7: 1\n",
      "K values > 0.5: 1\n",
      "p_eff for group  4 :  2.32660434164\n",
      "p_eff avg:  2.38192816926\n",
      "-------------------------------------\n",
      "Group  5\n",
      "K values > 0.7: 0\n",
      "K values > 0.5: 2\n",
      "p_eff for group  5 :  2.34123855658\n",
      "p_eff avg:  2.38192816926\n",
      "-------------------------------------\n",
      "Group  6\n",
      "K values > 0.7: 0\n",
      "K values > 0.5: 4\n",
      "p_eff for group  6 :  2.15004845183\n",
      "p_eff avg:  2.38192816926\n",
      "-------------------------------------\n",
      "Group  7\n",
      "K values > 0.7: 0\n",
      "K values > 0.5: 4\n",
      "p_eff for group  7 :  2.77561115305\n",
      "p_eff avg:  2.38192816926\n",
      "-------------------------------------\n",
      "Group  8\n",
      "K values > 0.7: 0\n",
      "K values > 0.5: 1\n",
      "p_eff for group  8 :  1.64696976203\n",
      "p_eff avg:  2.38192816926\n",
      "-------------------------------------\n",
      "Group  9\n",
      "K values > 0.7: 0\n",
      "K values > 0.5: 1\n",
      "p_eff for group  9 :  2.62880079583\n",
      "p_eff avg:  2.38192816926\n",
      "-------------------------------------\n",
      "Group  10\n",
      "K values > 0.7: 1\n",
      "K values > 0.5: 1\n",
      "p_eff for group  10 :  2.64984658409\n",
      "p_eff avg:  2.38192816926\n",
      "-------------------------------------\n",
      "Group  11\n",
      "K values > 0.7: 0\n",
      "K values > 0.5: 1\n",
      "p_eff for group  11 :  1.99553381326\n",
      "p_eff avg:  2.38192816926\n",
      "-------------------------------------\n",
      "Group  12\n",
      "K values > 0.7: 0\n",
      "K values > 0.5: 0\n",
      "p_eff for group  12 :  1.94288809725\n",
      "p_eff avg:  2.38192816926\n",
      "-------------------------------------\n",
      "Group  13\n",
      "K values > 0.7: 0\n",
      "K values > 0.5: 1\n",
      "p_eff for group  13 :  3.04490928189\n",
      "p_eff avg:  2.38192816926\n",
      "-------------------------------------\n",
      "Group  14\n",
      "K values > 0.7: 0\n",
      "K values > 0.5: 0\n",
      "p_eff for group  14 :  2.0942209358\n",
      "p_eff avg:  2.38192816926\n",
      "-------------------------------------\n",
      "Group  15\n",
      "K values > 0.7: 0\n",
      "K values > 0.5: 2\n",
      "p_eff for group  15 :  2.34968063436\n",
      "p_eff avg:  2.38192816926\n",
      "-------------------------------------\n",
      "Group  16\n",
      "K values > 0.7: 1\n",
      "K values > 0.5: 1\n",
      "p_eff for group  16 :  2.4821074725\n",
      "p_eff avg:  2.38192816926\n",
      "-------------------------------------\n",
      "Group  17\n",
      "K values > 0.7: 0\n",
      "K values > 0.5: 2\n",
      "p_eff for group  17 :  2.33158369484\n",
      "p_eff avg:  2.38192816926\n",
      "-------------------------------------\n",
      "Group  18\n",
      "K values > 0.7: 0\n",
      "K values > 0.5: 1\n",
      "p_eff for group  18 :  2.38353613058\n",
      "p_eff avg:  2.38192816926\n",
      "-------------------------------------\n",
      "Group  19\n",
      "K values > 0.7: 0\n",
      "K values > 0.5: 2\n",
      "p_eff for group  19 :  2.62405840457\n",
      "p_eff avg:  2.38192816926\n",
      "-------------------------------------\n",
      "Group  20\n",
      "K values > 0.7: 0\n",
      "K values > 0.5: 2\n",
      "p_eff for group  20 :  3.01945198476\n",
      "p_eff avg:  2.38192816926\n",
      "-------------------------------------\n",
      "Group  21\n",
      "K values > 0.7: 0\n",
      "K values > 0.5: 3\n",
      "p_eff for group  21 :  2.52828689895\n",
      "p_eff avg:  2.38192816926\n",
      "-------------------------------------\n",
      "Group  22\n",
      "K values > 0.7: 0\n",
      "K values > 0.5: 1\n",
      "p_eff for group  22 :  2.1810477052\n",
      "p_eff avg:  2.38192816926\n",
      "-------------------------------------\n",
      "Group  23\n",
      "K values > 0.7: 0\n",
      "K values > 0.5: 1\n",
      "p_eff for group  23 :  2.16547756413\n",
      "p_eff avg:  2.38192816926\n",
      "-------------------------------------\n",
      "Group  24\n",
      "K values > 0.7: 0\n",
      "K values > 0.5: 2\n",
      "p_eff for group  24 :  2.37973215022\n",
      "p_eff avg:  2.38192816926\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNCjlHxHRoJR/RESD/gKxeMwNSfbO6AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f997a952048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "S = samples[0][\"log_lik\"].shape[0]\n",
    "\n",
    "log_lik_sep = samples[0][\"log_lik\"].T\n",
    "for sample in samples[1:]:\n",
    "    log_lik = sample[\"log_lik\"].T\n",
    "    log_lik_sep = np.concatenate((log_lik_sep,log_lik))\n",
    "log_lik_sep = log_lik_sep.T\n",
    "\n",
    "loo_sep ,loos_sep, ks_sep = psis.psisloo(log_lik_sep)\n",
    "lppd_sep = np.sum(np.log(np.mean(np.exp(log_lik_sep), axis = 0)))\n",
    "p_eff_sep = lppd_sep - loo_sep\n",
    "print(\"Effective number of parameteres: \", p_eff_sep)\n",
    "\n",
    "ks_sep_matrix = ks_sep.reshape((25,10))\n",
    "\n",
    "for i, sample in enumerate(samples):\n",
    "    log_lik_i = sample[\"log_lik\"]\n",
    "    loo_sep_i ,loos_sep_i, ks_sep_i = psis.psisloo(log_lik_i)\n",
    "    lppd_sep_i = np.sum(np.log(np.mean(np.exp(log_lik_i), axis = 0)))\n",
    "    p_eff_sep_i = lppd_sep_i - loo_sep_i\n",
    "    print(\"-------------------------------------\")\n",
    "    print(\"Group \", i)\n",
    "    print(\"K values > 0.7: {}\".format(np.sum(ks_sep_i>=0.7)))\n",
    "    print(\"K values > 0.5: {}\".format(np.sum(ks_sep_i>0.5)))\n",
    "    #print(\"Agg K values > 0.7: {}\".format(np.sum(ks_sep_matrix[i]>=0.7)))\n",
    "    #print(\"Agg K values > 0.5: {}\".format(np.sum(ks_sep_matrix[i]>0.5)))\n",
    "    print(\"p_eff for group \", i, \": \", p_eff_sep_i)\n",
    "    print(\"p_eff avg: \", p_eff_sep/25)\n",
    "\n",
    "for i in range(25):\n",
    "    plt.scatter(range(1,11), ks_sep_matrix[i])\n",
    "plt.plot(range(1,11), [0.5 for i in range(1,11)], \"b--\")\n",
    "plt.plot(range(1,11), [0.7 for i in range(1,11)], \"r--\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that the number of effective estimated parameters for the model of each country is close to 3 for most of them, with some exceptions going as low as 2 and as high as 4. On average the number of effective estimated parameters approaces 3 as well. That is what we expected, since each model depends on three parameters: $\\alpha$, $\\beta$ and $\\sigma$.\n",
    "\n",
    "Accordingly, most of the models had appropriate k values. Anyway, in some of them we have that some values are over 0.5 and so in these cases the $p_{eff}$ values are biased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQkMl5YgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9992939ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "log_lik_hier = samples_hierarchical[\"log_lik\"]\n",
    "\n",
    "loo_hier ,loos_hier, ks_hier = psis.psisloo(log_lik_hier)\n",
    "lppd_hier = np.sum(np.log(np.mean(np.exp(log_lik_hier), axis = 0)))\n",
    "p_eff_hier = lppd_hier - loo_hier\n",
    "\n",
    "plt.scatter(range(1,251), ks_hier)\n",
    "plt.plot(range(1,251), [0.5 for i in range(1,251)], \"b--\")\n",
    "plt.plot(range(1,251), [0.7 for i in range(1,251)], \"r--\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "We want now to compare the predictive performance of the two models that we used in the previous sections.\n",
    "In order to do so we will use the produced PSIS-LOO values for both models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separate model:  -31.6342288342 59.5482042314\n",
      "Hierarchical model:  -58.4242965886 44.1347337955\n"
     ]
    }
   ],
   "source": [
    "print(\"Separate model: \", loo_sep, p_eff_sep)\n",
    "print(\"Hierarchical model: \", loo_hier, p_eff_hier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
